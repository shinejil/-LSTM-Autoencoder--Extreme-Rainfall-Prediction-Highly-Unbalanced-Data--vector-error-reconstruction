{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1180744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"rainfall_data.csv\")\n",
    "data=data.iloc[:,3:]\n",
    "# calculate 95th percentile for each column\n",
    "percentiles = data.quantile(0.95)\n",
    "\n",
    "# filter rows above 95th percentile for each column\n",
    "filtered_data = data[(data > percentiles).any(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d47e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"rainfall_data.csv\")\n",
    "# label extreme rows in original dataframe\n",
    "data['rainfall_class'] = 'normal'\n",
    "data.loc[data.index.isin(filtered_data.index), 'rainfall_class'] = 'extreme'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1352d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of extreme and normal rainfall values\n",
    "extreme_count = len(data[data['rainfall_class'] == 'extreme'])\n",
    "normal_count = len(data[data['rainfall_class'] == 'normal'])\n",
    "\n",
    "# Plot the counts as a bar chart\n",
    "plt.bar(['Normal', 'Extreme'], [normal_count, extreme_count], color=['blue', 'red'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Rainfall Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Rainfall Data')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb3cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import seed\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import skfuzzy as fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5bb3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_data=data[data[\"rainfall_class\"]==\"extreme\"]\n",
    "normal_data=data[data[\"rainfall_class\"]==\"normal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe9d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss(sparsity_weight=0.01, sparsity_target=0.001):\n",
    "    def loss(y_true, y_pred):\n",
    "        mse_loss = K.mean(K.square(abs(y_true[:,2] - y_pred[:,2])), axis=-1)\n",
    "        kl_divergence = sparsity_weight * K.sum(sparsity_target * K.log(abs(sparsity_target / K.mean(abs(y_pred[:, 2])))) \n",
    "                        + abs((1 - sparsity_target)) * K.log(abs((1 - sparsity_target) / (1 - K.mean(abs(y_pred[:, 2]))))))\n",
    "        return mse_loss + kl_divergence\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81dcedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size):\n",
    "    best_reg_strength = None\n",
    "    best_loss = float('inf')\n",
    "    for reg_strength in reg_strengths:\n",
    "        # Define the autoencoder architecture\n",
    "        input_layer = Input(shape=(3,))\n",
    "        encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=reg_strength))(input_layer)\n",
    "        decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "        # Create the autoencoder\n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "        # Compile the autoencoder with the weighted loss function\n",
    "        #autoencoder.compile(optimizer='adam', loss=\"mse\")\n",
    "        autoencoder.compile(optimizer='adam', loss=weighted_loss())\n",
    "        # Train the model\n",
    "        history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "        #history = autoencoder.fit(train_data,epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Calculate the validation loss\n",
    "        val_loss = np.mean(history.history['val_loss'])\n",
    "\n",
    "        # Update the best regularization strength and loss\n",
    "        if val_loss < best_loss:\n",
    "            best_reg_strength = reg_strength\n",
    "            best_loss = val_loss\n",
    "\n",
    "    print('Best regularization strength:', best_reg_strength)\n",
    "    return best_reg_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27438a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg_strengths = [0.1, 1, 10, 100]\n",
    "num_epochs = 3000\n",
    "batch_size = 100\n",
    "#train_data = normal_data[[\"windspeed_discretized\",\"tpw_discretized\"]].values.astype(\"float32\")\n",
    "train_data = normal_data[[\"windspeed\",\"tpw\",\"rainfall\"]].values.astype(\"float32\")\n",
    "\n",
    "#best_reg_strength = find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size)\n",
    "\n",
    "# Define the autoencoder architecture with the best regularization strength\n",
    "input_layer = Input(shape=(3,),name='input')\n",
    "encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=20),name='encoder')(input_layer)\n",
    "decoded = Dense(3, activation=None, name='decoder')(encoded)\n",
    "\n",
    "# Create the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder with the weighted loss function\n",
    "#autoencoder.compile(optimizer='adam', loss=\"mse\")\n",
    "autoencoder.compile(optimizer='adam', loss=weighted_loss())\n",
    "\n",
    "# Train the model with the majority class\n",
    "history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "#history = autoencoder.fit(train_data,epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de778135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Save the model to a file\n",
    "autoencoder.save(\"my_autoencoder_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data = data[[\"windspeed_discretized\",\"tpw_discretized\",\"rainfall_discretized\"]].values\n",
    "test_data = data[[\"windspeed\",\"tpw\",\"rainfall\"]].values\n",
    "autoencoder = load_model(\"my_autoencoder_model.h5\", custom_objects={\"loss\": weighted_loss()})\n",
    "\n",
    "predicted_data = autoencoder.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d769d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define the custom loss function\n",
    "\n",
    "# Load your autoencoder model\n",
    "autoencoder = load_model(\"my_autoencoder_model.h5\", custom_objects={\"loss\": weighted_loss()})\n",
    "#autoencoder = load_model(\"my_autoencoder_model.h5\")\n",
    "# Create a Keras model to get the output of the encoder and decoder layers\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(\"encoder\").output)\n",
    "decoder = Model(inputs=autoencoder.get_layer(\"decoder\").input, outputs=autoencoder.output)\n",
    "\n",
    "# Get the encoded representation of the input data\n",
    "encoded_X = encoder.predict(test_data)\n",
    "\n",
    "# Reconstruct the input data from the encoded representation\n",
    "reconstructed_X = decoder.predict(encoded_X)\n",
    "#print(test_data)\n",
    "print(reconstructed_X)\n",
    "# Compute the MSE reconstruction loss for each sample\n",
    "mse_loss = np.sum((abs(test_data - abs( reconstructed_X))), axis=1)\n",
    "#mse_loss = abs(np.sum(test_data)-np.sum(reconstructed_X), axis =1)\n",
    "data[\"mse_loss\"]=mse_loss\n",
    "# Print the MSE reconstruction loss for each sample\n",
    "print(data[\"mse_loss\"])\n",
    "top10_indices = mse_loss.argsort()[::-1][:10]\n",
    "\n",
    "# Print the MSE reconstruction loss for each of the top 10 samples\n",
    "for i in top10_indices:\n",
    "    print('MSE for sample', i+1, ':', mse_loss[i])\n",
    "sample_idx = 2387  # 0-based index of sample 5\n",
    "print('MSE loss for sample 2387:', mse_loss[sample_idx])\n",
    "sample_idx = 2388  # 0-based index of sample 5\n",
    "print('MSE loss for sample 2388:', mse_loss[sample_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc1543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram of the MSE loss distribution\n",
    "plt.hist(mse_loss, bins=50)\n",
    "plt.xlabel(\"MSE Loss\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"MSE Loss Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = data.sort_values('mse_loss', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3809cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8293b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max = df_sorted.nlargest(129, 'mse_loss')\n",
    "threshold=np.mean(df_max[\"mse_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda174c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['predicted_rainfall_class'] = np.where(data['mse_loss'] > threshold, 'extreme', 'normal')\n",
    "data[data[\"mse_loss\"]>threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24025f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_predictions = data[data['predicted_rainfall_class'] != data['rainfall_class']]\n",
    "\n",
    "percentage_of_error=(wrong_predictions.shape[0]/data.shape[0])*100\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(\"wrong predictions = \",wrong_predictions.shape[0])\n",
    "print(\"total data = \",data.shape[0])\n",
    "print(\"percentage of error = \",percentage_of_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['actual_rainfall_binary'] = (data['predicted_rainfall_class'] == data['rainfall_class'])\n",
    "\n",
    "# Compute the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(data['actual_rainfall_binary'], data['mse_loss'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# Calculate TNR\n",
    "tnr = 1 - fpr\n",
    "\n",
    "# Calculate Youden's J statistic for each threshold\n",
    "j_stat = tpr + tnr - 1\n",
    "\n",
    "# Find the index of the threshold that maximizes J\n",
    "best_threshold_idx = np.argmax(j_stat)\n",
    "\n",
    "# Get the best threshold\n",
    "best_threshold = thresholds[best_threshold_idx]\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Print the best threshold\n",
    "print(\"Best threshold:\", best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0c4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb9f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['predicted_rainfall_class'] = np.where(data['mse_loss'] > threshold, 'extreme', 'normal')\n",
    "data[data[\"mse_loss\"]>threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e3526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_predictions = data[data['predicted_rainfall_class'] != data['rainfall_class']]\n",
    "percentage_of_error=(wrong_predictions.shape[0]/data.shape[0])*100\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(\"wrong predictions = \",wrong_predictions.shape[0])\n",
    "print(\"total data = \",data.shape[0])\n",
    "print(\"percentage of error = \",percentage_of_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e2a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = len(data[data['predicted_rainfall_class'] == data['rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c0672a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b44a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4975ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ebfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

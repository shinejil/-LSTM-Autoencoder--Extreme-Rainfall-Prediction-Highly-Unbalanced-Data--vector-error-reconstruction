{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c808eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4c396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "df=pd.read_csv(r\"rainfall_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc279823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall  rainfall_tree  \\\n",
      "0      6/1/2000  12.245595  0.033285   4.688589              0   \n",
      "1      6/2/2000  12.825491  0.044874  11.330589              0   \n",
      "2      6/3/2000  12.921664  0.010243   6.894713              0   \n",
      "3      6/4/2000  15.149001  0.036881   4.613324              0   \n",
      "4      6/5/2000  18.495907  0.139491   8.111635              0   \n",
      "...         ...        ...       ...        ...            ...   \n",
      "2557  9/26/2020   5.577215  0.009963   4.467977              0   \n",
      "2558  9/27/2020   5.184293  0.002341   1.572182              0   \n",
      "2559  9/28/2020   4.469007  0.000867   1.819019              0   \n",
      "2560  9/29/2020   4.259090  0.001416   2.198017              0   \n",
      "2561  9/30/2020   5.513838  0.002474  12.713634              0   \n",
      "\n",
      "      windspeed_tree  tpw_tree  \n",
      "0                  0         0  \n",
      "1                  0         0  \n",
      "2                  0         0  \n",
      "3                  0         0  \n",
      "4                  0         0  \n",
      "...              ...       ...  \n",
      "2557               0         0  \n",
      "2558               0         0  \n",
      "2559               0         0  \n",
      "2560               0         0  \n",
      "2561               0         0  \n",
      "\n",
      "[2562 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shine\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\shine\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# create decision tree for target variable\n",
    "tree = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "\n",
    "# discretize target variable using decision tree\n",
    "data['rainfall_tree'] = pd.cut(data['rainfall'], bins=6, labels=False)\n",
    "tree.fit(data[['rainfall']], data['rainfall_tree'])\n",
    "data['rainfall_tree'] = tree.predict(data[['rainfall']])\n",
    "\n",
    "# create new columns for the discretized data\n",
    "data['windspeed_tree'] = tree.predict(pd.cut(data['windspeed'], bins=6, labels=False).values.reshape(-1,1))\n",
    "data['tpw_tree'] = tree.predict(pd.cut(data['tpw'], bins=6, labels=False).values.reshape(-1,1))\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c7451d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_data=df[df[\"rainfall\"]>50]\n",
    "normal_data=df[df[\"rainfall\"]<=50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a2fbe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shine\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\shine\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\shine\\anaconda3\\envs\\environment\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\shine\\AppData\\Local\\Temp\\ipykernel_1112\\553882722.py\", line 42, in loss  *\n        normal_loss = K.mean(K.square(y_true - y_pred))\n\n    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m     81\u001b[0m train_data \u001b[38;5;241m=\u001b[39m normal_data[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindspeed_tree\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpw_tree\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrainfall_tree\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 82\u001b[0m best_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_reg_strength\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_strengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Define the autoencoder architecture with the best regularization strength\u001b[39;00m\n\u001b[0;32m     85\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,))\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mfind_best_reg_strength\u001b[1;34m(train_data, reg_strengths, num_epochs, batch_size)\u001b[0m\n\u001b[0;32m     61\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39mweighted_loss(normal_weight, extreme_weight))\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Calculate the validation loss\u001b[39;00m\n\u001b[0;32m     67\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileog1v9nnj.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7dg2hloz.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__loss\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     10\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     11\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 12\u001b[0m normal_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39mmean, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39msquare, (ag__\u001b[38;5;241m.\u001b[39mld(y_true) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(y_pred),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m extreme_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39mmean, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39msquare, (ag__\u001b[38;5;241m.\u001b[39mld(y_true) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(y_pred),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mld(y_true) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\shine\\anaconda3\\envs\\environment\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\shine\\AppData\\Local\\Temp\\ipykernel_1112\\553882722.py\", line 42, in loss  *\n        normal_loss = K.mean(K.square(y_true - y_pred))\n\n    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "threshold = 9\n",
    "best_error = float('inf')\n",
    "normal_error = 0\n",
    "extreme_error = 0\n",
    "import numpy as np\n",
    "count=0\n",
    "flag=False\n",
    "\n",
    "\n",
    "for normal_weight in np.arange(0.005,0.006,0.0001):\n",
    "    for extreme_weight in np.arange(0.001,0.005, 0.001):\n",
    "        # Load the data\n",
    "        data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "        # create decision tree for target variable\n",
    "        tree = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "\n",
    "        # discretize target variable using decision tree\n",
    "        data['rainfall_tree'] = pd.cut(data['rainfall'], bins=6, labels=False)\n",
    "        tree.fit(data[['rainfall']], data['rainfall_tree'])\n",
    "        data['rainfall_tree'] = tree.predict(data[['rainfall']])\n",
    "\n",
    "        # create new columns for the discretized data\n",
    "        data['windspeed_tree'] = tree.predict(pd.cut(data['windspeed'], bins=6, labels=False).values.reshape(-1,1))\n",
    "        data['tpw_tree'] = tree.predict(pd.cut(data['tpw'], bins=6, labels=False).values.reshape(-1,1))\n",
    "\n",
    "        count+=1\n",
    "        print(count)\n",
    "        # Split the data into normal and extreme rainfall\n",
    "        normal_data = data[data['rainfall'] <= 50]\n",
    "        extreme_data = data[data['rainfall'] > 50]\n",
    "        def weighted_loss(normal_weight, extreme_weight):\n",
    "            def loss(y_true, y_pred):\n",
    "                normal_loss = K.mean(K.square(y_true - y_pred))\n",
    "                extreme_loss = K.mean(K.square(y_true - y_pred) * K.cast(y_true > 50, 'float32'))\n",
    "                return K.mean(normal_weight * normal_loss + extreme_weight * extreme_loss)\n",
    "            return loss\n",
    "        # train your model with the current combination of weights\n",
    "        # Define a function to find the best regularization strength\n",
    "        def find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size):\n",
    "            best_reg_strength = None\n",
    "            best_loss = float('inf')\n",
    "            for reg_strength in reg_strengths:\n",
    "                # Define the autoencoder architecture\n",
    "                input_layer = Input(shape=(3,))\n",
    "                encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=reg_strength))(input_layer)\n",
    "                decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "                # Create the autoencoder\n",
    "                autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "                # Compile the autoencoder with the weighted loss function\n",
    "                autoencoder.compile(optimizer='adam', loss=weighted_loss(normal_weight, extreme_weight))\n",
    "\n",
    "                # Train the model\n",
    "                history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "                # Calculate the validation loss\n",
    "                val_loss = np.mean(history.history['val_loss'])\n",
    "\n",
    "                # Update the best regularization strength and loss\n",
    "                if val_loss < best_loss:\n",
    "                    best_reg_strength = reg_strength\n",
    "                    best_loss = val_loss\n",
    "\n",
    "            print('Best regularization strength:', best_reg_strength)\n",
    "            return best_reg_strength\n",
    "\n",
    "        # Find the best regularization strength\n",
    "        reg_strengths = [0.01, 0.1, 1, 10]\n",
    "        num_epochs = 150\n",
    "        batch_size = 32\n",
    "        train_data = normal_data[[\"windspeed_tree\",\"tpw_tree\",\"rainfall_tree\"]].values\n",
    "        best_reg_strength = find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size)\n",
    "\n",
    "        # Define the autoencoder architecture with the best regularization strength\n",
    "        input_layer = Input(shape=(3,))\n",
    "        encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "        decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "        # Create the autoencoder\n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "        # Compile the autoencoder with the weighted loss function\n",
    "        autoencoder.compile(optimizer='adam', loss=weighted_loss(normal_weight, extreme_weight))\n",
    "\n",
    "        # Train the model with the majority class\n",
    "        train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "        history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "        # Use the trained autoencoder to predict the rainfall values for all data points\n",
    "        test_data = df[[\"windspeed_tree\",\"tpw_tree\",\"rainfall_tree\"]].values\n",
    "        predicted_data = autoencoder.predict(test_data)\n",
    "        data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "        # Calculate the error between the predicted and actual rainfall values\n",
    "        data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "        # Classify the rainfall data into normal and extreme based on the error threshold\n",
    "        # Define a range of threshold values to try\n",
    "        def best_threshold():\n",
    "            thresholds = np.arange(0.5, 10.0, 0.1)\n",
    "\n",
    "            # Initialize variables to store the best threshold and the lowest error\n",
    "            best_threshold = 0.0\n",
    "            lowest_error = float('inf')\n",
    "\n",
    "            # Loop over the threshold values and calculate the error for each\n",
    "            for threshold in thresholds:\n",
    "                # Classify the rainfall data into normal and extreme based on the threshold\n",
    "                pre_df['rainfall_class'] = np.where(data['error'] > threshold, 'extreme', 'normal')\n",
    "\n",
    "                # Calculate the accuracy of the classification\n",
    "                accuracy = sum(pre_df['rainfall_class'] == pre_df['actuall_rainfall_class']) / len(data)\n",
    "\n",
    "                # Calculate the error of the classification\n",
    "                error = 1 - accuracy\n",
    "\n",
    "                # Update the best threshold and lowest error if necessary\n",
    "                if error < lowest_error:\n",
    "                    best_threshold = threshold\n",
    "                    lowest_error = error\n",
    "\n",
    "            return threshold\n",
    "        \n",
    "        data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "        # Define the actual rainfall class based on the threshold of 11\n",
    "        data['actual_rainfall_class'] = np.where(data['rainfall'] > 50, 'Extreme', 'Normal')\n",
    "\n",
    "        result_list = []\n",
    "        for index, row in data.iterrows():\n",
    "            result_list.append(row['rainfall_class'] == row['actual_rainfall_class'])\n",
    "\n",
    "        if all(result_list):\n",
    "            print(\"found it\")\n",
    "            flag=True\n",
    "            break\n",
    "    if flag:\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print(normal_weight, extreme_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

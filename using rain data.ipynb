{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac392054",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ec2e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "df=pd.read_csv(r\"rainfall_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aacab558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0bb3d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum rainfall value:  80.63607437\n"
     ]
    }
   ],
   "source": [
    "max_rainfall = data['rainfall'].max()\n",
    "print(\"Maximum rainfall value: \", max_rainfall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f21602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54dc8699",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Read the data from the CSV file\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Define the threshold value\u001b[39;00m\n\u001b[0;32m      7\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = df\n",
    "\n",
    "# Define the threshold value\n",
    "threshold = 30\n",
    "\n",
    "# Add a new column to indicate if the rainfall is extreme or normal\n",
    "data['rainfall_category'] = ['Extreme' if rainfall > threshold else 'Normal' for rainfall in data['rainfall']]\n",
    "\n",
    "# Print the updated dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5ea30609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the count of extreme and normal categories\n",
    "counts = data['rainfall_category'].value_counts()\n",
    "\n",
    "# Create a bar chart to show the count of extreme and normal categories\n",
    "plt.bar(counts.index, counts.values)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Rainfall Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Rainfall Categories')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c71238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAHWCAYAAACBjZMqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+zklEQVR4nO3deVxVdf7H8fcF5Koo4AZXlBAzNQoztZRSc2FA04xscykFt3Kp3HCp3GdCnVyyMf01Gdhitow5ZaNGGjoJamnmklqSiqVAiYArspzfHz24481Srt3D5uv5eJzHw/M93/M9n+PjMd7ec873eyyGYRgCAAAAALiUW1kXAAAAAACVEWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAUCFZLBZNnz79ms4tKCjQhAkTFBgYKDc3N0VFRTl1fqNGjRQdHW3fT0pKksViUVJS0jXVAwConAhbAABTJSQkyGKx2DcPDw81aNBA0dHR+umnn8qkptdff11///vf9dBDD2n58uUaM2ZMqVz3t38XVatWVUBAgCIjI7Vo0SKdPn36msdOTk7W9OnTlZ2d7bqCAQB/ikdZFwAAuD7MnDlTwcHBunDhgrZu3aqEhAR98cUX2rt3r6pWrer0eOfPn5eHx7X9jG3cuFENGjTQggULrun8P6v47yI/P1/p6elKSkrS6NGjNX/+fH300Udq0aKF02MmJydrxowZio6Olq+vr+uLBgA4jbAFACgV3bt3V5s2bSRJQ4YMUd26dTVnzhx99NFHeuSRR5we71oCWrHMzMwyDSSX/l1I0uTJk7Vx40b17NlTvXr10v79+1WtWrUyqw8A4Bq8RggAKBMdOnSQJKWmptrbLl68qKlTp6p169by8fGRl5eXOnTooM8///yy8387Z2v69OmyWCw6dOiQ/emOj4+PYmJidO7cOUnSkSNHZLFY9Pnnn2vfvn321/mK51q9+OKLuuuuu1SnTh1Vq1ZNrVu31gcffGDeX8IlunTpoilTpujo0aN666237O27d+9WdHS0GjdurKpVq8pms2nQoEE6efKkw73HxsZKkoKDg+33deTIEUlSfHy8unTpIj8/P1mtVoWEhGjJkiWlcl8AcD3jyRYAoEwUB4FatWrZ23Jzc/Xaa6+pb9++Gjp0qE6fPq1ly5YpMjJS27dvV8uWLa867iOPPKLg4GDFxcVp586deu211+Tn56c5c+aoXr16evPNN/W3v/1NZ86cUVxcnCTp5ptvliS99NJL6tWrl/r376+LFy9q5cqVevjhh7VmzRr16NHD5X8Hv/X444/r2Wef1aeffqqhQ4dKkhITE/XDDz8oJiZGNptN+/bt06uvvqp9+/Zp69atslgs6t27t7777ju98847WrBggerWrStJqlevniRpyZIluuWWW9SrVy95eHjo448/1ogRI1RUVKSRI0eafl8AcL0ibAEASkVOTo5++eUXXbhwQdu2bdOMGTNktVrVs2dPe59atWrpyJEj8vT0tLcNHTpUzZs318svv6xly5Zd9Tq33367Q7+TJ09q2bJlmjNnjry8vPTYY4/ptddek7u7ux577DGHc7/77juH1/dGjRqlVq1aaf78+aUStho2bCgfHx+Hp30jRozQuHHjHPq1a9dOffv21RdffKEOHTqoRYsWatWqld555x1FRUWpUaNGDv03bdp02X1169ZN8+fPJ2wBgIl4jRAAUCrCw8NVr149BQYG6qGHHpKXl5c++ugjNWzY0N7H3d3dHrSKioqUlZWlgoICtWnTRjt37izRdZ588kmH/Q4dOujkyZPKzc296rmXBpJTp04pJydHHTp0KPG1XaFGjRoOqxJeWtOFCxf0yy+/qF27dpJU4rouHaM49N5zzz364YcflJOT46LKAQC/xZMtAECpWLx4sZo2baqcnBy9/vrr2rx5s6xW62X9li9frnnz5unAgQPKz8+3twcHB5foOjfccIPDfvFriqdOnZK3t/cVz12zZo3++te/ateuXcrLy7O3WyyWEl3bFc6cOSM/Pz/7flZWlmbMmKGVK1cqMzPToW9Jg9KWLVs0bdo0paSk2OevXTqGj4/Pny8cAHAZwhYAoFTceeed9hX4oqKi1L59e/Xr108HDx5UjRo1JElvvfWWoqOjFRUVpdjYWPn5+cnd3V1xcXEOr9Zdibu7+++2G4ZxxfP++9//qlevXurYsaNeeeUV1a9fX1WqVFF8fLxWrFjhxJ1eux9//FE5OTlq0qSJve2RRx5RcnKyYmNj1bJlS9WoUUNFRUXq1q2bioqKrjpmamqqunbtqubNm2v+/PkKDAyUp6en/vOf/2jBggUlGgMAcG0IWwCAUlccoDp37qx//OMfmjRpkiTpgw8+UOPGjbVq1SqHp0nTpk0zvaZ//etfqlq1qtavX+/wxC0+Pt70axd78803JUmRkZGSfn0at2HDBs2YMUNTp0619/v+++8vO/ePnr59/PHHysvL00cffeTw1O/3VngEALgWc7YAAGWiU6dOuvPOO7Vw4UJduHBB0v+eSl36FGrbtm1KSUkxvR53d3dZLBYVFhba244cOaLVq1ebfm3p1w8tz5o1S8HBwerfv7+9Junyp3ILFy687HwvLy9JUnZ2tkP7742Rk5NTqiESAK5XPNkCAJSZ2NhYPfzww0pISNCTTz6pnj17atWqVXrggQfUo0cPHT58WEuXLlVISIjOnDljai09evTQ/Pnz1a1bN/Xr10+ZmZlavHixmjRpot27d7v0WmvXrtWBAwdUUFCgjIwMbdy4UYmJiQoKCtJHH31k/2Czt7e3OnbsqLlz5yo/P18NGjTQp59+qsOHD182ZuvWrSVJzz33nPr06aMqVarovvvuU0REhDw9PXXffffpiSee0JkzZ/TPf/5Tfn5+OnHihEvvCwDgiLAFACgzvXv31o033qgXX3xRQ4cOVXR0tNLT0/V///d/Wr9+vUJCQvTWW2/p/ffft3942CxdunTRsmXLNHv2bI0ePVrBwcGaM2eOjhw54vKwVfxKoKenp2rXrq3Q0FAtXLhQMTExqlmzpkPfFStW6KmnntLixYtlGIYiIiK0du1aBQQEOPS74447NGvWLC1dulTr1q1TUVGRDh8+rGbNmumDDz7Q888/r/Hjx8tms2n48OGqV6+eBg0a5NL7AgA4shhXmzEMAAAAAHAac7YAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAHf2SqBoqIiHT9+XDVr1pTFYinrcgAAAACUEcMwdPr0aQUEBMjN7crPrghbJXD8+HEFBgaWdRkAAAAAyoljx46pYcOGV+xD2CqBmjVrSvr1L9Tb27uMqwEAAABQVnJzcxUYGGjPCFdC2CqB4lcHvb29CVsAAAAASjS9iAUyAAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMUKZhKy4uTnfccYdq1qwpPz8/RUVF6eDBgw59OnXqJIvF4rA9+eSTDn3S0tLUo0cPVa9eXX5+foqNjVVBQYFDn6SkJLVq1UpWq1VNmjRRQkKC2bdnKouFjY2NrfJuAABUBmUatjZt2qSRI0dq69atSkxMVH5+viIiInT27FmHfkOHDtWJEyfs29y5c+3HCgsL1aNHD128eFHJyclavny5EhISNHXqVHufw4cPq0ePHurcubN27dql0aNHa8iQIVq/fn2p3SsAAACA64vFMAyjrIso9vPPP8vPz0+bNm1Sx44dJf36ZKtly5ZauHDh756zdu1a9ezZU8ePH5e/v78kaenSpZo4caJ+/vlneXp6auLEifrkk0+0d+9e+3l9+vRRdna21q1bd9W6cnNz5ePjo5ycHHl7e//5G3UB/p9fAJVZ+fllAgDAkTPZoFzN2crJyZEk1a5d26H97bffVt26dXXrrbdq8uTJOnfunP1YSkqKQkND7UFLkiIjI5Wbm6t9+/bZ+4SHhzuMGRkZqZSUlN+tIy8vT7m5uQ4bAAAAADjDo6wLKFZUVKTRo0fr7rvv1q233mpv79evn4KCghQQEKDdu3dr4sSJOnjwoFatWiVJSk9Pdwhakuz76enpV+yTm5ur8+fPq1q1ag7H4uLiNGPGDJffIwAAAIDrR7kJWyNHjtTevXv1xRdfOLQPGzbM/ufQ0FDVr19fXbt2VWpqqm688UZTapk8ebLGjh1r38/NzVVgYKAp1wIAAABQOZWL1whHjRqlNWvW6PPPP1fDhg2v2Ldt27aSpEOHDkmSbDabMjIyHPoU79tstiv28fb2vuypliRZrVZ5e3s7bAAAAADgjDINW4ZhaNSoUfrwww+1ceNGBQcHX/WcXbt2SZLq168vSQoLC9OePXuUmZlp75OYmChvb2+FhITY+2zYsMFhnMTERIWFhbnoTgAAAADAUZmGrZEjR+qtt97SihUrVLNmTaWnpys9PV3nz5+XJKWmpmrWrFnasWOHjhw5oo8++kgDBgxQx44d1aJFC0lSRESEQkJC9Pjjj+ubb77R+vXr9fzzz2vkyJGyWq2SpCeffFI//PCDJkyYoAMHDuiVV17Re++9pzFjxpTZvQMAAACo3Mp06XfLH6xfHh8fr+joaB07dkyPPfaY9u7dq7NnzyowMFAPPPCAnn/+eYdX+44eParhw4crKSlJXl5eGjhwoGbPni0Pj/9NSUtKStKYMWP07bffqmHDhpoyZYqio6NLVCdLvwNA6WLpdwBAeeVMNihX39kqrwhbAFC6+GUCAJRXFfY7WwAAAABQWRC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEZRq24uLidMcdd6hmzZry8/NTVFSUDh486NDnwoULGjlypOrUqaMaNWrowQcfVEZGhkOftLQ09ejRQ9WrV5efn59iY2NVUFDg0CcpKUmtWrWS1WpVkyZNlJCQYPbtAQAAALiOlWnY2rRpk0aOHKmtW7cqMTFR+fn5ioiI0NmzZ+19xowZo48//ljvv/++Nm3apOPHj6t3797244WFherRo4cuXryo5ORkLV++XAkJCZo6daq9z+HDh9WjRw917txZu3bt0ujRozVkyBCtX7++VO8XAAAAwPXDYhiGUdZFFPv555/l5+enTZs2qWPHjsrJyVG9evW0YsUKPfTQQ5KkAwcO6Oabb1ZKSoratWuntWvXqmfPnjp+/Lj8/f0lSUuXLtXEiRP1888/y9PTUxMnTtQnn3yivXv32q/Vp08fZWdna926dVetKzc3Vz4+PsrJyZG3t7c5N+8ki6WsKwAA85SfXyYAABw5kw3K1ZytnJwcSVLt2rUlSTt27FB+fr7Cw8PtfZo3b64bbrhBKSkpkqSUlBSFhobag5YkRUZGKjc3V/v27bP3uXSM4j7FY/xWXl6ecnNzHTYAAAAAcEa5CVtFRUUaPXq07r77bt16662SpPT0dHl6esrX19ehr7+/v9LT0+19Lg1axceLj12pT25urs6fP39ZLXFxcfLx8bFvgYGBLrlHAAAAANePchO2Ro4cqb1792rlypVlXYomT56snJwc+3bs2LGyLgkAAABABeNR1gVI0qhRo7RmzRpt3rxZDRs2tLfbbDZdvHhR2dnZDk+3MjIyZLPZ7H22b9/uMF7xaoWX9vntCoYZGRny9vZWtWrVLqvHarXKarW65N4AAAAAXJ/K9MmWYRgaNWqUPvzwQ23cuFHBwcEOx1u3bq0qVapow4YN9raDBw8qLS1NYWFhkqSwsDDt2bNHmZmZ9j6JiYny9vZWSEiIvc+lYxT3KR4DAAAAAFytTFcjHDFihFasWKF///vfatasmb3dx8fH/sRp+PDh+s9//qOEhAR5e3vrqaeekiQlJydL+nXp95YtWyogIEBz585Venq6Hn/8cQ0ZMkQvvPCCpF+Xfr/11ls1cuRIDRo0SBs3btTTTz+tTz75RJGRkVetk9UIAaB0sRohAKC8ciYblGnYsvxBYoiPj1d0dLSkXz9qPG7cOL3zzjvKy8tTZGSkXnnlFfsrgpJ09OhRDR8+XElJSfLy8tLAgQM1e/ZseXj87y3JpKQkjRkzRt9++60aNmyoKVOm2K9xNYQtAChdhC0AQHlVYcJWRUHYAoDSxS8TAKC8qrDf2QIAAACAyoKwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACVwStrKzs10xDAAAAABUGk6HrTlz5ujdd9+17z/yyCOqU6eOGjRooG+++calxQEAAABAReV02Fq6dKkCAwMlSYmJiUpMTNTatWvVvXt3xcbGurxAAAAAAKiIPJw9IT093R621qxZo0ceeUQRERFq1KiR2rZt6/ICAQAAAKAicvrJVq1atXTs2DFJ0rp16xQeHi5JMgxDhYWFrq0OAAAAACoop59s9e7dW/369dNNN92kkydPqnv37pKkr7/+Wk2aNHF5gQAAAABQETkdthYsWKBGjRrp2LFjmjt3rmrUqCFJOnHihEaMGOHyAgEAAACgIrIYhmE4c8LZs2fl5eVlVj3lUm5urnx8fJSTkyNvb++yLkeSZLGUdQUAYB7nfpkAACg9zmQDp+ds+fv7a9CgQfriiy+uuUAAAAAAqOycDltvvfWWsrKy1KVLFzVt2lSzZ8/W8ePHzagNAAAAACosp8NWVFSUVq9erZ9++klPPvmkVqxYoaCgIPXs2VOrVq1SQUGBGXUCAAAAQIXi9Jyt3/Pyyy8rNjZWFy9eVN26dfXkk09q0qRJql69uitqLHPM2QKA0sWcLQBAeeVMNnB6NcJiGRkZWr58uRISEnT06FE99NBDGjx4sH788UfNmTNHW7du1aeffnqtwwMAAABAheZ02Fq1apXi4+O1fv16hYSEaMSIEXrsscfk6+tr73PXXXfp5ptvdmWdAAAAAFChOB22YmJi1KdPH23ZskV33HHH7/YJCAjQc88996eLAwAAAICKyuk5W+fOnas0c7FKijlbAFC6mLMFACivTJ2zdWnQunDhgi5evOhwvLyEEQAAAAAoS04v/X727FmNGjVKfn5+8vLyUq1atRw2AAAAAMA1hK0JEyZo48aNWrJkiaxWq1577TXNmDFDAQEBeuONN8yoEQAAAAAqHKdfI/z444/1xhtvqFOnToqJiVGHDh3UpEkTBQUF6e2331b//v3NqBMAAAAAKhSnn2xlZWWpcePGkn6dn5WVlSVJat++vTZv3uza6gAAAACggnI6bDVu3FiHDx+WJDVv3lzvvfeepF+feF36rS0AAAAAuJ45HbZiYmL0zTffSJImTZqkxYsXq2rVqhozZoxiY2NdXiAAAAAAVEROf2frt44ePaodO3aoSZMmatGihavqKlf4zhYAlC6+swUAKK9M/c7WbwUFBSkoKOjPDgMAAAAAlYpTYauoqEgJCQlatWqVjhw5IovFouDgYD300EN6/PHHZeFxCwAAAABIcmLOlmEY6tWrl4YMGaKffvpJoaGhuuWWW3T06FFFR0frgQcecPrimzdv1n333aeAgABZLBatXr3a4Xh0dLQsFovD1q1bN4c+WVlZ6t+/v7y9veXr66vBgwfrzJkzDn12796tDh06qGrVqgoMDNTcuXOdrhUAAAAAnFHiJ1sJCQnavHmzNmzYoM6dOzsc27hxo6KiovTGG29owIABJb742bNnddttt2nQoEHq3bv37/bp1q2b4uPj7ftWq9XheP/+/XXixAklJiYqPz9fMTExGjZsmFasWCHp13cqIyIiFB4erqVLl2rPnj0aNGiQfH19NWzYsBLXCgAAAADOKPECGREREerSpYsmTZr0u8dfeOEFbdq0SevXr7+2QiwWffjhh4qKirK3RUdHKzs7+7InXsX279+vkJAQffnll2rTpo0kad26dbr33nv1448/KiAgQEuWLNFzzz2n9PR0eXp6Svp1FcXVq1frwIEDJaqNBTIAoHSxQAYAoLxyJhuU+DXC3bt3X/YK36W6d+9uXxLelZKSkuTn56dmzZpp+PDhOnnypP1YSkqKfH197UFLksLDw+Xm5qZt27bZ+3Ts2NEetCQpMjJSBw8e1KlTp373mnl5ecrNzXXYAAAAAMAZJQ5bWVlZ8vf3/8Pj/v7+fxherlW3bt30xhtvaMOGDZozZ442bdqk7t27q7CwUJKUnp4uPz8/h3M8PDxUu3Ztpaen2/v8tu7i/eI+vxUXFycfHx/7FhgY6NL7AgAAAFD5lXjOVmFhoTw8/ri7u7u7CgoKXFJUsT59+tj/HBoaqhYtWujGG29UUlKSunbt6tJrXWry5MkaO3asfT83N5fABQAAAMApJQ5bhmEoOjr6sgUqiuXl5bmsqD/SuHFj1a1bV4cOHVLXrl1ls9mUmZnp0KegoEBZWVmy2WySJJvNpoyMDIc+xfvFfX7LarX+4X0CAAAAQEmUOGwNHDjwqn2cWYnwWvz44486efKk6tevL0kKCwtTdna2duzYodatW0v6dWXEoqIitW3b1t7nueeeU35+vqpUqSJJSkxMVLNmzVSrVi1T6wUAAABw/SrxaoRmOHPmjA4dOiRJuv322zV//nx17txZtWvXVu3atTVjxgw9+OCDstlsSk1N1YQJE3T69Gnt2bPH/uSpe/fuysjI0NKlS+1Lv7dp08a+9HtOTo6aNWumiIgITZw4UXv37tWgQYO0YMGCEi/9zmqEAFC6WI0QAFBeOZMNyjRsJSUlXfbNLunXp2hLlixRVFSUvv76a2VnZysgIEARERGaNWuWw4IXWVlZGjVqlD7++GO5ubnpwQcf1KJFi1SjRg17n927d2vkyJH68ssvVbduXT311FOaOHFiieskbAFA6SJsAQDKqwoTtioKwhYAlC5+mQAA5ZUp39kCAAAAAJQcYQsAAAAATFCisNWqVSv7B4tnzpypc+fOmVoUAAAAAFR0JQpb+/fv19mzZyVJM2bM0JkzZ0wtCgAAAAAquhJ9Z6tly5aKiYlR+/btZRiGXnzxRYfV/i41depUlxYIAAAAABVRiVYjPHjwoKZNm6bU1FTt3LlTISEh8vC4PKdZLBbt3LnTlELLEqsRAkDpYjVCAEB5ZerS725ubkpPT5efn9+fKrIiIWwBQOkibAEAyitnskGJXiO8VFFR0TUXBgAAAADXC6fDliSlpqZq4cKF2r9/vyQpJCREzzzzjG688UaXFgcAAAAAFZXT39lav369QkJCtH37drVo0UItWrTQtm3bdMsttygxMdGMGgEAAACgwnF6ztbtt9+uyMhIzZ4926F90qRJ+vTTT1kgo5QwZwtAZcacLQBAeeVMNnD6ydb+/fs1ePDgy9oHDRqkb7/91tnhAAAAAKBScjps1atXT7t27bqsfdeuXdfVCoUAAAAAcCVOL5AxdOhQDRs2TD/88IPuuusuSdKWLVs0Z84cjR071uUFAgAAAEBF5PScLcMwtHDhQs2bN0/Hjx+XJAUEBCg2NlZPP/20LJVwMhFztgCgdDFnCwBQXpn6UeNLnT59WpJUs2bNax2iQiBsAUDpImwBAMorUz9qfKnKHrIAAAAA4Fo5vUAGAAAAAODqCFsAAAAAYALCFgAAAACYwKmwlZ+fr65du+r77783qx4AAAAAqBScCltVqlTR7t27zaoFAAAAACoNp18jfOyxx7Rs2TIzagEAAACASsPppd8LCgr0+uuv67PPPlPr1q3l5eXlcHz+/PkuKw4AAAAAKiqnw9bevXvVqlUrSdJ3333ncMzCl3YBAAAAQNI1hK3PP//cjDoAAAAAoFK55qXfDx06pPXr1+v8+fOSJMMwXFYUAAAAAFR0ToetkydPqmvXrmratKnuvfdenThxQpI0ePBgjRs3zuUFAgAAAEBF5HTYGjNmjKpUqaK0tDRVr17d3v7oo49q3bp1Li0OAAAAACoqp+dsffrpp1q/fr0aNmzo0H7TTTfp6NGjLisMAAAAACoyp59snT171uGJVrGsrCxZrVaXFAUAAAAAFZ3TYatDhw5644037PsWi0VFRUWaO3euOnfu7NLiAAAAAKCicvo1wrlz56pr16766quvdPHiRU2YMEH79u1TVlaWtmzZYkaNAAAAAFDhOP1k69Zbb9V3332n9u3b6/7779fZs2fVu3dvff3117rxxhvNqBEAAAAAKhyLwQeyrio3N1c+Pj7KycmRt7d3WZcjSbJYyroCADAPv0wAgPLKmWzg9GuEknTq1CktW7ZM+/fvlySFhIQoJiZGtWvXvpbhAAAAAKDScfo1ws2bN6tRo0ZatGiRTp06pVOnTmnRokUKDg7W5s2bzagRAAAAACocp18jDA0NVVhYmJYsWSJ3d3dJUmFhoUaMGKHk5GTt2bPHlELLEq8RAkDp4jVCAEB55Uw2cPrJ1qFDhzRu3Dh70JIkd3d3jR07VocOHXK+WgAAAACohJwOW61atbLP1brU/v37ddttt7mkKAAAAACo6Eq0QMbu3bvtf3766af1zDPP6NChQ2rXrp0kaevWrVq8eLFmz55tTpUAAAAAUMGUaM6Wm5ubLBaLrtbVYrGosLDQZcWVF8zZAoDSxZwtAEB55fKl3w8fPuySwgAAAADgelGisBUUFGR2HQAAAABQqVzTR42PHz+uL774QpmZmSoqKnI49vTTT7ukMAAAAACoyJwOWwkJCXriiSfk6empOnXqyHLJ5CGLxULYAgAAAABdQ9iaMmWKpk6dqsmTJ8vNzemV4wEAAADguuB0Wjp37pz69OlD0AIAAACAK3A6MQ0ePFjvv/++GbUAAAAAQKVRou9sXaqwsFA9e/bU+fPnFRoaqipVqjgcnz9/vksLLA/4zhYAlC6+swUAKK9c/p2tS8XFxWn9+vVq1qyZJF22QAYAAAAA4BrC1rx58/T6668rOjrahHIAAAAAoHJwes6W1WrV3XffbUYtAAAAAFBpOB22nnnmGb388stm1AIAAAAAlYbTrxFu375dGzdu1Jo1a3TLLbdctkDGqlWrXFYcAAAAAFRUToctX19f9e7d24xaAAAAAKDScDpsxcfHm1EHAAAAAFQqTs/ZAgAAAABcndNPtoKDg6/4Pa0ffvjhTxUEAAAAAJWB02Fr9OjRDvv5+fn6+uuvtW7dOsXGxrqqLgAAAACo0JwOW88888zvti9evFhfffXVny4IAAAAACoDl83Z6t69u/71r3+5ajgAAAAAqNBcFrY++OAD1a5d26lzNm/erPvuu08BAQGyWCxavXq1w3HDMDR16lTVr19f1apVU3h4uL7//nuHPllZWerfv7+8vb3l6+urwYMH68yZMw59du/erQ4dOqhq1aoKDAzU3Llzr+keAQAAAKCknH6N8Pbbb3dYIMMwDKWnp+vnn3/WK6+84tRYZ8+e1W233aZBgwb97re75s6dq0WLFmn58uUKDg7WlClTFBkZqW+//VZVq1aVJPXv318nTpxQYmKi8vPzFRMTo2HDhmnFihWSpNzcXEVERCg8PFxLly7Vnj17NGjQIPn6+mrYsGHO3j4AAAAAlIjFMAzDmRNmzJjhsO/m5qZ69eqpU6dOat68+bUXYrHoww8/VFRUlKRfQ1xAQIDGjRun8ePHS5JycnLk7++vhIQE9enTR/v371dISIi+/PJLtWnTRpK0bt063Xvvvfrxxx8VEBCgJUuW6LnnnlN6ero8PT0lSZMmTdLq1at14MCBEtWWm5srHx8f5eTkyNvb+5rv0ZWusCAkAFR4zv0yAQBQepzJBk4/2Zo2bdo1F+aMw4cPKz09XeHh4fY2Hx8ftW3bVikpKerTp49SUlLk6+trD1qSFB4eLjc3N23btk0PPPCAUlJS1LFjR3vQkqTIyEjNmTNHp06dUq1atS67dl5envLy8uz7ubm5Jt0lAAAAgMqq3H7UOD09XZLk7+/v0O7v728/lp6eLj8/P4fjHh4eql27tkOf3xvj0mv8VlxcnHx8fOxbYGDgn78hAAAAANeVEoctNzc3ubu7X3Hz8HD6QVm5NHnyZOXk5Ni3Y8eOlXVJAAAAACqYEqejDz/88A+PpaSkaNGiRSoqKnJJUZJks9kkSRkZGapfv769PSMjQy1btrT3yczMdDivoKBAWVlZ9vNtNpsyMjIc+hTvF/f5LavVKqvV6pL7AAAAAHB9KnHYuv/++y9rO3jwoCZNmqSPP/5Y/fv318yZM11WWHBwsGw2mzZs2GAPV7m5udq2bZuGDx8uSQoLC1N2drZ27Nih1q1bS5I2btyooqIitW3b1t7nueeeU35+vqpUqSJJSkxMVLNmzX53vhYAAAAAuMI1zdk6fvy4hg4dqtDQUBUUFGjXrl1avny5goKCnBrnzJkz2rVrl3bt2iXp10Uxdu3apbS0NFksFo0ePVp//etf9dFHH2nPnj0aMGCAAgIC7CsW3nzzzerWrZuGDh2q7du3a8uWLRo1apT69OmjgIAASVK/fv3k6empwYMHa9++fXr33Xf10ksvaezYsddy6wAAAABQMoYTsrOzjQkTJhjVqlUzwsLCjM2bNztz+mU+//xzQ9Jl28CBAw3DMIyioiJjypQphr+/v2G1Wo2uXbsaBw8edBjj5MmTRt++fY0aNWoY3t7eRkxMjHH69GmHPt98843Rvn17w2q1Gg0aNDBmz57tVJ05OTmGJCMnJ+dP3a8r/bowMhsbG1vl3AAAKK+cyQYl/s7W3LlzNWfOHNlsNr3wwgu/+1phZcV3tgCgdJXslwkAgNLnTDYocdhyc3NTtWrVFB4eLnd39z/st2rVKueqrQAIWwBQughbAIDyypSPGg8YMEAW/gsfAAAAAEqkxGErISHBxDIAAAAAoHK5ptUIAQAAAABXRtgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAE5TrsDV9+nRZLBaHrXnz5vbjFy5c0MiRI1WnTh3VqFFDDz74oDIyMhzGSEtLU48ePVS9enX5+fkpNjZWBQUFpX0rAAAAAK4zHmVdwNXccsst+uyzz+z7Hh7/K3nMmDH65JNP9P7778vHx0ejRo1S7969tWXLFklSYWGhevToIZvNpuTkZJ04cUIDBgxQlSpV9MILL5T6vQAAAAC4fpT7sOXh4SGbzXZZe05OjpYtW6YVK1aoS5cukqT4+HjdfPPN2rp1q9q1a6dPP/1U3377rT777DP5+/urZcuWmjVrliZOnKjp06fL09OztG8HAAAAwHWiXL9GKEnff/+9AgIC1LhxY/Xv319paWmSpB07dig/P1/h4eH2vs2bN9cNN9yglJQUSVJKSopCQ0Pl7+9v7xMZGanc3Fzt27fvD6+Zl5en3Nxchw0AAAAAnFGuw1bbtm2VkJCgdevWacmSJTp8+LA6dOig06dPKz09XZ6envL19XU4x9/fX+np6ZKk9PR0h6BVfLz42B+Ji4uTj4+PfQsMDHTtjQEAAACo9Mr1a4Tdu3e3/7lFixZq27atgoKC9N5776latWqmXXfy5MkaO3asfT83N5fABQAAAMAp5frJ1m/5+vqqadOmOnTokGw2my5evKjs7GyHPhkZGfY5Xjab7bLVCYv3f28eWDGr1Spvb2+HDQAAAACcUaHC1pkzZ5Samqr69eurdevWqlKlijZs2GA/fvDgQaWlpSksLEySFBYWpj179igzM9PeJzExUd7e3goJCSn1+gEAAABcP8r1a4Tjx4/Xfffdp6CgIB0/flzTpk2Tu7u7+vbtKx8fHw0ePFhjx45V7dq15e3traeeekphYWFq166dJCkiIkIhISF6/PHHNXfuXKWnp+v555/XyJEjZbVay/juAAAAAFRm5Tps/fjjj+rbt69OnjypevXqqX379tq6davq1asnSVqwYIHc3Nz04IMPKi8vT5GRkXrllVfs57u7u2vNmjUaPny4wsLC5OXlpYEDB2rmzJlldUsAAAAArhMWwzCMsi6ivMvNzZWPj49ycnLKzfwti6WsKwAA8/DLBAAor5zJBhVqzhYAAAAAVBSELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwAQeZV0AAABwEYulrCsAAPMYRllX4LTr6snW4sWL1ahRI1WtWlVt27bV9u3by7okAAAAAJXUdRO23n33XY0dO1bTpk3Tzp07ddtttykyMlKZmZllXRoAAACASui6CVvz58/X0KFDFRMTo5CQEC1dulTVq1fX66+/XtalAQAAAKiEros5WxcvXtSOHTs0efJke5ubm5vCw8OVkpJyWf+8vDzl5eXZ93NyciRJubm55hcLABD/3AIALlNOfhyKM4FRgjlk10XY+uWXX1RYWCh/f3+Hdn9/fx04cOCy/nFxcZoxY8Zl7YGBgabVCAD4Hx+fsq4AAFDulLMfh9OnT8vnKjVdF2HLWZMnT9bYsWPt+0VFRcrKylKdOnVkYaUnXGdyc3MVGBioY8eOydvbu6zLAQCUE/w+4HplGIZOnz6tgICAq/a9LsJW3bp15e7uroyMDIf2jIwM2Wy2y/pbrVZZrVaHNl9fXzNLBMo9b29vfkwBAJfh9wHXo6s90Sp2XSyQ4enpqdatW2vDhg32tqKiIm3YsEFhYWFlWBkAAACAyuq6eLIlSWPHjtXAgQPVpk0b3XnnnVq4cKHOnj2rmJiYsi4NAAAAQCV03YStRx99VD///LOmTp2q9PR0tWzZUuvWrbts0QwAjqxWq6ZNm3bZq7UAgOsbvw/A1VmMkqxZCAAAAABwynUxZwsAAAAAShthCwAAAABMQNgCAAAAABMQtgCUiaSkJFksFmVnZ5d1KQAAAKYgbAGVQHR0tCwWi2bPnu3Qvnr1alksljKqCgBQ2op/D367devWrUTnd+rUSaNHjza3SOA6QtgCKomqVatqzpw5OnXqlMvGvHjxosvGAgCUjm7duunEiRMO2zvvvOOy8Q3DUEFBgcvGAyozwhZQSYSHh8tmsykuLu4P+/zrX//SLbfcIqvVqkaNGmnevHkOxxs1aqRZs2ZpwIAB8vb21rBhw5SQkCBfX1+tWbNGzZo1U/Xq1fXQQw/p3LlzWr58uRo1aqRatWrp6aefVmFhoX2sN998U23atFHNmjVls9nUr18/ZWZmmnb/AIBfWa1W2Ww2h61WrVpKSkqSp6en/vvf/9r7zp07V35+fsrIyFB0dLQ2bdqkl156yf5E7MiRI/bXvteuXavWrVvLarXqiy++UFFRkeLi4hQcHKxq1arptttu0wcffGAfu/i89evX6/bbb1e1atXUpUsXZWZmau3atbr55pvl7e2tfv366dy5c/bzrjYuUKEYACq8gQMHGvfff7+xatUqo2rVqsaxY8cMwzCMDz/80Cj+n/lXX31luLm5GTNnzjQOHjxoxMfHG9WqVTPi4+Pt4wQFBRne3t7Giy++aBw6dMg4dOiQER8fb1SpUsX4y1/+YuzcudPYtGmTUadOHSMiIsJ45JFHjH379hkff/yx4enpaaxcudI+1rJly4z//Oc/RmpqqpGSkmKEhYUZ3bt3tx///PPPDUnGqVOnSuXvCACuB8W/B38kNjbWCAoKMrKzs42dO3canp6exr///W/DMAwjOzvbCAsLM4YOHWqcOHHCOHHihFFQUGD/97pFixbGp59+ahw6dMg4efKk8de//tVo3ry5sW7dOiM1NdWIj483rFarkZSUZBjG//6db9eunfHFF18YO3fuNJo0aWLcc889RkREhLFz505j8+bNRp06dYzZs2fba7zauEBFQtgCKoFLf1zbtWtnDBo0yDAMx7DVr18/4y9/+YvDebGxsUZISIh9PygoyIiKinLoEx8fb0gyDh06ZG974oknjOrVqxunT5+2t0VGRhpPPPHEH9b45ZdfGpLs5xC2AMD1Bg4caLi7uxteXl4O29/+9jfDMAwjLy/PaNmypfHII48YISEhxtChQx3Ov+eee4xnnnnGoa343+vVq1fb2y5cuGBUr17dSE5Odug7ePBgo2/fvg7nffbZZ/bjcXFxhiQjNTXV3vbEE08YkZGRJR4XqEg8yuiBGgCTzJkzR126dNH48eMd2vfv36/777/foe3uu+/WwoULVVhYKHd3d0lSmzZtLhuzevXquvHGG+37/v7+atSokWrUqOHQdulrgjt27ND06dP1zTff6NSpUyoqKpIkpaWlKSQk5M/fKADgd3Xu3FlLlixxaKtdu7YkydPTU2+//bZatGihoKAgLViwoMTjXvr7cOjQIZ07d05/+ctfHPpcvHhRt99+u0NbixYt7H/29/dX9erV1bhxY4e27du3Oz0uUBEQtoBKpmPHjoqMjNTkyZMVHR3t9PleXl6XtVWpUsVh32Kx/G5bcaA6e/asIiMjFRkZqbffflv16tVTWlqaIiMjWXQDAEzm5eWlJk2a/OHx5ORkSVJWVpaysrJ+99/9Pxq32JkzZyRJn3zyiRo0aODQz2q1Ouxf+ntxtd8PZ8YFKgLCFlAJzZ49Wy1btlSzZs3sbTfffLO2bNni0G/Lli1q2rSp/amWqxw4cEAnT57U7NmzFRgYKEn66quvXHoNAIDzUlNTNWbMGP3zn//Uu+++q4EDB+qzzz6Tm9uva6Z5eno6LHb0R0JCQmS1WpWWlqZ77rnHZfWZNS5QVghbQCUUGhqq/v37a9GiRfa2cePG6Y477tCsWbP06KOPKiUlRf/4xz/0yiuvuPz6N9xwgzw9PfXyyy/rySef1N69ezVr1iyXXwcAcLm8vDylp6c7tHl4eKhWrVp67LHHFBkZqZiYGHXr1k2hoaGaN2+eYmNjJf26Ku22bdt05MgR1ahRw/764W/VrFlT48eP15gxY1RUVKT27dsrJydHW7Zskbe3twYOHHhNtZs1LlBWCFtAJTVz5ky9++679v1WrVrpvffe09SpUzVr1izVr19fM2fOvKZXDa+mXr16SkhI0LPPPqtFixapVatWevHFF9WrVy+XXwsA4GjdunWqX7++Q1uzZs3Ur18/HT16VGvWrJEk1a9fX6+++qr69u2riIgI3XbbbRo/frwGDhyokJAQnT9/XocPH/7D68yaNUv16tVTXFycfvjhB/n6+qpVq1Z69tln/1T9Zo0LlAWLYRhGWRcBAAAAAJUNHzUGAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAVBqdOnXS6NGjnTrnwIEDateunapWraqWLVuW6Jzp06c79I2OjlZUVJRT1wUAVH6ELQBAmYuOjpbFYpHFYlGVKlUUHBysCRMm6MKFC06Ns2rVKs2aNcupc6ZNmyYvLy8dPHhQGzZscOrckrj03n5va9SokcuvCQAoHzzKugAAACSpW7duio+PV35+vnbs2KGBAwfKYrFozpw5JR6jdu3aTl83NTVVPXr0UFBQkNPnlsRLL72k2bNn2/fr16+v+Ph4devWTZLk7u5uynUBAGWPJ1sAgHLBarXKZrMpMDBQUVFRCg8PV2Jiov34yZMn1bdvXzVo0EDVq1dXaGio3nnnHYcxfvsaYaNGjfTCCy9o0KBBqlmzpm644Qa9+uqr9uMWi0U7duzQzJkzZbFYNH36dEnSxIkT1bRpU1WvXl2NGzfWlClTlJ+ff0335ePjI5vNZt8kydfXVzabTc8++6xiYmIc+ufn58vPz0/Lli2z39OoUaM0atQo+fj4qG7dupoyZYoMw7Cfk5eXp/Hjx6tBgwby8vJS27ZtlZSUdE31AgBch7AFACh39u7dq+TkZHl6etrbLly4oNatW+uTTz7R3r17NWzYMD3++OPavn37FceaN2+e2rRpo6+//lojRozQ8OHDdfDgQUnSiRMndMstt2jcuHE6ceKExo8fL0mqWbOmEhIS9O233+qll17SP//5Ty1YsMDl9zlkyBCtW7dOJ06csLetWbNG586d06OPPmpvW758uTw8PLR9+3a99NJLmj9/vl577TX78VGjRiklJUUrV67U7t279fDDD6tbt276/vvvXV4zAKDkCFsAgHJhzZo1qlGjhqpWrarQ0FBlZmYqNjbWfrxBgwYaP368WrZsqcaNG+upp55St27d9N57711x3HvvvVcjRoxQkyZNNHHiRNWtW1eff/65JMlms8nDw0M1atSQzWZTjRo1JEnPP/+87rrrLjVq1Ej33Xefxo8ff9XrXIu77rpLzZo105tvvmlvi4+P18MPP2yvRZICAwO1YMECNWvWTP3799dTTz1lD39paWmKj4/X+++/rw4dOujGG2/U+PHj1b59e8XHx7u8ZgBAyTFnCwBQLnTu3FlLlizR2bNntWDBAnl4eOjBBx+0Hy8sLNQLL7yg9957Tz/99JMuXryovLw8Va9e/YrjtmjRwv5ni8Uim82mzMzMK57z7rvvatGiRUpNTdWZM2dUUFAgb2/vP3eDf2DIkCF69dVXNWHCBGVkZGjt2rXauHGjQ5927drJYrHY98PCwjRv3jwVFhZqz549KiwsVNOmTR3OycvLU506dUypGQBQMoQtAEC54OXlpSZNmkiSXn/9dd12221atmyZBg8eLEn6+9//rpdeekkLFy5UaGiovLy8NHr0aF28ePGK41apUsVh32KxqKio6A/7p6SkqH///poxY4YiIyPl4+OjlStXat68eX/yDn/fgAEDNGnSJKWkpCg5OVnBwcHq0KFDic8/c+aM3N3dtWPHjssW27j06RgAoPQRtgAA5Y6bm5ueffZZjR07Vv369VO1atW0ZcsW3X///XrsscckSUVFRfruu+8UEhLi0msnJycrKChIzz33nL3t6NGjLr3GperUqaOoqCjFx8crJSXlsgUzJGnbtm0O+1u3btVNN90kd3d33X777SosLFRmZqZTIQ0AYD7mbAEAyqWHH35Y7u7uWrx4sSTppptuUmJiopKTk7V//3498cQTysjIcPl1b7rpJqWlpWnlypVKTU3VokWL9OGHH7r8OpcaMmSIli9frv3792vgwIGXHU9LS9PYsWN18OBBvfPOO3r55Zf1zDPPSJKaNm2q/v37a8CAAVq1apUOHz6s7du3Ky4uTp988ompdQMAroywBQAolzw8PDRq1CjNnTtXZ8+e1fPPP69WrVopMjJSnTp1ks1mU1RUlMuv26tXL40ZM0ajRo1Sy5YtlZycrClTprj8OpcKDw9X/fr1FRkZqYCAgMuODxgwQOfPn9edd96pkSNH6plnntGwYcPsx+Pj4zVgwACNGzdOzZo1U1RUlL788kvdcMMNptYNALgyi3HphzoAAECpO3PmjBo0aKD4+Hj17t3b4VinTp3UsmVLLVy4sGyKAwBcM+ZsAQBQRoqKivTLL79o3rx58vX1Va9evcq6JACACxG2AAAoI2lpaQoODlbDhg2VkJAgDw9+lgGgMuE1QgAAAAAwAQtkAAAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAm+H8VFBpJC5HweQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 5ms/step - loss: 0.2926 - val_loss: 0.2694\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1817 - val_loss: 0.2110\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1536 - val_loss: 0.1987\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1462 - val_loss: 0.1943\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1430 - val_loss: 0.1909\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1406 - val_loss: 0.1892\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1386 - val_loss: 0.1858\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1365 - val_loss: 0.1847\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1347 - val_loss: 0.1817\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 0.1805\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1317 - val_loss: 0.1804\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1309 - val_loss: 0.1770\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1298 - val_loss: 0.1775\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1293 - val_loss: 0.1760\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1293 - val_loss: 0.1754\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1284 - val_loss: 0.1749\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1282 - val_loss: 0.1741\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1278 - val_loss: 0.1746\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1280 - val_loss: 0.1731\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1273 - val_loss: 0.1733\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1271 - val_loss: 0.1723\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1268 - val_loss: 0.1720\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1268 - val_loss: 0.1717\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1264 - val_loss: 0.1720\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1264 - val_loss: 0.1720\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1262 - val_loss: 0.1719\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1260 - val_loss: 0.1713\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1258 - val_loss: 0.1708\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1256 - val_loss: 0.1708\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1255 - val_loss: 0.1710\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1253 - val_loss: 0.1706\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1253 - val_loss: 0.1704\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1250 - val_loss: 0.1701\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1250 - val_loss: 0.1701\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1250 - val_loss: 0.1696\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1251 - val_loss: 0.1697\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1247 - val_loss: 0.1707\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1250 - val_loss: 0.1702\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.1699\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.1690\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.1698\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1242 - val_loss: 0.1700\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1240 - val_loss: 0.1683\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1243 - val_loss: 0.1701\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.1697\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1241 - val_loss: 0.1694\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.1694\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1237 - val_loss: 0.1698\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1236 - val_loss: 0.1687\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1240 - val_loss: 0.1695\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1243 - val_loss: 0.1704\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1237 - val_loss: 0.1682\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1236 - val_loss: 0.1680\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1234 - val_loss: 0.1684\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1233 - val_loss: 0.1679\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1230 - val_loss: 0.1675\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1230 - val_loss: 0.1693\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1230 - val_loss: 0.1689\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1231 - val_loss: 0.1694\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1226 - val_loss: 0.1679\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1226 - val_loss: 0.1668\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1222 - val_loss: 0.1675\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1224 - val_loss: 0.1670\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1220 - val_loss: 0.1664\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1219 - val_loss: 0.1684\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1221 - val_loss: 0.1678\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1219 - val_loss: 0.1662\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1213 - val_loss: 0.1690\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1217 - val_loss: 0.1660\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1212 - val_loss: 0.1660\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1215 - val_loss: 0.1657\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1207 - val_loss: 0.1649\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1202 - val_loss: 0.1645\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.1642\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1194 - val_loss: 0.1644\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1192 - val_loss: 0.1637\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1188 - val_loss: 0.1638\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1189 - val_loss: 0.1638\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1183 - val_loss: 0.1631\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1170 - val_loss: 0.1614\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1167 - val_loss: 0.1611\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1161 - val_loss: 0.1606\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1155 - val_loss: 0.1591\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1148 - val_loss: 0.1581\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1148 - val_loss: 0.1587\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1141 - val_loss: 0.1571\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1135 - val_loss: 0.1575\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1118 - val_loss: 0.1542\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1109 - val_loss: 0.1532\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1094 - val_loss: 0.1513\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1080 - val_loss: 0.1515\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1061 - val_loss: 0.1471\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1046 - val_loss: 0.1440\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1031 - val_loss: 0.1451\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1009 - val_loss: 0.1378\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0980 - val_loss: 0.1330\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0928 - val_loss: 0.1250\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0857 - val_loss: 0.1169\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0785 - val_loss: 0.1060\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0721 - val_loss: 0.0971\n",
      "81/81 [==============================] - 0s 775us/step\n",
      "predicted data = [[ 0.0543355  -0.19596434 -0.5190049 ]\n",
      " [ 0.7517345   0.31656232  0.4574989 ]\n",
      " [ 0.33034405 -0.28163007 -0.39969027]\n",
      " ...\n",
      " [-1.383641   -0.53153086 -0.92896914]\n",
      " [-1.4061692  -0.5361012  -0.8771279 ]\n",
      " [-1.6263257  -0.29546562  0.89343715]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 30]\n",
    "extreme_data = data[data['rainfall'] > 30]\n",
    "\n",
    "# Plot the normal and extreme rainfall data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar('Normal', len(normal_data), color='blue')\n",
    "plt.bar('Extreme', len(extreme_data), color='red')\n",
    "plt.title('Rainfall Data')\n",
    "plt.xlabel('Rainfall Type')\n",
    "plt.ylabel('Number of Days')\n",
    "plt.show()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(normal_data[['windspeed', 'tpw', 'rainfall']].values)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu')(input_layer)\n",
    "encoded = Dense(8, activation='relu')(encoded)\n",
    "encoded = Dense(4, activation='relu')(encoded)\n",
    "encoded = Dense(2, activation='relu')(encoded)\n",
    "decoded = Dense(4, activation='relu')(encoded)\n",
    "decoded = Dense(8, activation='relu')(decoded)\n",
    "decoded = Dense(16, activation='relu')(decoded)\n",
    "decoded = Dense(3, activation=None)(decoded)\n",
    "\n",
    "# Create the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='logcosh')\n",
    "\n",
    "# Train the model\n",
    "history = autoencoder.fit(train_data, train_data, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = scaler.transform(data[['windspeed', 'tpw', 'rainfall']].values)\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "print(\"predicted data = {}\".format(predicted_data))\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 9.699999999999998\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Save the predicted rainfall data to a CSV file\n",
    "data.to_csv('predicted_rainfall.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4a704643",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df=pd.read_csv(\"predicted_rainfall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8664d263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7/12/2000</td>\n",
       "      <td>23.003010</td>\n",
       "      <td>0.016358</td>\n",
       "      <td>33.846075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>8/4/2004</td>\n",
       "      <td>21.769238</td>\n",
       "      <td>0.043448</td>\n",
       "      <td>44.404364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>6/23/2007</td>\n",
       "      <td>23.908970</td>\n",
       "      <td>0.331606</td>\n",
       "      <td>49.356381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>7/17/2007</td>\n",
       "      <td>19.211468</td>\n",
       "      <td>0.030241</td>\n",
       "      <td>32.941690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>8/6/2007</td>\n",
       "      <td>17.366877</td>\n",
       "      <td>0.022628</td>\n",
       "      <td>31.609187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>7/28/2008</td>\n",
       "      <td>20.585272</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>32.559895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>7/16/2009</td>\n",
       "      <td>21.342918</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>38.754120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>7/17/2009</td>\n",
       "      <td>21.451813</td>\n",
       "      <td>0.105873</td>\n",
       "      <td>39.511546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>7/18/2009</td>\n",
       "      <td>16.981620</td>\n",
       "      <td>0.104322</td>\n",
       "      <td>38.342482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>6/2/2011</td>\n",
       "      <td>12.654827</td>\n",
       "      <td>0.550524</td>\n",
       "      <td>34.993438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>7/17/2011</td>\n",
       "      <td>19.214113</td>\n",
       "      <td>0.057870</td>\n",
       "      <td>45.056388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>8/7/2011</td>\n",
       "      <td>15.895408</td>\n",
       "      <td>0.019959</td>\n",
       "      <td>31.262259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>9/3/2011</td>\n",
       "      <td>17.204716</td>\n",
       "      <td>0.025290</td>\n",
       "      <td>32.701293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>6/18/2012</td>\n",
       "      <td>16.490360</td>\n",
       "      <td>0.060908</td>\n",
       "      <td>47.977786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>6/19/2012</td>\n",
       "      <td>12.745975</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>36.064092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>8/7/2012</td>\n",
       "      <td>11.919377</td>\n",
       "      <td>0.026516</td>\n",
       "      <td>33.012284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>6/1/2013</td>\n",
       "      <td>11.123766</td>\n",
       "      <td>0.174906</td>\n",
       "      <td>30.608754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>7/4/2013</td>\n",
       "      <td>16.945307</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>34.218388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>8/2/2013</td>\n",
       "      <td>20.746782</td>\n",
       "      <td>0.006965</td>\n",
       "      <td>30.364948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>8/1/2014</td>\n",
       "      <td>20.457104</td>\n",
       "      <td>0.029008</td>\n",
       "      <td>35.467946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>6/29/2016</td>\n",
       "      <td>21.279696</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>30.296499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>8/15/2018</td>\n",
       "      <td>21.421936</td>\n",
       "      <td>0.086819</td>\n",
       "      <td>41.892355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>8/16/2018</td>\n",
       "      <td>22.673685</td>\n",
       "      <td>0.060061</td>\n",
       "      <td>47.362358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>8/6/2019</td>\n",
       "      <td>18.526226</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>30.351481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>8/7/2019</td>\n",
       "      <td>21.010150</td>\n",
       "      <td>0.027716</td>\n",
       "      <td>44.681781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>8/8/2019</td>\n",
       "      <td>22.589184</td>\n",
       "      <td>0.024273</td>\n",
       "      <td>47.165503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>21.575483</td>\n",
       "      <td>0.042829</td>\n",
       "      <td>80.636074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>8/10/2019</td>\n",
       "      <td>20.276800</td>\n",
       "      <td>0.043114</td>\n",
       "      <td>58.431061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>8/11/2019</td>\n",
       "      <td>17.905354</td>\n",
       "      <td>0.011177</td>\n",
       "      <td>30.734900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>8/5/2020</td>\n",
       "      <td>27.123528</td>\n",
       "      <td>0.037294</td>\n",
       "      <td>32.042267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>8/7/2020</td>\n",
       "      <td>19.529327</td>\n",
       "      <td>0.108583</td>\n",
       "      <td>42.017340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>8/8/2020</td>\n",
       "      <td>17.372940</td>\n",
       "      <td>0.220405</td>\n",
       "      <td>35.905335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>9/20/2020</td>\n",
       "      <td>18.062815</td>\n",
       "      <td>0.326716</td>\n",
       "      <td>34.073259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall\n",
       "41    7/12/2000  23.003010  0.016358  33.846075\n",
       "552    8/4/2004  21.769238  0.043448  44.404364\n",
       "876   6/23/2007  23.908970  0.331606  49.356381\n",
       "900   7/17/2007  19.211468  0.030241  32.941690\n",
       "920    8/6/2007  17.366877  0.022628  31.609187\n",
       "1033  7/28/2008  20.585272  0.014900  32.559895\n",
       "1143  7/16/2009  21.342918  0.037500  38.754120\n",
       "1144  7/17/2009  21.451813  0.105873  39.511546\n",
       "1145  7/18/2009  16.981620  0.104322  38.342482\n",
       "1343   6/2/2011  12.654827  0.550524  34.993438\n",
       "1388  7/17/2011  19.214113  0.057870  45.056388\n",
       "1409   8/7/2011  15.895408  0.019959  31.262259\n",
       "1436   9/3/2011  17.204716  0.025290  32.701293\n",
       "1481  6/18/2012  16.490360  0.060908  47.977786\n",
       "1482  6/19/2012  12.745975  0.004308  36.064092\n",
       "1531   8/7/2012  11.919377  0.026516  33.012284\n",
       "1586   6/1/2013  11.123766  0.174906  30.608754\n",
       "1619   7/4/2013  16.945307  0.008078  34.218388\n",
       "1648   8/2/2013  20.746782  0.006965  30.364948\n",
       "1769   8/1/2014  20.457104  0.029008  35.467946\n",
       "1980  6/29/2016  21.279696  0.040968  30.296499\n",
       "2271  8/15/2018  21.421936  0.086819  41.892355\n",
       "2272  8/16/2018  22.673685  0.060061  47.362358\n",
       "2384   8/6/2019  18.526226  0.027500  30.351481\n",
       "2385   8/7/2019  21.010150  0.027716  44.681781\n",
       "2386   8/8/2019  22.589184  0.024273  47.165503\n",
       "2387   8/9/2019  21.575483  0.042829  80.636074\n",
       "2388  8/10/2019  20.276800  0.043114  58.431061\n",
       "2389  8/11/2019  17.905354  0.011177  30.734900\n",
       "2505   8/5/2020  27.123528  0.037294  32.042267\n",
       "2507   8/7/2020  19.529327  0.108583  42.017340\n",
       "2508   8/8/2020  17.372940  0.220405  35.905335\n",
       "2551  9/20/2020  18.062815  0.326716  34.073259"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be60bbc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpre_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactuall_rainfall_class\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrainfall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m threshold, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtreme\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pre_df' is not defined"
     ]
    }
   ],
   "source": [
    "threshold=30\n",
    "pre_df['actuall_rainfall_class'] = np.where(data['rainfall'] <= threshold, 'Normal', 'Extreme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d668381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>predicted_rainfall</th>\n",
       "      <th>error</th>\n",
       "      <th>rainfall_class</th>\n",
       "      <th>actuall_rainfall_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/1/2000</td>\n",
       "      <td>12.245595</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>4.688589</td>\n",
       "      <td>-0.027577</td>\n",
       "      <td>4.716165</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/2/2000</td>\n",
       "      <td>12.825491</td>\n",
       "      <td>0.044874</td>\n",
       "      <td>11.330589</td>\n",
       "      <td>-0.027577</td>\n",
       "      <td>11.358166</td>\n",
       "      <td>Extreme</td>\n",
       "      <td>Extreme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/3/2000</td>\n",
       "      <td>12.921664</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>6.894713</td>\n",
       "      <td>-0.027577</td>\n",
       "      <td>6.922290</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/4/2000</td>\n",
       "      <td>15.149001</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>4.613324</td>\n",
       "      <td>-0.027577</td>\n",
       "      <td>4.640901</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/5/2000</td>\n",
       "      <td>18.495907</td>\n",
       "      <td>0.139491</td>\n",
       "      <td>8.111635</td>\n",
       "      <td>-0.027577</td>\n",
       "      <td>8.139212</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>9/26/2020</td>\n",
       "      <td>5.577215</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>4.467977</td>\n",
       "      <td>-0.027577</td>\n",
       "      <td>4.495553</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>9/27/2020</td>\n",
       "      <td>5.184293</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>1.572182</td>\n",
       "      <td>-0.027577</td>\n",
       "      <td>1.599758</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>9/28/2020</td>\n",
       "      <td>4.469007</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>1.819019</td>\n",
       "      <td>-0.027577</td>\n",
       "      <td>1.846596</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>9/29/2020</td>\n",
       "      <td>4.259090</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>2.198017</td>\n",
       "      <td>-0.027577</td>\n",
       "      <td>2.225594</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>9/30/2020</td>\n",
       "      <td>5.513838</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>12.713634</td>\n",
       "      <td>-0.027577</td>\n",
       "      <td>12.741210</td>\n",
       "      <td>Extreme</td>\n",
       "      <td>Extreme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2562 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall  predicted_rainfall  \\\n",
       "0      6/1/2000  12.245595  0.033285   4.688589           -0.027577   \n",
       "1      6/2/2000  12.825491  0.044874  11.330589           -0.027577   \n",
       "2      6/3/2000  12.921664  0.010243   6.894713           -0.027577   \n",
       "3      6/4/2000  15.149001  0.036881   4.613324           -0.027577   \n",
       "4      6/5/2000  18.495907  0.139491   8.111635           -0.027577   \n",
       "...         ...        ...       ...        ...                 ...   \n",
       "2557  9/26/2020   5.577215  0.009963   4.467977           -0.027577   \n",
       "2558  9/27/2020   5.184293  0.002341   1.572182           -0.027577   \n",
       "2559  9/28/2020   4.469007  0.000867   1.819019           -0.027577   \n",
       "2560  9/29/2020   4.259090  0.001416   2.198017           -0.027577   \n",
       "2561  9/30/2020   5.513838  0.002474  12.713634           -0.027577   \n",
       "\n",
       "          error rainfall_class actuall_rainfall_class  \n",
       "0      4.716165         Normal                 Normal  \n",
       "1     11.358166        Extreme                Extreme  \n",
       "2      6.922290         Normal                 Normal  \n",
       "3      4.640901         Normal                 Normal  \n",
       "4      8.139212         Normal                 Normal  \n",
       "...         ...            ...                    ...  \n",
       "2557   4.495553         Normal                 Normal  \n",
       "2558   1.599758         Normal                 Normal  \n",
       "2559   1.846596         Normal                 Normal  \n",
       "2560   2.225594         Normal                 Normal  \n",
       "2561  12.741210        Extreme                Extreme  \n",
       "\n",
       "[2562 rows x 8 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2fcf32ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 9.699999999999998\n",
      "Lowest error: 0.023419203747072626\n"
     ]
    }
   ],
   "source": [
    "# Define a range of threshold values to try\n",
    "thresholds = np.arange(0.5, 10.0, 0.1)\n",
    "\n",
    "# Initialize variables to store the best threshold and the lowest error\n",
    "best_threshold = 0.0\n",
    "lowest_error = float('inf')\n",
    "\n",
    "# Loop over the threshold values and calculate the error for each\n",
    "for threshold in thresholds:\n",
    "    # Classify the rainfall data into normal and extreme based on the threshold\n",
    "    pre_df['rainfall_class'] = np.where(data['error'] > threshold, 'extreme', 'normal')\n",
    "\n",
    "    # Calculate the accuracy of the classification\n",
    "    accuracy = sum(pre_df['rainfall_class'] == pre_df['actuall_rainfall_class']) / len(data)\n",
    "\n",
    "    # Calculate the error of the classification\n",
    "    error = 1 - accuracy\n",
    "\n",
    "    # Update the best threshold and lowest error if necessary\n",
    "    if error < lowest_error:\n",
    "        best_threshold = threshold\n",
    "        lowest_error = error\n",
    "\n",
    "# Print the best threshold and lowest error\n",
    "print('Best threshold:', best_threshold)\n",
    "print('Lowest error:', lowest_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d7d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e28965eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shine\\AppData\\Local\\Temp\\ipykernel_8336\\272855910.py:41: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=50, batch_size=16, verbose=0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 45>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Grid search with custom scoring function\u001b[39;00m\n\u001b[0;32m     44\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mkeras_reg, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39mmake_scorer(custom_score))\n\u001b[1;32m---> 45\u001b[0m grid_result \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Print best hyperparameters\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py:164\u001b[0m, in \u001b[0;36mBaseWrapper.fit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m fit_args \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_sk_params(Sequential\u001b[38;5;241m.\u001b[39mfit))\n\u001b[0;32m    162\u001b[0m fit_args\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m--> 164\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_args)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\keras\\engine\\training.py:1413\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1411\u001b[0m   context\u001b[38;5;241m.\u001b[39masync_wait()\n\u001b[0;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m-> 1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m \u001b[43mdata_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_increment\u001b[49m\n\u001b[0;32m   1414\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\keras\\engine\\data_adapter.py:1268\u001b[0m, in \u001b[0;36mDataHandler.step_increment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1265\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m steps_remaining\n\u001b[0;32m   1266\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution\u001b[38;5;241m.\u001b[39massign(original_spe)\n\u001b[1;32m-> 1268\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_increment\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1270\u001b[0m   \u001b[38;5;124;03m\"\"\"The number to increment the step for `on_batch_end` methods.\"\"\"\u001b[39;00m\n\u001b[0;32m   1271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_increment\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['windspeed', 'tpw', 'rainfall']], data[['windspeed', 'tpw', 'rainfall']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom scoring function using mean squared error\n",
    "def custom_score(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return -mse  # negative because GridSearchCV maximizes score by default\n",
    "\n",
    "# Define model\n",
    "def create_model(activation='relu', loss='mse', num_layers=1, layer_size=16, input_size=3):\n",
    "    model = Sequential()\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(layer_size, activation=activation, input_shape=(input_size,)))\n",
    "        else:\n",
    "            model.add(Dense(layer_size, activation=activation))\n",
    "    model.add(Dense(input_size))\n",
    "    model.compile(optimizer='adam', loss=loss)\n",
    "    return model\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "    'loss': ['mse', 'mae', 'logcosh'],\n",
    "    'num_layers': [1, 2, 3],\n",
    "    'layer_size': [8, 16, 32]\n",
    "}\n",
    "\n",
    "# Create model object for KerasRegressor\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "# Grid search with custom scoring function\n",
    "grid_search = GridSearchCV(estimator=keras_reg, param_grid=param_grid, cv=5, scoring=make_scorer(custom_score))\n",
    "grid_result = grid_search.fit(X_train, X_train)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b423af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation': 'relu', 'layer_size': 16, 'loss': 'logcosh', 'num_layers': 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e3500ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall  predicted_rainfall  \\\n",
      "9     6/10/2000  13.513184  0.013990  10.267096           -0.027577   \n",
      "20    6/21/2000  11.903351  0.006026  10.733826           -0.027577   \n",
      "29    6/30/2000  16.988686  0.027998  10.824606           -0.027577   \n",
      "34     7/5/2000  18.936674  0.106010   9.676520           -0.027577   \n",
      "62     8/2/2000   7.451790  0.002032   9.756014           -0.027577   \n",
      "...         ...        ...       ...        ...                 ...   \n",
      "2457  6/18/2020  17.901005  0.013002  10.880255           -0.027577   \n",
      "2474   7/5/2020  16.509890  0.008430  10.325798           -0.027577   \n",
      "2518  8/18/2020  12.669357  0.009606  10.173612           -0.027577   \n",
      "2545  9/14/2020  11.074582  0.009636  10.598657           -0.027577   \n",
      "2549  9/18/2020  12.638571  0.069490  10.089147           -0.027577   \n",
      "\n",
      "          error rainfall_class actuall_rainfall_class  \n",
      "9     10.294673        Extreme                 Normal  \n",
      "20    10.761403        Extreme                 Normal  \n",
      "29    10.852183        Extreme                 Normal  \n",
      "34     9.704096        Extreme                 Normal  \n",
      "62     9.783590        Extreme                 Normal  \n",
      "...         ...            ...                    ...  \n",
      "2457  10.907832        Extreme                 Normal  \n",
      "2474  10.353374        Extreme                 Normal  \n",
      "2518  10.201189        Extreme                 Normal  \n",
      "2545  10.626234        Extreme                 Normal  \n",
      "2549  10.116724        Extreme                 Normal  \n",
      "\n",
      "[126 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = pre_df[pre_df['rainfall_class'] != pre_df['actuall_rainfall_class']]\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "337c4779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAHWCAYAAACBjZMqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAABGUUlEQVR4nO3deVxWdf7//+cFCCoKrnCJEaCWheFaKU2aliOaaY6Vk0uCeyqWkkY2uc+ES66N6ddJoSaXljGnbDJxZRLS0hjSlJJUagRsXLhCE1nO749+XB+vcOGy68j2uN9u59Z13ud93ud1uN3y3J63c877WAzDMAQAAAAAcCm38i4AAAAAAKoiwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgCgUrJYLJo5c+YN7VtYWKjnn39egYGBcnNzU79+/ZzaPzg4WFFRUfb1Xbt2yWKxaNeuXTdUDwCgaiJsAQBMlZCQIIvFYl88PDzUtGlTRUVF6b///W+51LRmzRotWLBAjz/+uN544w1NmjTpphz313+LmjVrKiAgQBEREVq2bJl++umnGx47OTlZM2fO1Llz51xXMADgN/Eo7wIAANXD7NmzFRISoosXL+qzzz5TQkKCPv30Ux08eFA1a9Z0eryff/5ZHh43dhnbsWOHmjZtqsWLF9/Q/r9Vyd+ioKBA2dnZ2rVrlyZOnKhFixbpgw8+UOvWrZ0eMzk5WbNmzVJUVJTq1avn+qIBAE4jbAEAbopevXrp7rvvliSNHDlSjRo10rx58/TBBx9owIABTo93IwGtxKlTp8o1kFz+t5CkqVOnaseOHXrkkUfUt29fHT58WLVq1Sq3+gAArsFjhACActG5c2dJUkZGhr3t0qVLmj59ujp06CBfX195e3urc+fO2rlzZ6n9f/3O1syZM2WxWHT06FH73R1fX18NGzZMFy5ckCQdP35cFotFO3fu1KFDh+yP85W8a/XKK6/ovvvuU8OGDVWrVi116NBB7733nnl/hMs8+OCDmjZtmk6cOKG33nrL3p6WlqaoqCg1a9ZMNWvWlNVq1fDhw3X69GmHc58yZYokKSQkxH5ex48flyTFx8frwQcflJ+fn7y8vBQaGqoVK1bclPMCgOqMO1sAgHJREgTq169vb7PZbHr99dc1cOBAjRo1Sj/99JNWr16tiIgI7du3T23btr3uuAMGDFBISIji4uJ04MABvf766/Lz89O8efPUuHFj/f3vf9df/vIX5eXlKS4uTpJ05513SpKWLl2qvn37avDgwbp06ZI2bNigJ554Qps3b1bv3r1d/jf4taeeekovvviitm7dqlGjRkmSEhMT9d1332nYsGGyWq06dOiQVq1apUOHDumzzz6TxWJR//799c0332j9+vVavHixGjVqJElq3LixJGnFihVq1aqV+vbtKw8PD3344YcaN26ciouLNX78eNPPCwCqK8IWAOCmyM3N1f/+9z9dvHhRe/fu1axZs+Tl5aVHHnnE3qd+/fo6fvy4PD097W2jRo3SHXfcoVdffVWrV6++7nHatWvn0O/06dNavXq15s2bJ29vbw0ZMkSvv/663N3dNWTIEId9v/nmG4fH96Kjo9W+fXstWrTopoStW265Rb6+vg53+8aNG6fnnnvOoV+nTp00cOBAffrpp+rcubNat26t9u3ba/369erXr5+Cg4Md+u/evbvUefXs2VOLFi0ibAGAiXiMEABwU3Tv3l2NGzdWYGCgHn/8cXl7e+uDDz7QLbfcYu/j7u5uD1rFxcU6c+aMCgsLdffdd+vAgQNlOs7TTz/tsN65c2edPn1aNpvtuvteHkjOnj2r3Nxcde7cuczHdoU6deo4zEp4eU0XL17U//73P3Xq1EmSylzX5WOUhN4HHnhA3333nXJzc11UOQDg17izBQC4KZYvX67bb79dubm5WrNmjZKSkuTl5VWq3xtvvKGFCxfqyJEjKigosLeHhISU6Ti33nqrw3rJY4pnz56Vj4/PNffdvHmz/vznPys1NVX5+fn2dovFUqZju0JeXp78/Pzs62fOnNGsWbO0YcMGnTp1yqFvWYPSnj17NGPGDKWkpNjfX7t8DF9f399eOACgFMIWAOCmuPfee+0z8PXr10/333+/Bg0apPT0dNWpU0eS9NZbbykqKkr9+vXTlClT5OfnJ3d3d8XFxTk8Wnct7u7uV2w3DOOa+/373/9W37591aVLF7322mtq0qSJatSoofj4eK1bt86JM71xP/zwg3Jzc9WiRQt724ABA5ScnKwpU6aobdu2qlOnjoqLi9WzZ08VFxdfd8yMjAw99NBDuuOOO7Ro0SIFBgbK09NT//rXv7R48eIyjQEAuDGELQDATVcSoLp166a//vWveuGFFyRJ7733npo1a6aNGzc63E2aMWOG6TX94x//UM2aNfXJJ5843HGLj483/dgl/v73v0uSIiIiJP1yN2779u2aNWuWpk+fbu/37bffltr3anffPvzwQ+Xn5+uDDz5wuOt3pRkeAQCuxTtbAIBy0bVrV917771asmSJLl68KOn/7kpdfhdq7969SklJMb0ed3d3WSwWFRUV2duOHz+uTZs2mX5s6ZcPLc+ZM0chISEaPHiwvSap9F25JUuWlNrf29tbknTu3DmH9iuNkZube1NDJABUV9zZAgCUmylTpuiJJ55QQkKCnn76aT3yyCPauHGj/vCHP6h37946duyYVq5cqdDQUOXl5ZlaS+/evbVo0SL17NlTgwYN0qlTp7R8+XK1aNFCaWlpLj3Wxx9/rCNHjqiwsFA5OTnasWOHEhMTFRQUpA8++MD+wWYfHx916dJF8+fPV0FBgZo2baqtW7fq2LFjpcbs0KGDJOlPf/qTnnzySdWoUUN9+vRRjx495OnpqT59+mjMmDHKy8vT3/72N/n5+SkrK8ul5wUAcETYAgCUm/79+6t58+Z65ZVXNGrUKEVFRSk7O1v/7//9P33yyScKDQ3VW2+9pXfffdf+4WGzPPjgg1q9erXmzp2riRMnKiQkRPPmzdPx48ddHrZKHgn09PRUgwYNFBYWpiVLlmjYsGGqW7euQ99169ZpwoQJWr58uQzDUI8ePfTxxx8rICDAod8999yjOXPmaOXKldqyZYuKi4t17NgxtWzZUu+9955eeuklTZ48WVarVWPHjlXjxo01fPhwl54XAMCRxbjeG8MAAAAAAKfxzhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJuA7W2VQXFyskydPqm7durJYLOVdDgAAAIByYhiGfvrpJwUEBMjN7dr3rghbZXDy5EkFBgaWdxkAAAAAKojvv/9et9xyyzX7lGvYiouL08aNG3XkyBHVqlVL9913n+bNm6eWLVva+1y8eFHPPfecNmzYoPz8fEVEROi1116Tv7+/vU9mZqbGjh2rnTt3qk6dOoqMjFRcXJw8PP7v9Hbt2qWYmBgdOnRIgYGBeumllxQVFVWmOuvWrSvplz+oj4+Pa04eAAAAQKVjs9kUGBhozwjXUq5ha/fu3Ro/frzuueceFRYW6sUXX1SPHj309ddfy9vbW5I0adIkffTRR3r33Xfl6+ur6Oho9e/fX3v27JEkFRUVqXfv3rJarUpOTlZWVpaGDh2qGjVq6OWXX5YkHTt2TL1799bTTz+ttWvXavv27Ro5cqSaNGmiiIiI69ZZ8uigj48PYQsAAABAmV4vshiGYdyEWsrkxx9/lJ+fn3bv3q0uXbooNzdXjRs31rp16/T4449Lko4cOaI777xTKSkp6tSpkz7++GM98sgjOnnypP1u18qVKxUbG6sff/xRnp6eio2N1UcffaSDBw/aj/Xkk0/q3Llz2rJly3Xrstls8vX1VW5uLmELAAAAqMacyQYVajbC3NxcSVKDBg0kSfv371dBQYG6d+9u73PHHXfo1ltvVUpKiiQpJSVFYWFhDo8VRkREyGaz6dChQ/Y+l49R0qdkjF/Lz8+XzWZzWAAAAADAGRUmbBUXF2vixIn63e9+p7vuukuSlJ2dLU9PT9WrV8+hr7+/v7Kzs+19Lg9aJdtLtl2rj81m088//1yqlri4OPn6+toXJscAAAAA4KwKE7bGjx+vgwcPasOGDeVdiqZOnarc3Fz78v3335d3SQAAAAAqmQox9Xt0dLQ2b96spKQkh+kTrVarLl26pHPnzjnc3crJyZHVarX32bdvn8N4OTk59m0l/y1pu7yPj4+PatWqVaoeLy8veXl5ueTcAAAAAFRP5XpnyzAMRUdH6/3339eOHTsUEhLisL1Dhw6qUaOGtm/fbm9LT09XZmamwsPDJUnh4eH66quvdOrUKXufxMRE+fj4KDQ01N7n8jFK+pSMAQAAAACuVq6zEY4bN07r1q3TP//5T4dva/n6+trvOI0dO1b/+te/lJCQIB8fH02YMEGSlJycLOmXqd/btm2rgIAAzZ8/X9nZ2Xrqqac0cuRIh6nf77rrLo0fP17Dhw/Xjh079Mwzz+ijjz4q09TvzEYIAAAAQHIuG5Rr2Lra3PTx8fH2Dw6XfNR4/fr1Dh81LnlEUJJOnDihsWPHateuXfL29lZkZKTmzp1b6qPGkyZN0tdff61bbrlF06ZNK/NHjQlbAAAAAKRKFLYqC8IWAAAAAKkSf2cLAAAAAKoKwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJjA4/pdUBFd5RNlAFAl8FESAEBVwJ0tAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExQrmErKSlJffr0UUBAgCwWizZt2uSw3WKxXHFZsGCBvU9wcHCp7XPnznUYJy0tTZ07d1bNmjUVGBio+fPn34zTAwAAAFCNlWvYOn/+vNq0aaPly5dfcXtWVpbDsmbNGlksFj322GMO/WbPnu3Qb8KECfZtNptNPXr0UFBQkPbv368FCxZo5syZWrVqlannBgAAAKB68yjPg/fq1Uu9evW66nar1eqw/s9//lPdunVTs2bNHNrr1q1bqm+JtWvX6tKlS1qzZo08PT3VqlUrpaamatGiRRo9evRvPwkAAAAAuIJK885WTk6OPvroI40YMaLUtrlz56phw4Zq166dFixYoMLCQvu2lJQUdenSRZ6enva2iIgIpaen6+zZs1c8Vn5+vmw2m8MCAAAAAM4o1ztbznjjjTdUt25d9e/f36H9mWeeUfv27dWgQQMlJydr6tSpysrK0qJFiyRJ2dnZCgkJcdjH39/fvq1+/fqljhUXF6dZs2aZdCYAAAAAqoNKE7bWrFmjwYMHq2bNmg7tMTEx9t+tW7eWp6enxowZo7i4OHl5ed3QsaZOneowrs1mU2Bg4I0VDgAAAKBaqhRh69///rfS09P19ttvX7dvx44dVVhYqOPHj6tly5ayWq3Kyclx6FOyfrX3vLy8vG44qAEAAACAVEne2Vq9erU6dOigNm3aXLdvamqq3Nzc5OfnJ0kKDw9XUlKSCgoK7H0SExPVsmXLKz5CCAAAAACuUK5hKy8vT6mpqUpNTZUkHTt2TKmpqcrMzLT3sdlsevfddzVy5MhS+6ekpGjJkiX6z3/+o++++05r167VpEmTNGTIEHuQGjRokDw9PTVixAgdOnRIb7/9tpYuXerwmCAAAAAAuFq5Pkb4xRdfqFu3bvb1kgAUGRmphIQESdKGDRtkGIYGDhxYan8vLy9t2LBBM2fOVH5+vkJCQjRp0iSHIOXr66utW7dq/Pjx6tChgxo1aqTp06cz7TsAAAAAU1kMwzDKu4iKzmazydfXV7m5ufLx8SnvciRJFkt5VwAA5uHKBACoqJzJBpXinS0AAAAAqGwIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmKNewlZSUpD59+iggIEAWi0WbNm1y2B4VFSWLxeKw9OzZ06HPmTNnNHjwYPn4+KhevXoaMWKE8vLyHPqkpaWpc+fOqlmzpgIDAzV//nyzTw0AAABANVeuYev8+fNq06aNli9fftU+PXv2VFZWln1Zv369w/bBgwfr0KFDSkxM1ObNm5WUlKTRo0fbt9tsNvXo0UNBQUHav3+/FixYoJkzZ2rVqlWmnRcAAAAAeJTnwXv16qVevXpds4+Xl5esVusVtx0+fFhbtmzR559/rrvvvluS9Oqrr+rhhx/WK6+8ooCAAK1du1aXLl3SmjVr5OnpqVatWik1NVWLFi1yCGUAAAAA4EoV/p2tXbt2yc/PTy1bttTYsWN1+vRp+7aUlBTVq1fPHrQkqXv37nJzc9PevXvtfbp06SJPT097n4iICKWnp+vs2bNXPGZ+fr5sNpvDAgAAAADOqNBhq2fPnnrzzTe1fft2zZs3T7t371avXr1UVFQkScrOzpafn5/DPh4eHmrQoIGys7Ptffz9/R36lKyX9Pm1uLg4+fr62pfAwEBXnxoAAACAKq5cHyO8nieffNL+OywsTK1bt1bz5s21a9cuPfTQQ6Ydd+rUqYqJibGv22w2AhcAAAAAp1ToO1u/1qxZMzVq1EhHjx6VJFmtVp06dcqhT2Fhoc6cOWN/z8tqtSonJ8ehT8n61d4F8/Lyko+Pj8MCAAAAAM6oVGHrhx9+0OnTp9WkSRNJUnh4uM6dO6f9+/fb++zYsUPFxcXq2LGjvU9SUpIKCgrsfRITE9WyZUvVr1//5p4AAAAAgGqjXMNWXl6eUlNTlZqaKkk6duyYUlNTlZmZqby8PE2ZMkWfffaZjh8/ru3bt+vRRx9VixYtFBERIUm688471bNnT40aNUr79u3Tnj17FB0drSeffFIBAQGSpEGDBsnT01MjRozQoUOH9Pbbb2vp0qUOjwkCAAAAgKtZDMMwyuvgu3btUrdu3Uq1R0ZGasWKFerXr5++/PJLnTt3TgEBAerRo4fmzJnjMOHFmTNnFB0drQ8//FBubm567LHHtGzZMtWpU8feJy0tTePHj9fnn3+uRo0aacKECYqNjS1znTabTb6+vsrNza0wjxRaLOVdAQCYp/yuTAAAXJsz2aBcw1ZlQdgCgJuLKxMAoKJyJhtUqne2AAAAAKCyIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJXBK2zp0754phAAAAAKDKcDpszZs3T2+//bZ9fcCAAWrYsKGaNm2q//znPy4tDgAAAAAqK6fD1sqVKxUYGChJSkxMVGJioj7++GP16tVLU6ZMcXmBAAAAAFAZeTi7Q3Z2tj1sbd68WQMGDFCPHj0UHBysjh07urxAAAAAAKiMnL6zVb9+fX3//feSpC1btqh79+6SJMMwVFRU5NrqAAAAAKCScvrOVv/+/TVo0CDddtttOn36tHr16iVJ+vLLL9WiRQuXFwgAAAAAlZHTYWvx4sUKDg7W999/r/nz56tOnTqSpKysLI0bN87lBQIAAABAZeT0Y4SXLl3S5MmTtXTpUrVr187ePmnSJI0cOdKpsZKSktSnTx8FBATIYrFo06ZN9m0FBQWKjY1VWFiYvL29FRAQoKFDh+rkyZMOYwQHB8tisTgsc+fOdeiTlpamzp07q2bNmgoMDNT8+fOdPW0AAAAAcIrTYcvf31/Dhw/Xp59++psPfv78ebVp00bLly8vte3ChQs6cOCApk2bpgMHDmjjxo1KT09X3759S/WdPXu2srKy7MuECRPs22w2m3r06KGgoCDt379fCxYs0MyZM7Vq1arfXD8AAAAAXI3TjxG+9dZbSkhI0IMPPqjg4GANHz5cQ4cOVUBAgNMH79Wrl/2dr1/z9fVVYmKiQ9tf//pX3XvvvcrMzNStt95qb69bt66sVusVx1m7dq0uXbqkNWvWyNPTU61atVJqaqoWLVqk0aNHO10zAAAAAJSF03e2+vXrp02bNum///2vnn76aa1bt05BQUF65JFHtHHjRhUWFppRpyQpNzdXFotF9erVc2ifO3euGjZsqHbt2mnBggUONaSkpKhLly7y9PS0t0VERCg9PV1nz5694nHy8/Nls9kcFgAAAABwhtNhq0Tjxo0VExOjtLQ0LVq0SNu2bdPjjz+ugIAATZ8+XRcuXHBlnbp48aJiY2M1cOBA+fj42NufeeYZbdiwQTt37tSYMWP08ssv6/nnn7dvz87Olr+/v8NYJevZ2dlXPFZcXJx8fX3tS8l3xQAAAACgrJx+jLBETk6O3njjDSUkJOjEiRN6/PHHNWLECP3www+aN2+ePvvsM23dutUlRRYUFGjAgAEyDEMrVqxw2BYTE2P/3bp1a3l6emrMmDGKi4uTl5fXDR1v6tSpDuPabDYCFwAAAACnOB22Nm7cqPj4eH3yyScKDQ3VuHHjNGTIEIdH++677z7deeedLimwJGidOHFCO3bscLirdSUdO3ZUYWGhjh8/rpYtW8pqtSonJ8ehT8n61d7z8vLyuuGgBgAAAADSDTxGOGzYMAUEBGjPnj1KTU1VdHR0qXeoAgIC9Kc//ek3F1cStL799ltt27ZNDRs2vO4+qampcnNzk5+fnyQpPDxcSUlJKigosPdJTExUy5YtVb9+/d9cIwAAAABcidN3trKyslS7du1r9qlVq5ZmzJhx3bHy8vJ09OhR+/qxY8eUmpqqBg0aqEmTJnr88cd14MABbd68WUVFRfZ3rBo0aCBPT0+lpKRo79696tatm+rWrauUlBRNmjRJQ4YMsQepQYMGadasWRoxYoRiY2N18OBBLV26VIsXL3b21AEAAACgzCyGYRg3uvPFixd16dIlh7brPeZ3uV27dqlbt26l2iMjIzVz5kyFhIRccb+dO3eqa9euOnDggMaNG6cjR44oPz9fISEheuqppxQTE+PwGGBaWprGjx+vzz//XI0aNdKECRMUGxtb5jptNpt8fX2Vm5vr1PmZyWIp7woAwDw3fmUCAMBczmQDp8PW+fPnFRsbq3feeUenT58utb2oqMi5aisBwhYA3FyELQBAReVMNnD6na3nn39eO3bs0IoVK+Tl5aXXX39ds2bNUkBAgN58880bLhoAAAAAqhKn39n68MMP9eabb6pr164aNmyYOnfurBYtWigoKEhr167V4MGDzagTAAAAACoVp+9snTlzRs2aNZP0y/tZZ86ckSTdf//9SkpKcm11AAAAAFBJOR22mjVrpmPHjkmS7rjjDr3zzjuSfrnj9esp4AEAAACgurqh72z95z//kSS98MILWr58uWrWrKlJkyZpypQpLi8QAAAAACqj3zT1uySdOHFC+/fvV4sWLdS6dWtX1VWhMBshANxczEYIAKionMkGTk+Q8WtBQUEKCgr6rcMAAAAAQJXiVNgqLi5WQkKCNm7cqOPHj8tisSgkJESPP/64nnrqKVm43QIAAAAAkpx4Z8swDPXt21cjR47Uf//7X4WFhalVq1Y6ceKEoqKi9Ic//MHMOgEAAACgUinzna2EhAQlJSVp+/bt6tatm8O2HTt2qF+/fnrzzTc1dOhQlxcJAAAAAJVNme9srV+/Xi+++GKpoCVJDz74oF544QWtXbvWpcUBAAAAQGVV5rCVlpamnj17XnV7r1697FPCAwAAAEB1V+awdebMGfn7+191u7+/v86ePeuSogAAAACgsitz2CoqKpKHx9Vf8XJ3d1dhYaFLigIAAACAyq7ME2QYhqGoqCh5eXldcXt+fr7LigIAAACAyq7MYSsyMvK6fZiJEAAAAAB+UeawFR8fb2YdAAAAAFCllPmdLQAAAABA2RG2AAAAAMAEhC0AAAAAMAFhCwAAAABMUKaw1b59e/sHi2fPnq0LFy6YWhQAAAAAVHZlCluHDx/W+fPnJUmzZs1SXl6eqUUBAAAAQGVXpqnf27Ztq2HDhun++++XYRh65ZVXVKdOnSv2nT59uksLBAAAAIDKyGIYhnG9Tunp6ZoxY4YyMjJ04MABhYaGysOjdE6zWCw6cOCAKYWWJ5vNJl9fX+Xm5srHx6e8y5EkWSzlXQEAmOf6VyYAAMqHM9mgTGHrcm5ubsrOzpafn99vKrIyIWwBwM1F2AIAVFTOZIMyPUZ4ueLi4hsuDAAAAACqC6fDliRlZGRoyZIlOnz4sCQpNDRUzz77rJo3b+7S4gAAAACgsnL6O1uffPKJQkNDtW/fPrVu3VqtW7fW3r171apVKyUmJppRIwAAAABUOk6/s9WuXTtFRERo7ty5Du0vvPCCtm7dygQZNwnvbAGoynhnCwBQUTmTDZy+s3X48GGNGDGiVPvw4cP19ddfOzscAAAAAFRJToetxo0bKzU1tVR7ampqtZqhEAAAAACuxekJMkaNGqXRo0fru+++03333SdJ2rNnj+bNm6eYmBiXFwgAAAAAlZHT72wZhqElS5Zo4cKFOnnypCQpICBAU6ZM0TPPPCNLFXyZiHe2AODm4p0tAEBFZepHjS/3008/SZLq1q17o0NUCoQtALi5CFsAgIrK1I8aX66qhywAAAAAuFFOT5DhSklJSerTp48CAgJksVi0adMmh+2GYWj69Olq0qSJatWqpe7du+vbb7916HPmzBkNHjxYPj4+qlevnkaMGKG8vDyHPmlpaercubNq1qypwMBAzZ8/3+xTAwAAAFDNlWvYOn/+vNq0aaPly5dfcfv8+fO1bNkyrVy5Unv37pW3t7ciIiJ08eJFe5/Bgwfr0KFDSkxM1ObNm5WUlKTRo0fbt9tsNvXo0UNBQUHav3+/FixYoJkzZ2rVqlWmnx8AAACA6us3vbPlShaLRe+//7769esn6Ze7WgEBAXruuec0efJkSVJubq78/f2VkJCgJ598UocPH1ZoaKg+//xz3X333ZKkLVu26OGHH9YPP/yggIAArVixQn/605+UnZ0tT09PSb98gHnTpk06cuRImWrjnS0AuLkqxpUJAIDSTPuocUFBgR566KFSj/KZ4dixY8rOzlb37t3tbb6+vurYsaNSUlIkSSkpKapXr549aElS9+7d5ebmpr1799r7dOnSxR60JCkiIkLp6ek6e/bsFY+dn58vm83msAAAAACAM5wKWzVq1FBaWppZtTjIzs6WJPn7+zu0+/v727dlZ2eX+pCyh4eHGjRo4NDnSmNcfoxfi4uLk6+vr30JDAz87ScEAAAAoFpx+p2tIUOGaPXq1WbUUmFMnTpVubm59uX7778v75IAAAAAVDJOT/1eWFioNWvWaNu2berQoYO8vb0dti9atMglhVmtVklSTk6OmjRpYm/PyclR27Zt7X1OnTpVqr4zZ87Y97darcrJyXHoU7Je0ufXvLy85OXl5ZLzAAAAAFA9OX1n6+DBg2rfvr3q1q2rb775Rl9++aV9SU1NdVlhISEhslqt2r59u73NZrNp7969Cg8PlySFh4fr3Llz2r9/v73Pjh07VFxcrI4dO9r7JCUlqaCgwN4nMTFRLVu2VP369V1WLwAAAABczuk7Wzt37nTZwfPy8nT06FH7+rFjx5SamqoGDRro1ltv1cSJE/XnP/9Zt912m0JCQjRt2jQFBATYZyy888471bNnT40aNUorV65UQUGBoqOj9eSTTyogIECSNGjQIM2aNUsjRoxQbGysDh48qKVLl2rx4sUuOw8AAAAA+LUbnvr96NGjysjIUJcuXVSrVi0ZhiGLk/OR79q1S926dSvVHhkZqYSEBBmGoRkzZmjVqlU6d+6c7r//fr322mu6/fbb7X3PnDmj6Ohoffjhh3Jzc9Njjz2mZcuWqU6dOvY+aWlpGj9+vD7//HM1atRIEyZMUGxsbJnrZOp3ALi5mPodAFBROZMNnA5bp0+f1oABA7Rz505ZLBZ9++23atasmYYPH6769etr4cKFv6n4ioiwBQA3F2ELAFBRmfadLUmaNGmSatSooczMTNWuXdve/sc//lFbtmxxvloAAAAAqIKcfmdr69at+uSTT3TLLbc4tN922206ceKEywoDAAAAgMrM6Ttb58+fd7ijVeLMmTNMlw4AAAAA/z+nw1bnzp315ptv2tctFouKi4s1f/78K052AQAAAADVkdOPEc6fP18PPfSQvvjiC126dEnPP/+8Dh06pDNnzmjPnj1m1AgAAAAAlY7Td7buuusuffPNN7r//vv16KOP6vz58+rfv7++/PJLNW/e3IwaAQAAAKDSueHvbFUnTP0OADcXVyYAQEXlTDZw+jFCSTp79qxWr16tw4cPS5JCQ0M1bNgwNWjQ4EaGAwAAAIAqx+nHCJOSkhQcHKxly5bp7NmzOnv2rJYtW6aQkBAlJSWZUSMAAAAAVDpOP0YYFham8PBwrVixQu7u7pKkoqIijRs3TsnJyfrqq69MKbQ88RghANxcPEYIAKionMkGTt/ZOnr0qJ577jl70JIkd3d3xcTE6OjRo85XCwAAAABVkNNhq3379vZ3tS53+PBhtWnTxiVFAQAAAEBlV6YJMtLS0uy/n3nmGT377LM6evSoOnXqJEn67LPPtHz5cs2dO9ecKgEAAACgkinTO1tubm6yWCy6XleLxaKioiKXFVdR8M4WANxcvLMFAKioXD71+7Fjx1xSGAAAAABUF2UKW0FBQWbXAQAAAABVyg191PjkyZP69NNPderUKRUXFztse+aZZ1xSGAAAAABUZk6HrYSEBI0ZM0aenp5q2LChLJe9PGSxWAhbAAAAAKAbCFvTpk3T9OnTNXXqVLm5OT1zPAAAAABUC06npQsXLujJJ58kaAEAAADANTidmEaMGKF3333XjFoAAAAAoMoo03e2LldUVKRHHnlEP//8s8LCwlSjRg2H7YsWLXJpgRUB39kCgJuL72wBACoql39n63JxcXH65JNP1LJlS0kqNUEGAAAAAOAGwtbChQu1Zs0aRUVFmVAOAAAAAFQNTr+z5eXlpd/97ndm1AIAAAAAVYbTYevZZ5/Vq6++akYtAAAAAFBlOP0Y4b59+7Rjxw5t3rxZrVq1KjVBxsaNG11WHAAAAABUVk6HrXr16ql///5m1AIAAAAAVYbTYSs+Pt6MOgAAAACgSnH6nS0AAAAAwPU5fWcrJCTkmt/T+u67735TQQAAAABQFTgdtiZOnOiwXlBQoC+//FJbtmzRlClTXFUXAAAAAFRqToetZ5999orty5cv1xdffPGbCwIAAACAqsBl72z16tVL//jHP1w1HAAAAABUai4LW++9954aNGjgquEAAAAAoFJz+jHCdu3aOUyQYRiGsrOz9eOPP+q1115zaXEAAAAAUFk5Hbb69evnsO7m5qbGjRura9euuuOOO1xVFwAAAABUahbDMIzyLuJagoODdeLEiVLt48aN0/Lly9W1a1ft3r3bYduYMWO0cuVK+3pmZqbGjh2rnTt3qk6dOoqMjFRcXJw8PMqWNW02m3x9fZWbmysfH5/fdkIuco3Z9wGg0qvYVyYAQHXmTDZw+s7Wzfb555+rqKjIvn7w4EH9/ve/1xNPPGFvGzVqlGbPnm1fr127tv13UVGRevfuLavVquTkZGVlZWno0KGqUaOGXn755ZtzEgAAAACqnTKHLTc3t2t+zFiSLBaLCgsLf3NRl2vcuLHD+ty5c9W8eXM98MAD9rbatWvLarVecf+tW7fq66+/1rZt2+Tv76+2bdtqzpw5io2N1cyZM+Xp6Vlqn/z8fOXn59vXbTabi84GAAAAQHVR5rD1/vvvX3VbSkqKli1bpuLiYpcUdTWXLl3SW2+9pZiYGIfgt3btWr311luyWq3q06ePpk2bZr+7lZKSorCwMPn7+9v7R0REaOzYsTp06JDatWtX6jhxcXGaNWuWqecCAAAAoGorc9h69NFHS7Wlp6frhRde0IcffqjBgwc7PMpnhk2bNuncuXOKioqytw0aNEhBQUEKCAhQWlqaYmNjlZ6ero0bN0qSsrOzHYKWJPt6dnb2FY8zdepUxcTE2NdtNpsCAwNdfDYAAAAAqrIbemfr5MmTmjFjht544w1FREQoNTVVd911l6trK2X16tXq1auXAgIC7G2jR4+2/w4LC1OTJk300EMPKSMjQ82bN7+h43h5ecnLy+s31wsAAACg+nLqo8a5ubmKjY1VixYtdOjQIW3fvl0ffvjhTQlaJ06c0LZt2zRy5Mhr9uvYsaMk6ejRo5Ikq9WqnJwchz4l61d7zwsAAAAAfqsyh6358+erWbNm2rx5s9avX6/k5GR17tzZzNocxMfHy8/PT717975mv9TUVElSkyZNJEnh4eH66quvdOrUKXufxMRE+fj4KDQ01LR6AQAAAFRvZf7Olpubm2rVqqXu3bvL3d39qv1K3pVypeLiYoWEhGjgwIGaO3euvT0jI0Pr1q3Tww8/rIYNGyotLU2TJk3SLbfcYv/2VlFRkdq2bauAgADNnz9f2dnZeuqppzRy5MgyT/3Od7YA4ObiO1sAgIrKlO9sDR069LpTv5tl27ZtyszM1PDhwx3aPT09tW3bNi1ZskTnz59XYGCgHnvsMb300kv2Pu7u7tq8ebPGjh2r8PBweXt7KzIy0vTJPAAAAABUb2W+s1WdcWcLAG4urkwAgIrKmWzg1AQZAAAAAICyIWwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAo/yLgAAALiIxVLeFQCAeQyjvCtwGne2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATVOiwNXPmTFksFofljjvusG+/ePGixo8fr4YNG6pOnTp67LHHlJOT4zBGZmamevfurdq1a8vPz09TpkxRYWHhzT4VAAAAANWMR3kXcD2tWrXStm3b7OseHv9X8qRJk/TRRx/p3Xffla+vr6Kjo9W/f3/t2bNHklRUVKTevXvLarUqOTlZWVlZGjp0qGrUqKGXX375pp8LAAAAgOqjwoctDw8PWa3WUu25ublavXq11q1bpwcffFCSFB8frzvvvFOfffaZOnXqpK1bt+rrr7/Wtm3b5O/vr7Zt22rOnDmKjY3VzJkz5enpebNPBwAAAEA1UaEfI5Skb7/9VgEBAWrWrJkGDx6szMxMSdL+/ftVUFCg7t272/vecccduvXWW5WSkiJJSklJUVhYmPz9/e19IiIiZLPZdOjQoaseMz8/XzabzWEBAAAAAGdU6LDVsWNHJSQkaMuWLVqxYoWOHTumzp0766efflJ2drY8PT1Vr149h338/f2VnZ0tScrOznYIWiXbS7ZdTVxcnHx9fe1LYGCga08MAAAAQJVXoR8j7NWrl/1369at1bFjRwUFBemdd95RrVq1TDvu1KlTFRMTY1+32WwELgAAAABOqdB3tn6tXr16uv3223X06FFZrVZdunRJ586dc+iTk5Njf8fLarWWmp2wZP1K74GV8PLyko+Pj8MCAAAAAM6oVGErLy9PGRkZatKkiTp06KAaNWpo+/bt9u3p6enKzMxUeHi4JCk8PFxfffWVTp06Ze+TmJgoHx8fhYaG3vT6AQAAAFQfFfoxwsmTJ6tPnz4KCgrSyZMnNWPGDLm7u2vgwIHy9fXViBEjFBMTowYNGsjHx0cTJkxQeHi4OnXqJEnq0aOHQkND9dRTT2n+/PnKzs7WSy+9pPHjx8vLy6uczw4AAABAVVahw9YPP/yggQMH6vTp02rcuLHuv/9+ffbZZ2rcuLEkafHixXJzc9Njjz2m/Px8RURE6LXXXrPv7+7urs2bN2vs2LEKDw+Xt7e3IiMjNXv27PI6JQAAAADVhMUwDKO8i6jobDabfH19lZubW2He37JYyrsCADAPV6YbxMUBQFVWQS4OzmSDSvXOFgAAAABUFoQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABNU6LAVFxene+65R3Xr1pWfn5/69eun9PR0hz5du3aVxWJxWJ5++mmHPpmZmerdu7dq164tPz8/TZkyRYWFhTfzVAAAAABUMx7lXcC17N69W+PHj9c999yjwsJCvfjii+rRo4e+/vpreXt72/uNGjVKs2fPtq/Xrl3b/ruoqEi9e/eW1WpVcnKysrKyNHToUNWoUUMvv/zyTT0fAAAAANWHxTAMo7yLKKsff/xRfn5+2r17t7p06SLplztbbdu21ZIlS664z8cff6xHHnlEJ0+elL+/vyRp5cqVio2N1Y8//ihPT8/rHtdms8nX11e5ubny8fFx2fn8FhZLeVcAAOapPFemCoaLA4CqrIJcHJzJBhX6McJfy83NlSQ1aNDAoX3t2rVq1KiR7rrrLk2dOlUXLlywb0tJSVFYWJg9aElSRESEbDabDh06dMXj5Ofny2azOSwAAAAA4IwK/Rjh5YqLizVx4kT97ne/01133WVvHzRokIKCghQQEKC0tDTFxsYqPT1dGzdulCRlZ2c7BC1J9vXs7OwrHisuLk6zZs0y6UwAAAAAVAeVJmyNHz9eBw8e1KeffurQPnr0aPvvsLAwNWnSRA899JAyMjLUvHnzGzrW1KlTFRMTY1+32WwKDAy8scIBAAAAVEuV4jHC6Ohobd68WTt37tQtt9xyzb4dO3aUJB09elSSZLValZOT49CnZN1qtV5xDC8vL/n4+DgsAAAAAOCMCh22DMNQdHS03n//fe3YsUMhISHX3Sc1NVWS1KRJE0lSeHi4vvrqK506dcreJzExUT4+PgoNDTWlbgAAAACo0I8Rjh8/XuvWrdM///lP1a1b1/6Ola+vr2rVqqWMjAytW7dODz/8sBo2bKi0tDRNmjRJXbp0UevWrSVJPXr0UGhoqJ566inNnz9f2dnZeumllzR+/Hh5eXmV5+kBAAAAqMIq9NTvlqtMYRsfH6+oqCh9//33GjJkiA4ePKjz588rMDBQf/jDH/TSSy85PPp34sQJjR07Vrt27ZK3t7ciIyM1d+5ceXiULWsy9TsA3FwV98pUwXFxAFCVVZCLgzPZoEKHrYqCsAUANxdXphvExQFAVVZBLg5V9jtbAAAAAFBZELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATVKuwtXz5cgUHB6tmzZrq2LGj9u3bV94lAQAAAKiiqk3YevvttxUTE6MZM2bowIEDatOmjSIiInTq1KnyLg0AAABAFVRtwtaiRYs0atQoDRs2TKGhoVq5cqVq166tNWvWlHdpAAAAAKogj/Iu4Ga4dOmS9u/fr6lTp9rb3Nzc1L17d6WkpJTqn5+fr/z8fPt6bm6uJMlms5lfLABA/HMLACilglwcSjKBYRjX7Vstwtb//vc/FRUVyd/f36Hd399fR44cKdU/Li5Os2bNKtUeGBhoWo0AgP/j61veFQAAKpwKdnH46aef5HudmqpF2HLW1KlTFRMTY18vLi7WmTNn1LBhQ1kslnKsDLj5bDabAgMD9f3338vHx6e8ywEAVBBcH1BdGYahn376SQEBAdftWy3CVqNGjeTu7q6cnByH9pycHFmt1lL9vby85OXl5dBWr149M0sEKjwfHx8upgCAUrg+oDq63h2tEtViggxPT0916NBB27dvt7cVFxdr+/btCg8PL8fKAAAAAFRV1eLOliTFxMQoMjJSd999t+69914tWbJE58+f17Bhw8q7NAAAAABVULUJW3/84x/1448/avr06crOzlbbtm21ZcuWUpNmAHDk5eWlGTNmlHq0FgBQvXF9AK7PYpRlzkIAAAAAgFOqxTtbAAAAAHCzEbYAAAAAwASELQAAAAAwAWELQLnYtWuXLBaLzp07V96lAAAAmIKwBVQBUVFRslgsmjt3rkP7pk2bZLFYyqkqAMDNVnI9+PXSs2fPMu3ftWtXTZw40dwigWqEsAVUETVr1tS8efN09uxZl4156dIll40FALg5evbsqaysLIdl/fr1LhvfMAwVFha6bDygKiNsAVVE9+7dZbVaFRcXd9U+//jHP9SqVSt5eXkpODhYCxcudNgeHBysOXPmaOjQofLx8dHo0aOVkJCgevXqafPmzWrZsqVq166txx9/XBcuXNAbb7yh4OBg1a9fX88884yKiorsY/3973/X3Xffrbp168pqtWrQoEE6deqUaecPAPiFl5eXrFarw1K/fn3t2rVLnp6e+ve//23vO3/+fPn5+SknJ0dRUVHavXu3li5dar8jdvz4cftj3x9//LE6dOggLy8vffrppyouLlZcXJxCQkJUq1YttWnTRu+995597JL9PvnkE7Vr1061atXSgw8+qFOnTunjjz/WnXfeKR8fHw0aNEgXLlyw73e9cYFKxQBQ6UVGRhqPPvqosXHjRqNmzZrG999/bxiGYbz//vtGyf/mX3zxheHm5mbMnj3bSE9PN+Lj441atWoZ8fHx9nGCgoIMHx8f45VXXjGOHj1qHD161IiPjzdq1Khh/P73vzcOHDhg7N6922jYsKHRo0cPY8CAAcahQ4eMDz/80PD09DQ2bNhgH2v16tXGv/71LyMjI8NISUkxwsPDjV69etm379y505BknD179qb8jQCgOii5HlzNlClTjKCgIOPcuXPGgQMHDE9PT+Of//ynYRiGce7cOSM8PNwYNWqUkZWVZWRlZRmFhYX2f69bt25tbN261Th69Khx+vRp489//rNxxx13GFu2bDEyMjKM+Ph4w8vLy9i1a5dhGP/373ynTp2MTz/91Dhw4IDRokUL44EHHjB69OhhHDhwwEhKSjIaNmxozJ07117j9cYFKhPCFlAFXH5x7dSpkzF8+HDDMBzD1qBBg4zf//73DvtNmTLFCA0Nta8HBQUZ/fr1c+gTHx9vSDKOHj1qbxszZoxRu3Zt46effrK3RUREGGPGjLlqjZ9//rkhyb4PYQsAXC8yMtJwd3c3vL29HZa//OUvhmEYRn5+vtG2bVtjwIABRmhoqDFq1CiH/R944AHj2WefdWgr+fd606ZN9raLFy8atWvXNpKTkx36jhgxwhg4cKDDftu2bbNvj4uLMyQZGRkZ9rYxY8YYERERZR4XqEw8yumGGgCTzJs3Tw8++KAmT57s0H748GE9+uijDm2/+93vtGTJEhUVFcnd3V2SdPfdd5cas3bt2mrevLl93d/fX8HBwapTp45D2+WPCe7fv18zZ87Uf/7zH509e1bFxcWSpMzMTIWGhv72EwUAXFG3bt20YsUKh7YGDRpIkjw9PbV27Vq1bt1aQUFBWrx4cZnHvfz6cPToUV24cEG///3vHfpcunRJ7dq1c2hr3bq1/be/v79q166tZs2aObTt27fP6XGByoCwBVQxXbp0UUREhKZOnaqoqCin9/f29i7VVqNGDYd1i8VyxbaSQHX+/HlFREQoIiJCa9euVePGjZWZmamIiAgm3QAAk3l7e6tFixZX3Z6cnCxJOnPmjM6cOXPFf/evNm6JvLw8SdJHH32kpk2bOvTz8vJyWL/8enG964cz4wKVAWELqILmzp2rtm3bqmXLlva2O++8U3v27HHot2fPHt1+++32u1qucuTIEZ0+fVpz585VYGCgJOmLL75w6TEAAM7LyMjQpEmT9Le//U1vv/22IiMjtW3bNrm5/TJnmqenp8NkR1cTGhoqLy8vZWZm6oEHHnBZfWaNC5QXwhZQBYWFhWnw4MFatmyZve25557TPffcozlz5uiPf/yjUlJS9Ne//lWvvfaay49/6623ytPTU6+++qqefvppHTx4UHPmzHH5cQAApeXn5ys7O9uhzcPDQ/Xr19eQIUMUERGhYcOGqWfPngoLC9PChQs1ZcoUSb/MSrt3714dP35cderUsT9++Gt169bV5MmTNWnSJBUXF+v+++9Xbm6u9uzZIx8fH0VGRt5Q7WaNC5QXwhZQRc2ePVtvv/22fb19+/Z65513NH36dM2ZM0dNmjTR7Nmzb+hRw+tp3LixEhIS9OKLL2rZsmVq3769XnnlFfXt29flxwIAONqyZYuaNGni0NayZUsNGjRIJ06c0ObNmyVJTZo00apVqzRw4ED16NFDbdq00eTJkxUZGanQ0FD9/PPPOnbs2FWPM2fOHDVu3FhxcXH67rvvVK9ePbVv314vvvjib6rfrHGB8mAxDMMo7yIAAAAAoKrho8YAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwCAKqNr166aOHGiU/scOXJEnTp1Us2aNdW2bdsy7TNz5kyHvlFRUerXr59TxwUAVH2ELQBAuYuKipLFYpHFYlGNGjUUEhKi559/XhcvXnRqnI0bN2rOnDlO7TNjxgx5e3srPT1d27dvd2rfsrj83K60BAcHu/yYAICKwaO8CwAAQJJ69uyp+Ph4FRQUaP/+/YqMjJTFYtG8efPKPEaDBg2cPm5GRoZ69+6toKAgp/cti6VLl2ru3Ln29SZNmig+Pl49e/aUJLm7u5tyXABA+ePOFgCgQvDy8pLValVgYKD69eun7t27KzEx0b799OnTGjhwoJo2baratWsrLCxM69evdxjj148RBgcH6+WXX9bw4cNVt25d3XrrrVq1apV9u8Vi0f79+zV79mxZLBbNnDlTkhQbG6vbb79dtWvXVrNmzTRt2jQVFBTc0Hn5+vrKarXaF0mqV6+erFarXnzxRQ0bNsyhf0FBgfz8/LR69Wr7OUVHRys6Olq+vr5q1KiRpk2bJsMw7Pvk5+dr8uTJatq0qby9vdWxY0ft2rXrhuoFALgOYQsAUOEcPHhQycnJ8vT0tLddvHhRHTp00EcffaSDBw9q9OjReuqpp7Rv375rjrVw4ULdfffd+vLLLzVu3DiNHTtW6enpkqSsrCy1atVKzz33nLKysjR58mRJUt26dZWQkKCvv/5aS5cu1d/+9jctXrzY5ec5cuRIbdmyRVlZWfa2zZs368KFC/rjH/9ob3vjjTfk4eGhffv2aenSpVq0aJFef/11+/bo6GilpKRow4YNSktL0xNPPKGePXvq22+/dXnNAICyI2wBACqEzZs3q06dOqpZs6bCwsJ06tQpTZkyxb69adOmmjx5stq2batmzZppwoQJ6tmzp955551rjvvwww9r3LhxatGihWJjY9WoUSPt3LlTkmS1WuXh4aE6derIarWqTp06kqSXXnpJ9913n4KDg9WnTx9Nnjz5use5Effdd59atmypv//97/a2+Ph4PfHEE/ZaJCkwMFCLFy9Wy5YtNXjwYE2YMMEe/jIzMxUfH693331XnTt3VvPmzTV58mTdf//9io+Pd3nNAICy450tAECF0K1bN61YsULnz5/X4sWL5eHhoccee8y+vaioSC+//LLeeecd/fe//9WlS5eUn5+v2rVrX3Pc1q1b239bLBZZrVadOnXqmvu8/fbbWrZsmTIyMpSXl6fCwkL5+Pj8thO8ipEjR2rVqlV6/vnnlZOTo48//lg7duxw6NOpUydZLBb7enh4uBYuXKiioiJ99dVXKioq0u233+6wT35+vho2bGhKzQCAsiFsAQAqBG9vb7Vo0UKStGbNGrVp00arV6/WiBEjJEkLFizQ0qVLtWTJEoWFhcnb21sTJ07UpUuXrjlujRo1HNYtFouKi4uv2j8lJUWDBw/WrFmzFBERIV9fX23YsEELFy78jWd4ZUOHDtULL7yglJQUJScnKyQkRJ07dy7z/nl5eXJ3d9f+/ftLTbZx+d0xAMDNR9gCAFQ4bm5uevHFFxUTE6NBgwapVq1a2rNnjx599FENGTJEklRcXKxvvvlGoaGhLj12cnKygoKC9Kc//cneduLECZce43INGzZUv379FB8fr5SUlFITZkjS3r17HdY/++wz3XbbbXJ3d1e7du1UVFSkU6dOORXSAADm450tAECF9MQTT8jd3V3Lly+XJN12221KTExUcnKyDh8+rDFjxignJ8flx73tttuUmZmpDRs2KCMjQ8uWLdP777/v8uNcbuTIkXrjjTd0+PBhRUZGltqemZmpmJgYpaena/369Xr11Vf17LPPSpJuv/12DR48WEOHDtXGjRt17Ngx7du3T3Fxcfroo49MrRsAcG2ELQBAheTh4aHo6GjNnz9f58+f10svvaT27dsrIiJCXbt2ldVqVb9+/Vx+3L59+2rSpEmKjo5W27ZtlZycrGnTprn8OJfr3r27mjRpooiICAUEBJTaPnToUP3888+69957NX78eD377LMaPXq0fXt8fLyGDh2q5557Ti1btlS/fv30+eef69ZbbzW1bgDAtVmMyz/UAQAAbrq8vDw1bdpU8fHx6t+/v8O2rl27qm3btlqyZEn5FAcAuGG8swUAQDkpLi7W//73Py1cuFD16tVT3759y7skAIALEbYAACgnmZmZCgkJ0S233KKEhAR5eHBZBoCqhMcIAQAAAMAETJABAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJjg/wPIalO1NWI/DwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.4380 - val_loss: 0.4040\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3196 - val_loss: 0.2979\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2193 - val_loss: 0.2023\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1347 - val_loss: 0.1263\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0747 - val_loss: 0.0761\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0417 - val_loss: 0.0485\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0252 - val_loss: 0.0322\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0227\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0177\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0097 - val_loss: 0.0150\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0085 - val_loss: 0.0131\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0077 - val_loss: 0.0118\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0070 - val_loss: 0.0106\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0064 - val_loss: 0.0096\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0086\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0054 - val_loss: 0.0078\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0050 - val_loss: 0.0071\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0064\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 0.0060\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0038 - val_loss: 0.0050\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.0047\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0027 - val_loss: 0.0035\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0025 - val_loss: 0.0033\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0024 - val_loss: 0.0031\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0022 - val_loss: 0.0029\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 9.7819e-04 - val_loss: 0.0013\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 9.5348e-04 - val_loss: 0.0013\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 9.2640e-04 - val_loss: 0.0012\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 8.9536e-04 - val_loss: 0.0013\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 8.6404e-04 - val_loss: 0.0012\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 8.5441e-04 - val_loss: 0.0012\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 8.2054e-04 - val_loss: 0.0011\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 8.0601e-04 - val_loss: 0.0011\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 7.7873e-04 - val_loss: 0.0010\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 7.5658e-04 - val_loss: 0.0010\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 7.3019e-04 - val_loss: 9.6684e-04\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 7.0507e-04 - val_loss: 9.6900e-04\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 6.8924e-04 - val_loss: 9.3019e-04\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 6.6420e-04 - val_loss: 0.0010\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 6.5117e-04 - val_loss: 8.7594e-04\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 6.2634e-04 - val_loss: 8.3562e-04\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 6.0916e-04 - val_loss: 8.0783e-04\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 5.8353e-04 - val_loss: 7.7678e-04\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.7216e-04 - val_loss: 7.9639e-04\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 5.4631e-04 - val_loss: 7.7321e-04\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 5.3961e-04 - val_loss: 7.5974e-04\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 5.1655e-04 - val_loss: 7.0385e-04\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.9333e-04 - val_loss: 6.8559e-04\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.7504e-04 - val_loss: 6.5054e-04\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.5988e-04 - val_loss: 6.3427e-04\n",
      "Epoch 74/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.4772e-04 - val_loss: 6.1463e-04\n",
      "Epoch 75/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.3494e-04 - val_loss: 5.9901e-04\n",
      "Epoch 76/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.1275e-04 - val_loss: 5.8937e-04\n",
      "Epoch 77/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 4.0446e-04 - val_loss: 5.7827e-04\n",
      "Epoch 78/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.8733e-04 - val_loss: 5.6620e-04\n",
      "Epoch 79/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.8055e-04 - val_loss: 5.1781e-04\n",
      "Epoch 80/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.6547e-04 - val_loss: 4.9866e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.4638e-04 - val_loss: 4.9964e-04\n",
      "Epoch 82/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.4201e-04 - val_loss: 4.8410e-04\n",
      "Epoch 83/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.3365e-04 - val_loss: 4.4763e-04\n",
      "Epoch 84/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.1333e-04 - val_loss: 4.4447e-04\n",
      "Epoch 85/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 3.0589e-04 - val_loss: 4.0222e-04\n",
      "Epoch 86/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.9795e-04 - val_loss: 4.0530e-04\n",
      "Epoch 87/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.8611e-04 - val_loss: 3.9939e-04\n",
      "Epoch 88/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.7721e-04 - val_loss: 3.7283e-04\n",
      "Epoch 89/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.7617e-04 - val_loss: 3.5883e-04\n",
      "Epoch 90/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.6152e-04 - val_loss: 3.4141e-04\n",
      "Epoch 91/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.4691e-04 - val_loss: 3.1961e-04\n",
      "Epoch 92/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.3972e-04 - val_loss: 3.1404e-04\n",
      "Epoch 93/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.3931e-04 - val_loss: 3.3885e-04\n",
      "Epoch 94/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.4608e-04 - val_loss: 3.2360e-04\n",
      "Epoch 95/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.1948e-04 - val_loss: 2.8012e-04\n",
      "Epoch 96/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.1138e-04 - val_loss: 2.8726e-04\n",
      "Epoch 97/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.0481e-04 - val_loss: 3.0125e-04\n",
      "Epoch 98/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 2.0280e-04 - val_loss: 2.5536e-04\n",
      "Epoch 99/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.9276e-04 - val_loss: 2.5154e-04\n",
      "Epoch 100/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.8527e-04 - val_loss: 2.3869e-04\n",
      "81/81 [==============================] - 0s 575us/step\n",
      "Accuracy: 98.48%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 11.0]\n",
    "extreme_data = data[data['rainfall'] > 11.0]\n",
    "\n",
    "# Plot the normal and extreme rainfall data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar('Normal', len(normal_data), color='blue')\n",
    "plt.bar('Extreme', len(extreme_data), color='red')\n",
    "plt.title('Rainfall Data')\n",
    "plt.xlabel('Rainfall Type')\n",
    "plt.ylabel('Number of Days')\n",
    "plt.show()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(normal_data[['windspeed', 'tpw', 'rainfall']].values)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu')(input_layer)\n",
    "decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "# Create the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='logcosh')\n",
    "\n",
    "# Train the model\n",
    "history = autoencoder.fit(train_data, train_data, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = scaler.transform(data[['windspeed', 'tpw', 'rainfall']].values)\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 8.6\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "data['actual_rainfall_class'] = np.where(data['rainfall'] > 11.0, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n",
    "\n",
    "# Reverse the standardization of the predicted rainfall values and save the results to a new CSV file\n",
    "predicted_rainfall = scaler.inverse_transform(predicted_data)[:, 2]\n",
    "data['predicted_rainfall'] = predicted_rainfall\n",
    "data.to_csv('predicted_rainfall_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6a1575c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.9130 - val_loss: 6.3296\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.7577 - val_loss: 2.9758\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.6140 - val_loss: 0.7972\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4633 - val_loss: 0.3091\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1598 - val_loss: 0.1170\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0714 - val_loss: 0.0657\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0452 - val_loss: 0.0437\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0332 - val_loss: 0.0317\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0261 - val_loss: 0.0248\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0201\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0180 - val_loss: 0.0170\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0142\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0121\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0096 - val_loss: 0.0091\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 0.0078\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 0.0048\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0030 - val_loss: 0.0034\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0027 - val_loss: 0.0030\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.4567e-04 - val_loss: 0.0012\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.5404e-04 - val_loss: 0.0011\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.7518e-04 - val_loss: 0.0010\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.8790e-04 - val_loss: 9.0877e-04\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.3546e-04 - val_loss: 8.3375e-04\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.4808e-04 - val_loss: 7.1677e-04\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.8007e-04 - val_loss: 6.2686e-04\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.2833e-04 - val_loss: 5.5112e-04\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3.7766e-04 - val_loss: 5.0414e-04\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3.3531e-04 - val_loss: 4.4643e-04\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.9819e-04 - val_loss: 3.8090e-04\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.6264e-04 - val_loss: 3.4764e-04\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.3552e-04 - val_loss: 3.1277e-04\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.1055e-04 - val_loss: 2.8312e-04\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.8871e-04 - val_loss: 2.6067e-04\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.8007e-04 - val_loss: 2.4266e-04\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.5776e-04 - val_loss: 2.0777e-04\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.3933e-04 - val_loss: 1.8473e-04\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.4752e-04 - val_loss: 1.5709e-04\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.1283e-04 - val_loss: 1.3345e-04\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.0057e-04 - val_loss: 1.1245e-04\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.8780e-05 - val_loss: 1.0278e-04\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.3225e-05 - val_loss: 9.2366e-05\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.1663e-05 - val_loss: 1.5875e-04\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.2004e-05 - val_loss: 7.0213e-05\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.8676e-05 - val_loss: 5.5641e-05\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.2896e-05 - val_loss: 5.7159e-05\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.9296e-05 - val_loss: 3.8501e-05\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.5423e-05 - val_loss: 3.5933e-05\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3.8175e-05 - val_loss: 3.0622e-05\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3.7099e-05 - val_loss: 2.4952e-05\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3.4013e-05 - val_loss: 2.1265e-05\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.9682e-05 - val_loss: 2.0475e-05\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.9935e-05 - val_loss: 1.4873e-05\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.4484e-05 - val_loss: 1.7025e-05\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.2448e-05 - val_loss: 2.2432e-05\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.1636e-05 - val_loss: 1.1603e-05\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.0125e-05 - val_loss: 1.0008e-05\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.8259e-05 - val_loss: 7.7713e-06\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6674e-05 - val_loss: 6.6071e-06\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.6864e-05 - val_loss: 5.1071e-06\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.5869e-05 - val_loss: 4.2687e-06\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.5061e-05 - val_loss: 3.7303e-06\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.2786e-05 - val_loss: 2.5670e-06\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.2938e-05 - val_loss: 2.0924e-06\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.2518e-05 - val_loss: 5.3887e-06\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.2229e-05 - val_loss: 3.1941e-06\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.0461e-05 - val_loss: 1.4157e-06\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.5330e-06 - val_loss: 3.4963e-06\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.0037e-05 - val_loss: 8.0600e-07\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.2978e-06 - val_loss: 2.5946e-06\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.6843e-06 - val_loss: 1.2981e-06\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.6005e-06 - val_loss: 1.5313e-06\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.9949e-06 - val_loss: 5.4890e-06\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.3955e-06 - val_loss: 3.9149e-06\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.3619e-06 - val_loss: 9.6117e-07\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.9782e-06 - val_loss: 6.2161e-07\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.0781e-06 - val_loss: 4.5406e-07\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.4738e-06 - val_loss: 3.6002e-07\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.9912e-06 - val_loss: 1.3493e-06\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.2066e-06 - val_loss: 6.7964e-07\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.9777e-06 - val_loss: 5.2270e-07\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.3912e-06 - val_loss: 5.9127e-06\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.5668e-06 - val_loss: 4.6206e-06\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.7051e-06 - val_loss: 9.8297e-07\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.4422e-06 - val_loss: 1.5847e-06\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3.2582e-06 - val_loss: 1.3872e-06\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3.5251e-06 - val_loss: 2.2414e-06\n",
      "81/81 [==============================] - 0s 575us/step\n",
      "Accuracy: 98.71%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 30]\n",
    "extreme_data = data[data['rainfall'] > 30]\n",
    "\n",
    "# Plot the normal and extreme rainfall data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar('Normal', len(normal_data), color='blue')\n",
    "plt.bar('Extreme', len(extreme_data), color='red')\n",
    "plt.title('Rainfall Data')\n",
    "plt.xlabel('Rainfall Type')\n",
    "plt.ylabel('Number of Days')\n",
    "plt.show()\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu')(input_layer)\n",
    "decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "# Create the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='logcosh')\n",
    "\n",
    "# Train the model\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "history = autoencoder.fit(train_data, train_data, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 9.6\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "data['actual_rainfall_class'] = np.where(data['rainfall'] > 30, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a581306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>predicted_rainfall</th>\n",
       "      <th>error</th>\n",
       "      <th>rainfall_class</th>\n",
       "      <th>actual_rainfall_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/1/2000</td>\n",
       "      <td>12.245595</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>4.688589</td>\n",
       "      <td>4.689952</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/2/2000</td>\n",
       "      <td>12.825491</td>\n",
       "      <td>0.044874</td>\n",
       "      <td>11.330589</td>\n",
       "      <td>11.333991</td>\n",
       "      <td>0.003402</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/3/2000</td>\n",
       "      <td>12.921664</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>6.894713</td>\n",
       "      <td>6.897418</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/4/2000</td>\n",
       "      <td>15.149001</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>4.613324</td>\n",
       "      <td>4.614687</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/5/2000</td>\n",
       "      <td>18.495907</td>\n",
       "      <td>0.139491</td>\n",
       "      <td>8.111635</td>\n",
       "      <td>8.113751</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>9/26/2020</td>\n",
       "      <td>5.577215</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>4.467977</td>\n",
       "      <td>4.469275</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>9/27/2020</td>\n",
       "      <td>5.184293</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>1.572182</td>\n",
       "      <td>1.572849</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>9/28/2020</td>\n",
       "      <td>4.469007</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>1.819019</td>\n",
       "      <td>1.819735</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>9/29/2020</td>\n",
       "      <td>4.259090</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>2.198017</td>\n",
       "      <td>2.198763</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>9/30/2020</td>\n",
       "      <td>5.513838</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>12.713634</td>\n",
       "      <td>12.716312</td>\n",
       "      <td>0.002679</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2562 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall  predicted_rainfall     error  \\\n",
       "0      6/1/2000  12.245595  0.033285   4.688589            4.689952  0.001364   \n",
       "1      6/2/2000  12.825491  0.044874  11.330589           11.333991  0.003402   \n",
       "2      6/3/2000  12.921664  0.010243   6.894713            6.897418  0.002705   \n",
       "3      6/4/2000  15.149001  0.036881   4.613324            4.614687  0.001363   \n",
       "4      6/5/2000  18.495907  0.139491   8.111635            8.113751  0.002116   \n",
       "...         ...        ...       ...        ...                 ...       ...   \n",
       "2557  9/26/2020   5.577215  0.009963   4.467977            4.469275  0.001298   \n",
       "2558  9/27/2020   5.184293  0.002341   1.572182            1.572849  0.000668   \n",
       "2559  9/28/2020   4.469007  0.000867   1.819019            1.819735  0.000716   \n",
       "2560  9/29/2020   4.259090  0.001416   2.198017            2.198763  0.000746   \n",
       "2561  9/30/2020   5.513838  0.002474  12.713634           12.716312  0.002679   \n",
       "\n",
       "     rainfall_class actual_rainfall_class  \n",
       "0            Normal                Normal  \n",
       "1            Normal                Normal  \n",
       "2            Normal                Normal  \n",
       "3            Normal                Normal  \n",
       "4            Normal                Normal  \n",
       "...             ...                   ...  \n",
       "2557         Normal                Normal  \n",
       "2558         Normal                Normal  \n",
       "2559         Normal                Normal  \n",
       "2560         Normal                Normal  \n",
       "2561         Normal                Normal  \n",
       "\n",
       "[2562 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee547e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c37de167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall  predicted_rainfall     error  \\\n",
      "41    7/12/2000  23.003010  0.016358  33.846075           33.845470  0.000604   \n",
      "552    8/4/2004  21.769238  0.043448  44.404364           44.383682  0.020682   \n",
      "876   6/23/2007  23.908970  0.331606  49.356381           49.326328  0.030053   \n",
      "900   7/17/2007  19.211468  0.030241  32.941690           32.941795  0.000106   \n",
      "920    8/6/2007  17.366877  0.022628  31.609187           31.609510  0.000323   \n",
      "1033  7/28/2008  20.585272  0.014900  32.559895           32.559639  0.000256   \n",
      "1143  7/16/2009  21.342918  0.037500  38.754120           38.754650  0.000530   \n",
      "1144  7/17/2009  21.451813  0.105873  39.511546           39.512154  0.000608   \n",
      "1145  7/18/2009  16.981620  0.104322  38.342482           38.319210  0.023272   \n",
      "1343   6/2/2011  12.654827  0.550524  34.993438           34.963329  0.030108   \n",
      "1388  7/17/2011  19.214113  0.057870  45.056388           45.029278  0.027110   \n",
      "1409   8/7/2011  15.895408  0.019959  31.262259           31.261595  0.000664   \n",
      "1436   9/3/2011  17.204716  0.025290  32.701293           32.701817  0.000524   \n",
      "1481  6/18/2012  16.490360  0.060908  47.977786           47.952579  0.025206   \n",
      "1482  6/19/2012  12.745975  0.004308  36.064092           36.049526  0.014566   \n",
      "1531   8/7/2012  11.919377  0.026516  33.012284           32.999214  0.013070   \n",
      "1586   6/1/2013  11.123766  0.174906  30.608754           30.593178  0.015576   \n",
      "1619   7/4/2013  16.945307  0.008078  34.218388           34.206348  0.012040   \n",
      "1648   8/2/2013  20.746782  0.006965  30.364948           30.364319  0.000629   \n",
      "1769   8/1/2014  20.457104  0.029008  35.467946           35.468166  0.000221   \n",
      "1980  6/29/2016  21.279696  0.040968  30.296499           30.295731  0.000769   \n",
      "2271  8/15/2018  21.421936  0.086819  41.892355           41.895130  0.002775   \n",
      "2272  8/16/2018  22.673685  0.060061  47.362358           47.330303  0.032055   \n",
      "2384   8/6/2019  18.526226  0.027500  30.351481           30.351343  0.000138   \n",
      "2385   8/7/2019  21.010150  0.027716  44.681781           44.653645  0.028136   \n",
      "2386   8/8/2019  22.589184  0.024273  47.165503           47.134678  0.030825   \n",
      "2387   8/9/2019  21.575483  0.042829  80.636074           80.592758  0.043316   \n",
      "2388  8/10/2019  20.276800  0.043114  58.431061           58.397923  0.033139   \n",
      "2389  8/11/2019  17.905354  0.011177  30.734900           30.734964  0.000065   \n",
      "2505   8/5/2020  27.123528  0.037294  32.042267           32.040451  0.001816   \n",
      "2507   8/7/2020  19.529327  0.108583  42.017340           41.989574  0.027766   \n",
      "2508   8/8/2020  17.372940  0.220405  35.905335           35.882488  0.022846   \n",
      "2551  9/20/2020  18.062815  0.326716  34.073259           34.075687  0.002428   \n",
      "\n",
      "     rainfall_class actual_rainfall_class  \n",
      "41           Normal               Extreme  \n",
      "552          Normal               Extreme  \n",
      "876          Normal               Extreme  \n",
      "900          Normal               Extreme  \n",
      "920          Normal               Extreme  \n",
      "1033         Normal               Extreme  \n",
      "1143         Normal               Extreme  \n",
      "1144         Normal               Extreme  \n",
      "1145         Normal               Extreme  \n",
      "1343         Normal               Extreme  \n",
      "1388         Normal               Extreme  \n",
      "1409         Normal               Extreme  \n",
      "1436         Normal               Extreme  \n",
      "1481         Normal               Extreme  \n",
      "1482         Normal               Extreme  \n",
      "1531         Normal               Extreme  \n",
      "1586         Normal               Extreme  \n",
      "1619         Normal               Extreme  \n",
      "1648         Normal               Extreme  \n",
      "1769         Normal               Extreme  \n",
      "1980         Normal               Extreme  \n",
      "2271         Normal               Extreme  \n",
      "2272         Normal               Extreme  \n",
      "2384         Normal               Extreme  \n",
      "2385         Normal               Extreme  \n",
      "2386         Normal               Extreme  \n",
      "2387         Normal               Extreme  \n",
      "2388         Normal               Extreme  \n",
      "2389         Normal               Extreme  \n",
      "2505         Normal               Extreme  \n",
      "2507         Normal               Extreme  \n",
      "2508         Normal               Extreme  \n",
      "2551         Normal               Extreme  \n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = data[data['rainfall_class'] != data['actual_rainfall_class']]\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc908d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c167cace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 9ms/step - loss: 6.3474 - val_loss: 4.8926\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.4545 - val_loss: 2.0922\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.9704 - val_loss: 0.5840\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3808 - val_loss: 0.3199\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1909 - val_loss: 0.1512\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 0.0874\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0675 - val_loss: 0.0743\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0621 - val_loss: 0.0688\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0587 - val_loss: 0.0646\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0559 - val_loss: 0.0615\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.0584\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.0562\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.0544\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.0522\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0468 - val_loss: 0.0506\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0455 - val_loss: 0.0489\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.0476\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.0462\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.0452\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.0442\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.0431\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.0420\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.0412\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.0402\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.0396\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.0389\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.0383\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.0377\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0373\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0368\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0363\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0359\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0356\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0352\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 0.0348\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0345\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0341\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0338\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0335\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 0.0332\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0329\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0320 - val_loss: 0.0326\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0317 - val_loss: 0.0323\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0320\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 0.0317\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0314\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0312\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0310\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.0307\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0304\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0302\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0299\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0296\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0294\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0289 - val_loss: 0.0291\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0289\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0286\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0283\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0280\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0276 - val_loss: 0.0277\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0274\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.0271\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0269\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0265\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0263\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0260\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0257\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0254\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0251\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0248\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0245\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0242\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0239\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0235\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0231\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0228\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0226\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0223\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0221\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0219\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.0216\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0214\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0212\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0209\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0206\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.0204\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0202\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0199\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0197\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0193 - val_loss: 0.0194\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0191 - val_loss: 0.0192\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.0189\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.0187\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0183 - val_loss: 0.0184\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0180 - val_loss: 0.0181\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0179\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0175 - val_loss: 0.0176\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0173 - val_loss: 0.0174\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0171\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "Accuracy: 98.71%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 30]\n",
    "extreme_data = data[data['rainfall'] > 30]\n",
    "\n",
    "# Plot the normal and extreme rainfall data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar('Normal', len(normal_data), color='blue')\n",
    "plt.bar('Extreme', len(extreme_data), color='red')\n",
    "plt.title('Rainfall Data')\n",
    "plt.xlabel('Rainfall Type')\n",
    "plt.ylabel('Number of Days')\n",
    "plt.show()\n",
    "\n",
    "# Define the autoencoder architecture with regularization\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu', activity_regularizer=regularizers.l1(0.001))(input_layer)\n",
    "decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "# Create the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='logcosh')\n",
    "\n",
    "# Train the model\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "history = autoencoder.fit(train_data, train_data, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 9.6\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "data['actual_rainfall_class'] = np.where(data['rainfall'] > 30, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ed7efee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>predicted_rainfall</th>\n",
       "      <th>error</th>\n",
       "      <th>rainfall_class</th>\n",
       "      <th>actual_rainfall_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/1/2000</td>\n",
       "      <td>12.245595</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>4.688589</td>\n",
       "      <td>4.685352</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/2/2000</td>\n",
       "      <td>12.825491</td>\n",
       "      <td>0.044874</td>\n",
       "      <td>11.330589</td>\n",
       "      <td>11.320268</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/3/2000</td>\n",
       "      <td>12.921664</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>6.894713</td>\n",
       "      <td>6.889604</td>\n",
       "      <td>0.005110</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/4/2000</td>\n",
       "      <td>15.149001</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>4.613324</td>\n",
       "      <td>4.612628</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/5/2000</td>\n",
       "      <td>18.495907</td>\n",
       "      <td>0.139491</td>\n",
       "      <td>8.111635</td>\n",
       "      <td>8.109536</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>9/26/2020</td>\n",
       "      <td>5.577215</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>4.467977</td>\n",
       "      <td>4.459395</td>\n",
       "      <td>0.008581</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>9/27/2020</td>\n",
       "      <td>5.184293</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>1.572182</td>\n",
       "      <td>1.566577</td>\n",
       "      <td>0.005604</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>9/28/2020</td>\n",
       "      <td>4.469007</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>1.819019</td>\n",
       "      <td>1.812531</td>\n",
       "      <td>0.006488</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>9/29/2020</td>\n",
       "      <td>4.259090</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>2.198017</td>\n",
       "      <td>2.190919</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>9/30/2020</td>\n",
       "      <td>5.513838</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>12.713634</td>\n",
       "      <td>12.698752</td>\n",
       "      <td>0.014881</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2562 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall  predicted_rainfall     error  \\\n",
       "0      6/1/2000  12.245595  0.033285   4.688589            4.685352  0.003236   \n",
       "1      6/2/2000  12.825491  0.044874  11.330589           11.320268  0.010322   \n",
       "2      6/3/2000  12.921664  0.010243   6.894713            6.889604  0.005110   \n",
       "3      6/4/2000  15.149001  0.036881   4.613324            4.612628  0.000697   \n",
       "4      6/5/2000  18.495907  0.139491   8.111635            8.109536  0.002099   \n",
       "...         ...        ...       ...        ...                 ...       ...   \n",
       "2557  9/26/2020   5.577215  0.009963   4.467977            4.459395  0.008581   \n",
       "2558  9/27/2020   5.184293  0.002341   1.572182            1.566577  0.005604   \n",
       "2559  9/28/2020   4.469007  0.000867   1.819019            1.812531  0.006488   \n",
       "2560  9/29/2020   4.259090  0.001416   2.198017            2.190919  0.007098   \n",
       "2561  9/30/2020   5.513838  0.002474  12.713634           12.698752  0.014881   \n",
       "\n",
       "     rainfall_class actual_rainfall_class  \n",
       "0            Normal                Normal  \n",
       "1            Normal                Normal  \n",
       "2            Normal                Normal  \n",
       "3            Normal                Normal  \n",
       "4            Normal                Normal  \n",
       "...             ...                   ...  \n",
       "2557         Normal                Normal  \n",
       "2558         Normal                Normal  \n",
       "2559         Normal                Normal  \n",
       "2560         Normal                Normal  \n",
       "2561         Normal                Normal  \n",
       "\n",
       "[2562 rows x 8 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "58f15bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall  predicted_rainfall     error  \\\n",
      "41    7/12/2000  23.003010  0.016358  33.846075           33.803745  0.042330   \n",
      "552    8/4/2004  21.769238  0.043448  44.404364           44.360970  0.043394   \n",
      "876   6/23/2007  23.908970  0.331606  49.356381           49.333420  0.022962   \n",
      "900   7/17/2007  19.211468  0.030241  32.941690           32.905117  0.036573   \n",
      "920    8/6/2007  17.366877  0.022628  31.609187           31.574574  0.034614   \n",
      "1033  7/28/2008  20.585272  0.014900  32.559895           32.520660  0.039235   \n",
      "1143  7/16/2009  21.342918  0.037500  38.754120           38.713226  0.040894   \n",
      "1144  7/17/2009  21.451813  0.105873  39.511546           39.476124  0.035422   \n",
      "1145  7/18/2009  16.981620  0.104322  38.342482           38.312340  0.030143   \n",
      "1343   6/2/2011  12.654827  0.550524  34.993438           35.008255  0.014817   \n",
      "1388  7/17/2011  19.214113  0.057870  45.056388           45.016743  0.039645   \n",
      "1409   8/7/2011  15.895408  0.019959  31.262259           31.229176  0.033084   \n",
      "1436   9/3/2011  17.204716  0.025290  32.701293           32.666595  0.034697   \n",
      "1481  6/18/2012  16.490360  0.060908  47.977786           47.940063  0.037722   \n",
      "1482  6/19/2012  12.745975  0.004308  36.064092           36.030926  0.033167   \n",
      "1531   8/7/2012  11.919377  0.026516  33.012284           32.983326  0.028958   \n",
      "1586   6/1/2013  11.123766  0.174906  30.608754           30.594629  0.014125   \n",
      "1619   7/4/2013  16.945307  0.008078  34.218388           34.181793  0.036595   \n",
      "1648   8/2/2013  20.746782  0.006965  30.364948           30.325844  0.039104   \n",
      "1769   8/1/2014  20.457104  0.029008  35.467946           35.428764  0.039181   \n",
      "1980  6/29/2016  21.279696  0.040968  30.296499           30.259792  0.036707   \n",
      "2271  8/15/2018  21.421936  0.086819  41.892355           41.854233  0.038122   \n",
      "2272  8/16/2018  22.673685  0.060061  47.362358           47.318081  0.044277   \n",
      "2384   8/6/2019  18.526226  0.027500  30.351481           30.316589  0.034892   \n",
      "2385   8/7/2019  21.010150  0.027716  44.681781           44.637726  0.044055   \n",
      "2386   8/8/2019  22.589184  0.024273  47.165503           47.118301  0.047202   \n",
      "2387   8/9/2019  21.575483  0.042829  80.636074           80.576485  0.059590   \n",
      "2388  8/10/2019  20.276800  0.043114  58.431061           58.382935  0.048127   \n",
      "2389  8/11/2019  17.905354  0.011177  30.734900           30.699097  0.035803   \n",
      "2505   8/5/2020  27.123528  0.037294  32.042267           32.020557  0.021709   \n",
      "2507   8/7/2020  19.529327  0.108583  42.017340           41.983124  0.034217   \n",
      "2508   8/8/2020  17.372940  0.220405  35.905335           35.885933  0.019402   \n",
      "2551  9/20/2020  18.062815  0.326716  34.073259           34.063160  0.010099   \n",
      "\n",
      "     rainfall_class actual_rainfall_class  \n",
      "41           Normal               Extreme  \n",
      "552          Normal               Extreme  \n",
      "876          Normal               Extreme  \n",
      "900          Normal               Extreme  \n",
      "920          Normal               Extreme  \n",
      "1033         Normal               Extreme  \n",
      "1143         Normal               Extreme  \n",
      "1144         Normal               Extreme  \n",
      "1145         Normal               Extreme  \n",
      "1343         Normal               Extreme  \n",
      "1388         Normal               Extreme  \n",
      "1409         Normal               Extreme  \n",
      "1436         Normal               Extreme  \n",
      "1481         Normal               Extreme  \n",
      "1482         Normal               Extreme  \n",
      "1531         Normal               Extreme  \n",
      "1586         Normal               Extreme  \n",
      "1619         Normal               Extreme  \n",
      "1648         Normal               Extreme  \n",
      "1769         Normal               Extreme  \n",
      "1980         Normal               Extreme  \n",
      "2271         Normal               Extreme  \n",
      "2272         Normal               Extreme  \n",
      "2384         Normal               Extreme  \n",
      "2385         Normal               Extreme  \n",
      "2386         Normal               Extreme  \n",
      "2387         Normal               Extreme  \n",
      "2388         Normal               Extreme  \n",
      "2389         Normal               Extreme  \n",
      "2505         Normal               Extreme  \n",
      "2507         Normal               Extreme  \n",
      "2508         Normal               Extreme  \n",
      "2551         Normal               Extreme  \n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = data[data['rainfall_class'] != data['actual_rainfall_class']]\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28156ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 30]\n",
    "extreme_data = data[data['rainfall'] > 30]\n",
    "\n",
    "# Define the autoencoder architecture with the best regularization strength found by grid search\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu', kernel_regularizer='l2', bias_regularizer='l2', activity_regularizer='l2', kernel_initializer='he_uniform')(input_layer)\n",
    "decoded = Dense(3, activation=None, kernel_initializer='he_uniform')(encoded)\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='logcosh')\n",
    "\n",
    "# Train the model\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "history = autoencoder.fit(train_data, train_data, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 9.6\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "data['actual_rainfall_class'] = np.where(data['rainfall'] > 30, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3996fcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shine\\AppData\\Local\\Temp\\ipykernel_25120\\2823681750.py:26: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 3ms/step - loss: 6.7714 - mse: 37.0081\n",
      "Best reg_strength: 100.000 using {'reg_strength': 100}\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 2s 9ms/step - loss: 24.2573 - val_loss: 20.7919\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 17.9178 - val_loss: 15.3913\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 13.3975 - val_loss: 11.7155\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 10.1581 - val_loss: 8.8651\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.7945 - val_loss: 7.0431\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.3053 - val_loss: 5.8035\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.2292 - val_loss: 4.8605\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.3881 - val_loss: 4.1164\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.7160 - val_loss: 3.5110\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.1686 - val_loss: 3.0124\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.7197 - val_loss: 2.6036\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.3478 - val_loss: 2.2633\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.0382 - val_loss: 1.9754\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.7803 - val_loss: 1.7396\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.5664 - val_loss: 1.5393\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.3855 - val_loss: 1.3692\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.2346 - val_loss: 1.2297\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.1089 - val_loss: 1.1106\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0027 - val_loss: 1.0102\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.9135 - val_loss: 0.9248\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.8387 - val_loss: 0.8532\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.7748 - val_loss: 0.7922\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.7207 - val_loss: 0.7405\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.6738 - val_loss: 0.6938\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.6333 - val_loss: 0.6536\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.5983 - val_loss: 0.6190\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.5673 - val_loss: 0.5885\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.5402 - val_loss: 0.5640\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.5155 - val_loss: 0.5360\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4934 - val_loss: 0.5144\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4732 - val_loss: 0.4932\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4550 - val_loss: 0.4750\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4381 - val_loss: 0.4578\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4223 - val_loss: 0.4434\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4079 - val_loss: 0.4267\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3944 - val_loss: 0.4129\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3813 - val_loss: 0.3993\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3691 - val_loss: 0.3867\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3576 - val_loss: 0.3756\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3469 - val_loss: 0.3639\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3366 - val_loss: 0.3529\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3266 - val_loss: 0.3426\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3172 - val_loss: 0.3323\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3079 - val_loss: 0.3227\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2992 - val_loss: 0.3136\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2908 - val_loss: 0.3050\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2826 - val_loss: 0.2962\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2746 - val_loss: 0.2879\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2671 - val_loss: 0.2799\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2599 - val_loss: 0.2722\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2526 - val_loss: 0.2647\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2458 - val_loss: 0.2576\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2391 - val_loss: 0.2505\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2327 - val_loss: 0.2438\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2266 - val_loss: 0.2374\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2207 - val_loss: 0.2312\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2150 - val_loss: 0.2253\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2094 - val_loss: 0.2193\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2041 - val_loss: 0.2138\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1991 - val_loss: 0.2084\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1940 - val_loss: 0.2033\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1892 - val_loss: 0.1983\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1846 - val_loss: 0.1933\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1801 - val_loss: 0.1886\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1757 - val_loss: 0.1841\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1700 - val_loss: 0.1744\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1606 - val_loss: 0.1656\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1532 - val_loss: 0.1586\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1469 - val_loss: 0.1528\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1414 - val_loss: 0.1467\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1364 - val_loss: 0.1421\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1318 - val_loss: 0.1372\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1277 - val_loss: 0.1329\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1287\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1201 - val_loss: 0.1252\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1167 - val_loss: 0.1219\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1137 - val_loss: 0.1185\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1106 - val_loss: 0.1160\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1078 - val_loss: 0.1126\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1051 - val_loss: 0.1097\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 0.1073\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1003 - val_loss: 0.1047\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0980 - val_loss: 0.1027\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0959 - val_loss: 0.1007\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0940 - val_loss: 0.0979\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0918 - val_loss: 0.0963\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 0.0945\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0883 - val_loss: 0.0923\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0866 - val_loss: 0.0906\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0850 - val_loss: 0.0896\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0836 - val_loss: 0.0875\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0820 - val_loss: 0.0858\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0805 - val_loss: 0.0841\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0790 - val_loss: 0.0828\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0776 - val_loss: 0.0811\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 0.0800\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0752 - val_loss: 0.0786\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0738 - val_loss: 0.0774\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0760\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0748\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "Accuracy: 98.71%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 30]\n",
    "extreme_data = data[data['rainfall'] > 30]\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "def create_model(reg_strength):\n",
    "    input_layer = Input(shape=(3,))\n",
    "    encoded = Dense(16, activation='relu', kernel_regularizer='l2', bias_regularizer='l2', activity_regularizer='l2', kernel_initializer='he_uniform')(input_layer)\n",
    "    decoded = Dense(3, activation=None, kernel_initializer='he_uniform')(encoded)\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='logcosh', metrics=['mse'])\n",
    "    return autoencoder\n",
    "\n",
    "# Create the KerasRegressor\n",
    "keras_reg = KerasRegressor(build_fn=create_model)\n",
    "\n",
    "# Define the grid search parameters\n",
    "reg_strengths = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = dict(reg_strength=reg_strengths)\n",
    "\n",
    "# Perform the grid search\n",
    "grid = GridSearchCV(estimator=keras_reg, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_result = grid.fit(normal_data[['windspeed', 'tpw', 'rainfall']].values, normal_data[['windspeed', 'tpw', 'rainfall']].values)\n",
    "\n",
    "# Print the best regularization strength found\n",
    "print(\"Best reg_strength: {:.3f} using {}\".format(grid_result.best_params_['reg_strength'], grid_result.best_params_))\n",
    "\n",
    "# Define the autoencoder architecture with the best regularization strength found by grid search\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu', kernel_regularizer='l2', bias_regularizer='l2', activity_regularizer='l2', kernel_initializer='he_uniform')(input_layer)\n",
    "decoded = Dense(3, activation=None, kernel_initializer='he_uniform')(encoded)\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='logcosh')\n",
    "\n",
    "# Train the model\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "history = autoencoder.fit(train_data, train_data, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 9.6\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "data['actual_rainfall_class'] = np.where(data['rainfall'] > 30, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "297d2868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shine\\AppData\\Local\\Temp\\ipykernel_25120\\3824242652.py:34: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasRegressor(build_fn=create_model, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -5.645572 using {'reg_strength': 10}\n",
      "Best regularization strength: 10.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 30]\n",
    "extreme_data = data[data['rainfall'] > 30]\n",
    "\n",
    "# Plot the normal and extreme rainfall data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar('Normal', len(normal_data), color='blue')\n",
    "plt.bar('Extreme', len(extreme_data), color='red')\n",
    "plt.title('Rainfall Data')\n",
    "plt.xlabel('Rainfall Type')\n",
    "plt.ylabel('Number of Days')\n",
    "plt.show()\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "def create_model(reg_strength):\n",
    "    input_layer = Input(shape=(3,))\n",
    "    encoded = Dense(16, activation='relu', kernel_regularizer='l2', bias_regularizer='l2', activity_regularizer='l2', kernel_constraint='max_norm', bias_constraint='max_norm')(input_layer)\n",
    "    decoded = Dense(3, activation=None)(encoded)\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='logcosh')\n",
    "    return autoencoder\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the grid search parameters\n",
    "reg_strengths = [0.01, 0.1, 1, 10]\n",
    "param_grid = dict(reg_strength=reg_strengths)\n",
    "\n",
    "# Perform grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(normal_data[['windspeed', 'tpw', 'rainfall']].values, normal_data[['windspeed', 'tpw', 'rainfall']].values)\n",
    "\n",
    "# Print the best results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "best_reg_strength = grid_result.best_params_['reg_strength']\n",
    "print(\"Best regularization strength: %f\" % (best_reg_strength))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b600844",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regularizers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define the autoencoder architecture with the best regularization strength\u001b[39;00m\n\u001b[0;32m      2\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,))\n\u001b[1;32m----> 3\u001b[0m encoded \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_regularizer\u001b[38;5;241m=\u001b[39m\u001b[43mregularizers\u001b[49m\u001b[38;5;241m.\u001b[39ml2(best_reg_strength))(input_layer)\n\u001b[0;32m      4\u001b[0m decoded \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)(encoded)\n\u001b[0;32m      5\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m Model(input_layer, decoded)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'regularizers' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the autoencoder architecture with the best regularization strength\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu', kernel_regularizer=regularizers.l2(best_reg_strength))(input_layer)\n",
    "decoded = Dense(3, activation=None)(encoded)\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='logcosh')\n",
    "history = autoencoder.fit(train_data, train_data, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 9.6\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "data['actual_rainfall_class'] = np.where(data['rainfall'] > 30, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0b460098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>predicted_rainfall</th>\n",
       "      <th>error</th>\n",
       "      <th>rainfall_class</th>\n",
       "      <th>actual_rainfall_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/1/2000</td>\n",
       "      <td>12.245595</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>4.688589</td>\n",
       "      <td>4.757562</td>\n",
       "      <td>0.068973</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/2/2000</td>\n",
       "      <td>12.825491</td>\n",
       "      <td>0.044874</td>\n",
       "      <td>11.330589</td>\n",
       "      <td>11.277840</td>\n",
       "      <td>0.052750</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/3/2000</td>\n",
       "      <td>12.921664</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>6.894713</td>\n",
       "      <td>6.926691</td>\n",
       "      <td>0.031977</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/4/2000</td>\n",
       "      <td>15.149001</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>4.613324</td>\n",
       "      <td>4.704754</td>\n",
       "      <td>0.091430</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/5/2000</td>\n",
       "      <td>18.495907</td>\n",
       "      <td>0.139491</td>\n",
       "      <td>8.111635</td>\n",
       "      <td>8.161212</td>\n",
       "      <td>0.049577</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>9/26/2020</td>\n",
       "      <td>5.577215</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>4.467977</td>\n",
       "      <td>4.492797</td>\n",
       "      <td>0.024821</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>9/27/2020</td>\n",
       "      <td>5.184293</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>1.572182</td>\n",
       "      <td>1.649051</td>\n",
       "      <td>0.076869</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>9/28/2020</td>\n",
       "      <td>4.469007</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>1.819019</td>\n",
       "      <td>1.886024</td>\n",
       "      <td>0.067005</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>9/29/2020</td>\n",
       "      <td>4.259090</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>2.198017</td>\n",
       "      <td>2.256318</td>\n",
       "      <td>0.058301</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>9/30/2020</td>\n",
       "      <td>5.513838</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>12.713634</td>\n",
       "      <td>12.581615</td>\n",
       "      <td>0.132018</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2562 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall  predicted_rainfall     error  \\\n",
       "0      6/1/2000  12.245595  0.033285   4.688589            4.757562  0.068973   \n",
       "1      6/2/2000  12.825491  0.044874  11.330589           11.277840  0.052750   \n",
       "2      6/3/2000  12.921664  0.010243   6.894713            6.926691  0.031977   \n",
       "3      6/4/2000  15.149001  0.036881   4.613324            4.704754  0.091430   \n",
       "4      6/5/2000  18.495907  0.139491   8.111635            8.161212  0.049577   \n",
       "...         ...        ...       ...        ...                 ...       ...   \n",
       "2557  9/26/2020   5.577215  0.009963   4.467977            4.492797  0.024821   \n",
       "2558  9/27/2020   5.184293  0.002341   1.572182            1.649051  0.076869   \n",
       "2559  9/28/2020   4.469007  0.000867   1.819019            1.886024  0.067005   \n",
       "2560  9/29/2020   4.259090  0.001416   2.198017            2.256318  0.058301   \n",
       "2561  9/30/2020   5.513838  0.002474  12.713634           12.581615  0.132018   \n",
       "\n",
       "     rainfall_class actual_rainfall_class  \n",
       "0            Normal                Normal  \n",
       "1            Normal                Normal  \n",
       "2            Normal                Normal  \n",
       "3            Normal                Normal  \n",
       "4            Normal                Normal  \n",
       "...             ...                   ...  \n",
       "2557         Normal                Normal  \n",
       "2558         Normal                Normal  \n",
       "2559         Normal                Normal  \n",
       "2560         Normal                Normal  \n",
       "2561         Normal                Normal  \n",
       "\n",
       "[2562 rows x 8 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7e4d5d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall  predicted_rainfall     error  \\\n",
      "41    7/12/2000  23.003010  0.016358  33.846075           33.439987  0.406088   \n",
      "552    8/4/2004  21.769238  0.043448  44.404364           41.227684  3.176680   \n",
      "876   6/23/2007  23.908970  0.331606  49.356381           44.983994  4.372388   \n",
      "900   7/17/2007  19.211468  0.030241  32.941690           32.505436  0.436254   \n",
      "920    8/6/2007  17.366877  0.022628  31.609187           31.194632  0.414556   \n",
      "1033  7/28/2008  20.585272  0.014900  32.559895           32.160690  0.399205   \n",
      "1143  7/16/2009  21.342918  0.037500  38.754120           37.393295  1.360825   \n",
      "1144  7/17/2009  21.451813  0.105873  39.511546           37.919037  1.592509   \n",
      "1145  7/18/2009  16.981620  0.104322  38.342482           36.145439  2.197043   \n",
      "1343   6/2/2011  12.654827  0.550524  34.993438           32.962402  2.031035   \n",
      "1388  7/17/2011  19.214113  0.057870  45.056388           41.087616  3.968772   \n",
      "1409   8/7/2011  15.895408  0.019959  31.262259           30.806091  0.456168   \n",
      "1436   9/3/2011  17.204716  0.025290  32.701293           32.189137  0.512156   \n",
      "1481  6/18/2012  16.490360  0.060908  47.977786           42.411537  5.566248   \n",
      "1482  6/19/2012  12.745975  0.004308  36.064092           33.689983  2.374109   \n",
      "1531   8/7/2012  11.919377  0.026516  33.012284           31.485617  1.526667   \n",
      "1586   6/1/2013  11.123766  0.174906  30.608754           29.717476  0.891278   \n",
      "1619   7/4/2013  16.945307  0.008078  34.218388           33.407997  0.810391   \n",
      "1648   8/2/2013  20.746782  0.006965  30.364948           30.008514  0.356434   \n",
      "1769   8/1/2014  20.457104  0.029008  35.467946           34.883175  0.584771   \n",
      "1980  6/29/2016  21.279696  0.040968  30.296499           29.945303  0.351196   \n",
      "2271  8/15/2018  21.421936  0.086819  41.892355           39.487793  2.404562   \n",
      "2272  8/16/2018  22.673685  0.060061  47.362358           43.387486  3.974873   \n",
      "2384   8/6/2019  18.526226  0.027500  30.351481           29.979275  0.372206   \n",
      "2385   8/7/2019  21.010150  0.027716  44.681781           41.241421  3.440360   \n",
      "2386   8/8/2019  22.589184  0.024273  47.165503           43.238228  3.927275   \n",
      "2388  8/10/2019  20.276800  0.043114  58.431061           50.175900  8.255162   \n",
      "2389  8/11/2019  17.905354  0.011177  30.734900           30.350887  0.384012   \n",
      "2505   8/5/2020  27.123528  0.037294  32.042267           31.700268  0.341999   \n",
      "2507   8/7/2020  19.529327  0.108583  42.017340           39.147182  2.870158   \n",
      "2508   8/8/2020  17.372940  0.220405  35.905335           34.620491  1.284844   \n",
      "2551  9/20/2020  18.062815  0.326716  34.073259           33.489792  0.583468   \n",
      "\n",
      "     rainfall_class actual_rainfall_class  \n",
      "41           Normal               Extreme  \n",
      "552          Normal               Extreme  \n",
      "876          Normal               Extreme  \n",
      "900          Normal               Extreme  \n",
      "920          Normal               Extreme  \n",
      "1033         Normal               Extreme  \n",
      "1143         Normal               Extreme  \n",
      "1144         Normal               Extreme  \n",
      "1145         Normal               Extreme  \n",
      "1343         Normal               Extreme  \n",
      "1388         Normal               Extreme  \n",
      "1409         Normal               Extreme  \n",
      "1436         Normal               Extreme  \n",
      "1481         Normal               Extreme  \n",
      "1482         Normal               Extreme  \n",
      "1531         Normal               Extreme  \n",
      "1586         Normal               Extreme  \n",
      "1619         Normal               Extreme  \n",
      "1648         Normal               Extreme  \n",
      "1769         Normal               Extreme  \n",
      "1980         Normal               Extreme  \n",
      "2271         Normal               Extreme  \n",
      "2272         Normal               Extreme  \n",
      "2384         Normal               Extreme  \n",
      "2385         Normal               Extreme  \n",
      "2386         Normal               Extreme  \n",
      "2388         Normal               Extreme  \n",
      "2389         Normal               Extreme  \n",
      "2505         Normal               Extreme  \n",
      "2507         Normal               Extreme  \n",
      "2508         Normal               Extreme  \n",
      "2551         Normal               Extreme  \n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = data[data['rainfall_class'] != data['actual_rainfall_class']]\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a3f29473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regularization strength: 0.01\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 7ms/step - loss: 81.8210 - val_loss: 63.9346\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 46.0786 - val_loss: 29.9775\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 17.6711 - val_loss: 8.8465\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.4824 - val_loss: 2.7178\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.6524 - val_loss: 1.5855\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.9964 - val_loss: 1.0073\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.6122 - val_loss: 0.6118\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3451 - val_loss: 0.3218\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1770 - val_loss: 0.1867\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1268 - val_loss: 0.1498\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 0.1354\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1058 - val_loss: 0.1266\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1000 - val_loss: 0.1186\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0951 - val_loss: 0.1120\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0910 - val_loss: 0.1068\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0867 - val_loss: 0.1015\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0831 - val_loss: 0.0969\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0795 - val_loss: 0.0924\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0762 - val_loss: 0.0881\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0842\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0704 - val_loss: 0.0812\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0678 - val_loss: 0.0771\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0652 - val_loss: 0.0738\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0630 - val_loss: 0.0716\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0606 - val_loss: 0.0681\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0587 - val_loss: 0.0660\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0568 - val_loss: 0.0638\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.0611\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0533 - val_loss: 0.0600\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.0573\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.0559\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.0548\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0477 - val_loss: 0.0525\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.0504\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.0503\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.0483\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.0468\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.0459\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.0453\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.0441\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.0427\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.0421\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.0415\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.0405\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.0395\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.0387\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0389\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0381\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0371\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0368\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0359\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0361\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0352\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0347\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0342\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 0.0338\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0319 - val_loss: 0.0332\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0328\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0324\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0323\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0319\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0315\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0314\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.0309\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0306\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0304\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0304\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0298\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.0296\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0295\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0291\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0282 - val_loss: 0.0289\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0288\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0285\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0284\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0274 - val_loss: 0.0285\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0278\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0275\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0273\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0270\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0270\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0266\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0265\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0263\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0259\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0257\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0256\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0254\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.0251\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0249\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0247\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0244\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0242\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0242\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.0238\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0236\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0233\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0232\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0230\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "Accuracy: 98.71%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 30]\n",
    "extreme_data = data[data['rainfall'] > 30]\n",
    "\n",
    "# Define the weight for the extreme cases\n",
    "extreme_weight = len(normal_data) / len(extreme_data)\n",
    "\n",
    "# Define a function to calculate the weighted loss\n",
    "def weighted_loss(extreme_weight):\n",
    "    def loss(y_true, y_pred):\n",
    "        normal_loss = K.mean(K.square(y_true - y_pred))\n",
    "        extreme_loss = K.mean(K.square(y_true - y_pred)) * extreme_weight\n",
    "        return tf.where(y_true > 30, extreme_loss, normal_loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Define a function to find the best regularization strength\n",
    "def find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size):\n",
    "    best_reg_strength = None\n",
    "    best_loss = float('inf')\n",
    "    for reg_strength in reg_strengths:\n",
    "        # Define the autoencoder architecture\n",
    "        input_layer = Input(shape=(3,))\n",
    "        encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=reg_strength))(input_layer)\n",
    "        decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "        # Create the autoencoder\n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "        # Compile the autoencoder with the weighted loss function\n",
    "        autoencoder.compile(optimizer='adam', loss=weighted_loss(extreme_weight))\n",
    "\n",
    "        # Train the model\n",
    "        history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Calculate the validation loss\n",
    "        val_loss = np.mean(history.history['val_loss'])\n",
    "\n",
    "        # Update the best regularization strength and loss\n",
    "        if val_loss < best_loss:\n",
    "            best_reg_strength = reg_strength\n",
    "            best_loss = val_loss\n",
    "\n",
    "    print('Best regularization strength:', best_reg_strength)\n",
    "    return best_reg_strength\n",
    "\n",
    "# Find the best regularization strength\n",
    "reg_strengths = [0.01, 0.1, 1, 10]\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "best_reg_strength = find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size)\n",
    "\n",
    "# Define the autoencoder architecture with the best regularization strength\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "# Create the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder with the weighted loss function\n",
    "autoencoder.compile(optimizer='adam', loss=weighted_loss(extreme_weight))\n",
    "\n",
    "# Train the model with the majority class\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 9.6\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "data['actual_rainfall_class'] = np.where(data['rainfall'] > 30, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e7bc3792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall  predicted_rainfall     error  \\\n",
      "41    7/12/2000  23.003010  0.016358  33.846075           33.856327  0.010252   \n",
      "552    8/4/2004  21.769238  0.043448  44.404364           44.422832  0.018468   \n",
      "876   6/23/2007  23.908970  0.331606  49.356381           49.369034  0.012652   \n",
      "900   7/17/2007  19.211468  0.030241  32.941690           32.955303  0.013613   \n",
      "920    8/6/2007  17.366877  0.022628  31.609187           31.623812  0.014624   \n",
      "1033  7/28/2008  20.585272  0.014900  32.559895           32.571793  0.011897   \n",
      "1143  7/16/2009  21.342918  0.037500  38.754120           38.769569  0.015449   \n",
      "1144  7/17/2009  21.451813  0.105873  39.511546           39.527405  0.015859   \n",
      "1145  7/18/2009  16.981620  0.104322  38.342482           38.322002  0.020480   \n",
      "1343   6/2/2011  12.654827  0.550524  34.993438           34.915676  0.077761   \n",
      "1388  7/17/2011  19.214113  0.057870  45.056388           45.013912  0.042476   \n",
      "1409   8/7/2011  15.895408  0.019959  31.262259           31.278191  0.015931   \n",
      "1436   9/3/2011  17.204716  0.025290  32.701293           32.716854  0.015561   \n",
      "1481  6/18/2012  16.490360  0.060908  47.977786           47.847351  0.130434   \n",
      "1482  6/19/2012  12.745975  0.004308  36.064092           35.977306  0.086786   \n",
      "1531   8/7/2012  11.919377  0.026516  33.012284           32.939484  0.072801   \n",
      "1586   6/1/2013  11.123766  0.174906  30.608754           30.543880  0.064874   \n",
      "1619   7/4/2013  16.945307  0.008078  34.218388           34.235291  0.016903   \n",
      "1648   8/2/2013  20.746782  0.006965  30.364948           30.375134  0.010185   \n",
      "1769   8/1/2014  20.457104  0.029008  35.467946           35.482021  0.014076   \n",
      "1980  6/29/2016  21.279696  0.040968  30.296499           30.306072  0.009573   \n",
      "2271  8/15/2018  21.421936  0.086819  41.892355           41.909916  0.017561   \n",
      "2272  8/16/2018  22.673685  0.060061  47.362358           47.369457  0.007099   \n",
      "2384   8/6/2019  18.526226  0.027500  30.351481           30.363995  0.012514   \n",
      "2385   8/7/2019  21.010150  0.027716  44.681781           44.681286  0.000495   \n",
      "2386   8/8/2019  22.589184  0.024273  47.165503           47.172848  0.007345   \n",
      "2387   8/9/2019  21.575483  0.042829  80.636074           80.276146  0.359928   \n",
      "2388  8/10/2019  20.276800  0.043114  58.431061           58.272968  0.158093   \n",
      "2389  8/11/2019  17.905354  0.011177  30.734900           30.748343  0.013443   \n",
      "2505   8/5/2020  27.123528  0.037294  32.042267           32.046902  0.004635   \n",
      "2507   8/7/2020  19.529327  0.108583  42.017340           42.012939  0.004401   \n",
      "2508   8/8/2020  17.372940  0.220405  35.905335           35.918304  0.012970   \n",
      "2551  9/20/2020  18.062815  0.326716  34.073259           34.088825  0.015566   \n",
      "\n",
      "     rainfall_class actual_rainfall_class  \n",
      "41           Normal               Extreme  \n",
      "552          Normal               Extreme  \n",
      "876          Normal               Extreme  \n",
      "900          Normal               Extreme  \n",
      "920          Normal               Extreme  \n",
      "1033         Normal               Extreme  \n",
      "1143         Normal               Extreme  \n",
      "1144         Normal               Extreme  \n",
      "1145         Normal               Extreme  \n",
      "1343         Normal               Extreme  \n",
      "1388         Normal               Extreme  \n",
      "1409         Normal               Extreme  \n",
      "1436         Normal               Extreme  \n",
      "1481         Normal               Extreme  \n",
      "1482         Normal               Extreme  \n",
      "1531         Normal               Extreme  \n",
      "1586         Normal               Extreme  \n",
      "1619         Normal               Extreme  \n",
      "1648         Normal               Extreme  \n",
      "1769         Normal               Extreme  \n",
      "1980         Normal               Extreme  \n",
      "2271         Normal               Extreme  \n",
      "2272         Normal               Extreme  \n",
      "2384         Normal               Extreme  \n",
      "2385         Normal               Extreme  \n",
      "2386         Normal               Extreme  \n",
      "2387         Normal               Extreme  \n",
      "2388         Normal               Extreme  \n",
      "2389         Normal               Extreme  \n",
      "2505         Normal               Extreme  \n",
      "2507         Normal               Extreme  \n",
      "2508         Normal               Extreme  \n",
      "2551         Normal               Extreme  \n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = data[data['rainfall_class'] != data['actual_rainfall_class']]\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1a00fbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regularization strength: 0.1\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.6782 - val_loss: 0.5597\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4458 - val_loss: 0.3687\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3007 - val_loss: 0.2602\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2167 - val_loss: 0.1948\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1636 - val_loss: 0.1512\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1275 - val_loss: 0.1212\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1025 - val_loss: 0.1000\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0830 - val_loss: 0.0804\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0676 - val_loss: 0.0668\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0569 - val_loss: 0.0568\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0488 - val_loss: 0.0489\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0427 - val_loss: 0.0426\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0380 - val_loss: 0.0381\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0344 - val_loss: 0.0348\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0316 - val_loss: 0.0321\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0293 - val_loss: 0.0299\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0275 - val_loss: 0.0282\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0259 - val_loss: 0.0266\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0245 - val_loss: 0.0254\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0234 - val_loss: 0.0243\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0225 - val_loss: 0.0232\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 0.0225\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0208 - val_loss: 0.0217\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0201 - val_loss: 0.0210\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0195 - val_loss: 0.0204\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0189 - val_loss: 0.0198\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0184 - val_loss: 0.0191\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0179 - val_loss: 0.0186\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0174 - val_loss: 0.0182\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0169 - val_loss: 0.0177\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0165 - val_loss: 0.0172\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0169\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0157 - val_loss: 0.0164\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0160\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0149 - val_loss: 0.0156\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0146 - val_loss: 0.0152\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0143 - val_loss: 0.0148\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0145\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0136 - val_loss: 0.0142\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0138\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0136\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0130\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0127\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0124\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0122\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.0119\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0117\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0106 - val_loss: 0.0110\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0102 - val_loss: 0.0106\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0098 - val_loss: 0.0101\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0096 - val_loss: 0.0100\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0095 - val_loss: 0.0099\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0091 - val_loss: 0.0094\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0090 - val_loss: 0.0093\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0088 - val_loss: 0.0091\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0087 - val_loss: 0.0091\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0085 - val_loss: 0.0088\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0083 - val_loss: 0.0085\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0081 - val_loss: 0.0083\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0079 - val_loss: 0.0081\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0077 - val_loss: 0.0079\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0075 - val_loss: 0.0078\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0074 - val_loss: 0.0076\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0072 - val_loss: 0.0074\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0064 - val_loss: 0.0066\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0063 - val_loss: 0.0065\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0062 - val_loss: 0.0064\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0061 - val_loss: 0.0063\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0060 - val_loss: 0.0062\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0061\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0058 - val_loss: 0.0060\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0057 - val_loss: 0.0059\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0056 - val_loss: 0.0058\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0055 - val_loss: 0.0057\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0054 - val_loss: 0.0056\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0053 - val_loss: 0.0055\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0051 - val_loss: 0.0053\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0050 - val_loss: 0.0051\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0050 - val_loss: 0.0051\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.0050\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0048 - val_loss: 0.0049\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0048 - val_loss: 0.0049\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0048\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.0048\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.0047\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "81/81 [==============================] - 0s 663us/step\n",
      "Accuracy: 99.92%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 50]\n",
    "extreme_data = data[data['rainfall'] > 50]\n",
    "\n",
    "def weighted_loss(normal_weight=0.005, extreme_weight=0.001):\n",
    "    def loss(y_true, y_pred):\n",
    "        normal_loss = K.mean(K.square(y_true - y_pred))\n",
    "        extreme_loss = K.mean(K.square(y_true - y_pred) * K.cast(y_true > 50, 'float32'))\n",
    "        return K.mean(normal_weight * normal_loss + extreme_weight * extreme_loss)\n",
    "    return loss\n",
    "\n",
    "# Define a function to find the best regularization strength\n",
    "def find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size):\n",
    "    best_reg_strength = None\n",
    "    best_loss = float('inf')\n",
    "    for reg_strength in reg_strengths:\n",
    "        # Define the autoencoder architecture\n",
    "        input_layer = Input(shape=(3,))\n",
    "        encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=reg_strength))(input_layer)\n",
    "        decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "        # Create the autoencoder\n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "        # Compile the autoencoder with the weighted loss function\n",
    "        autoencoder.compile(optimizer='adam', loss=weighted_loss(normal_weight, extreme_weight))\n",
    "\n",
    "        # Train the model\n",
    "        history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Calculate the validation loss\n",
    "        val_loss = np.mean(history.history['val_loss'])\n",
    "\n",
    "        # Update the best regularization strength and loss\n",
    "        if val_loss < best_loss:\n",
    "            best_reg_strength = reg_strength\n",
    "            best_loss = val_loss\n",
    "\n",
    "    print('Best regularization strength:', best_reg_strength)\n",
    "    return best_reg_strength\n",
    "\n",
    "# Find the best regularization strength\n",
    "reg_strengths = [0.01, 0.1, 1, 10]\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "best_reg_strength = find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size)\n",
    "\n",
    "# Define the autoencoder architecture with the best regularization strength\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "# Create the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder with the weighted loss function\n",
    "autoencoder.compile(optimizer='adam', loss=weighted_loss())\n",
    "\n",
    "# Train the model with the majority class\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 9.6\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "data['actual_rainfall_class'] = np.where(data['rainfall'] > 50, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8868da4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall  predicted_rainfall     error  \\\n",
      "2387   8/9/2019  21.575483  0.042829  80.636074           71.615608  9.020466   \n",
      "2388  8/10/2019  20.276800  0.043114  58.431061           54.596336  3.834725   \n",
      "\n",
      "     rainfall_class actual_rainfall_class  \n",
      "2387         Normal               Extreme  \n",
      "2388         Normal               Extreme  \n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = data[data['rainfall_class'] != data['actual_rainfall_class']]\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb039910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>21.575483</td>\n",
       "      <td>0.042829</td>\n",
       "      <td>80.636074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>8/10/2019</td>\n",
       "      <td>20.276800</td>\n",
       "      <td>0.043114</td>\n",
       "      <td>58.431061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall\n",
       "2387   8/9/2019  21.575483  0.042829  80.636074\n",
       "2388  8/10/2019  20.276800  0.043114  58.431061"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f481471e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>predicted_rainfall</th>\n",
       "      <th>error</th>\n",
       "      <th>rainfall_class</th>\n",
       "      <th>actual_rainfall_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>6/23/2007</td>\n",
       "      <td>23.908970</td>\n",
       "      <td>0.331606</td>\n",
       "      <td>49.356381</td>\n",
       "      <td>40.345200</td>\n",
       "      <td>9.011182</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>7/17/2011</td>\n",
       "      <td>19.214113</td>\n",
       "      <td>0.057870</td>\n",
       "      <td>45.056388</td>\n",
       "      <td>36.048218</td>\n",
       "      <td>9.008170</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>6/18/2012</td>\n",
       "      <td>16.490360</td>\n",
       "      <td>0.060908</td>\n",
       "      <td>47.977786</td>\n",
       "      <td>37.389614</td>\n",
       "      <td>10.588171</td>\n",
       "      <td>Extreme</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>21.575483</td>\n",
       "      <td>0.042829</td>\n",
       "      <td>80.636074</td>\n",
       "      <td>61.865894</td>\n",
       "      <td>18.770180</td>\n",
       "      <td>Extreme</td>\n",
       "      <td>Extreme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>8/10/2019</td>\n",
       "      <td>20.276800</td>\n",
       "      <td>0.043114</td>\n",
       "      <td>58.431061</td>\n",
       "      <td>45.799969</td>\n",
       "      <td>12.631092</td>\n",
       "      <td>Extreme</td>\n",
       "      <td>Extreme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall  predicted_rainfall  \\\n",
       "876   6/23/2007  23.908970  0.331606  49.356381           40.345200   \n",
       "1388  7/17/2011  19.214113  0.057870  45.056388           36.048218   \n",
       "1481  6/18/2012  16.490360  0.060908  47.977786           37.389614   \n",
       "2387   8/9/2019  21.575483  0.042829  80.636074           61.865894   \n",
       "2388  8/10/2019  20.276800  0.043114  58.431061           45.799969   \n",
       "\n",
       "          error rainfall_class actual_rainfall_class  \n",
       "876    9.011182         Normal                Normal  \n",
       "1388   9.008170         Normal                Normal  \n",
       "1481  10.588171        Extreme                Normal  \n",
       "2387  18.770180        Extreme               Extreme  \n",
       "2388  12.631092        Extreme               Extreme  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"error\"]>9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7031a075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regularization strength: 0.01\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 4ms/step - loss: 138.4827 - val_loss: 119.9598\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 84.3867 - val_loss: 70.2653\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 44.6393 - val_loss: 34.2881\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 19.4167 - val_loss: 14.4217\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.8106 - val_loss: 6.4292\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3.6706 - val_loss: 3.4647\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2.1032 - val_loss: 2.0539\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.2791 - val_loss: 1.2350\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.7795 - val_loss: 0.7691\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4957 - val_loss: 0.5079\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3398 - val_loss: 0.3669\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2572 - val_loss: 0.2933\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2137 - val_loss: 0.2510\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1898 - val_loss: 0.2278\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1758 - val_loss: 0.2125\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1663 - val_loss: 0.2028\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1596 - val_loss: 0.1967\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1544 - val_loss: 0.1870\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1496 - val_loss: 0.1834\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1458 - val_loss: 0.1758\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1416 - val_loss: 0.1701\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1383 - val_loss: 0.1649\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1347 - val_loss: 0.1650\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1316 - val_loss: 0.1570\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1282 - val_loss: 0.1549\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1254 - val_loss: 0.1524\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1224 - val_loss: 0.1449\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1194 - val_loss: 0.1420\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1169 - val_loss: 0.1375\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1142 - val_loss: 0.1355\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1113 - val_loss: 0.1307\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1089 - val_loss: 0.1282\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1062 - val_loss: 0.1236\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1038 - val_loss: 0.1195\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1017 - val_loss: 0.1174\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0993 - val_loss: 0.1159\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0969 - val_loss: 0.1106\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0951 - val_loss: 0.1088\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0927 - val_loss: 0.1059\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0907 - val_loss: 0.1039\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0886 - val_loss: 0.1001\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0867 - val_loss: 0.0989\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0849 - val_loss: 0.0965\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0829 - val_loss: 0.0931\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0813 - val_loss: 0.0908\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0795 - val_loss: 0.0884\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0777 - val_loss: 0.0870\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0758 - val_loss: 0.0839\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0743 - val_loss: 0.0822\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0726 - val_loss: 0.0801\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0710 - val_loss: 0.0779\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0695 - val_loss: 0.0768\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0680 - val_loss: 0.0750\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0667 - val_loss: 0.0727\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0653 - val_loss: 0.0716\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0640 - val_loss: 0.0694\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0627 - val_loss: 0.0681\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0614 - val_loss: 0.0665\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0602 - val_loss: 0.0649\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0591 - val_loss: 0.0637\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0581 - val_loss: 0.0625\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0570 - val_loss: 0.0613\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0562 - val_loss: 0.0604\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0552 - val_loss: 0.0597\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0544 - val_loss: 0.0589\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0535 - val_loss: 0.0574\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0527 - val_loss: 0.0563\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0519 - val_loss: 0.0552\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0512 - val_loss: 0.0546\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0506 - val_loss: 0.0539\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0499 - val_loss: 0.0530\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0493 - val_loss: 0.0523\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0487 - val_loss: 0.0520\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0481 - val_loss: 0.0508\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0475 - val_loss: 0.0502\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0470 - val_loss: 0.0497\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0464 - val_loss: 0.0488\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0460 - val_loss: 0.0482\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0453 - val_loss: 0.0476\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0449 - val_loss: 0.0471\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0443 - val_loss: 0.0466\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0440 - val_loss: 0.0461\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0435 - val_loss: 0.0456\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0430 - val_loss: 0.0451\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0426 - val_loss: 0.0446\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0422 - val_loss: 0.0441\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0418 - val_loss: 0.0440\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0415 - val_loss: 0.0435\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0411 - val_loss: 0.0430\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0407 - val_loss: 0.0425\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0403 - val_loss: 0.0420\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0400 - val_loss: 0.0416\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0396 - val_loss: 0.0412\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0392 - val_loss: 0.0407\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0390 - val_loss: 0.0404\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0386 - val_loss: 0.0400\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0383 - val_loss: 0.0395\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0380 - val_loss: 0.0393\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0376 - val_loss: 0.0389\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0372 - val_loss: 0.0385\n",
      "81/81 [==============================] - 0s 563us/step\n",
      "Accuracy: 99.92%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 50]\n",
    "extreme_data = data[data['rainfall'] > 50]\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    normal_loss = K.mean(K.square(y_true[y_true < 50.0] - y_pred[y_true < 50.0]))\n",
    "    extreme_loss = K.mean(K.square(y_true[y_true >= 50.0] - y_pred[y_true >= 50.0]))\n",
    "    return 1* normal_loss + 0 * extreme_loss\n",
    "\n",
    "\n",
    "# Define a function to find the best regularization strength\n",
    "def find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size):\n",
    "    best_reg_strength = None\n",
    "    best_loss = float('inf')\n",
    "    for reg_strength in reg_strengths:\n",
    "        # Define the autoencoder architecture\n",
    "        input_layer = Input(shape=(3,))\n",
    "        encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=reg_strength))(input_layer)\n",
    "        decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "        # Create the autoencoder\n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "        # Compile the autoencoder with the weighted loss function\n",
    "        autoencoder.compile(optimizer='adam', loss=weighted_loss(normal_weight, extreme_weight))\n",
    "\n",
    "        # Train the model\n",
    "        history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Calculate the validation loss\n",
    "        val_loss = np.mean(history.history['val_loss'])\n",
    "\n",
    "        # Update the best regularization strength and loss\n",
    "        if val_loss < best_loss:\n",
    "            best_reg_strength = reg_strength\n",
    "            best_loss = val_loss\n",
    "\n",
    "    print('Best regularization strength:', best_reg_strength)\n",
    "    return best_reg_strength\n",
    "\n",
    "# Find the best regularization strength\n",
    "reg_strengths = [0.01, 0.1, 1, 10]\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "best_reg_strength = find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size)\n",
    "\n",
    "# Define the autoencoder architecture with the best regularization strength\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "# Create the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder with the weighted loss function\n",
    "autoencoder.compile(optimizer='adam', loss=custom_loss)\n",
    "\n",
    "# Train the model with the majority class\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 9.6\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "data['actual_rainfall_class'] = np.where(data['rainfall'] > 50, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a7c313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cba3a357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [date, windspeed, tpw, rainfall, predicted_rainfall, error, rainfall_class, actual_rainfall_class]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = data[data['rainfall_class'] != data['actual_rainfall_class']]\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5f86f19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>21.575483</td>\n",
       "      <td>0.042829</td>\n",
       "      <td>80.636074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>8/10/2019</td>\n",
       "      <td>20.276800</td>\n",
       "      <td>0.043114</td>\n",
       "      <td>58.431061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall\n",
       "2387   8/9/2019  21.575483  0.042829  80.636074\n",
       "2388  8/10/2019  20.276800  0.043114  58.431061"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "dda600b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regularization strength: 0.01\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 3ms/step - loss: 0.0915 - val_loss: 0.0676\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0543 - val_loss: 0.0463\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0394 - val_loss: 0.0354\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0304 - val_loss: 0.0276\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0238 - val_loss: 0.0217\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0189 - val_loss: 0.0174\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0141\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0115\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0100 - val_loss: 0.0094\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 0.0079\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0052 - val_loss: 0.0053\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0048\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0031 - val_loss: 0.0033\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.9026e-04 - val_loss: 9.9735e-04\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.6508e-04 - val_loss: 9.7696e-04\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.3902e-04 - val_loss: 9.4965e-04\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.1791e-04 - val_loss: 9.2771e-04\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.9210e-04 - val_loss: 9.0425e-04\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.7441e-04 - val_loss: 8.8615e-04\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.5284e-04 - val_loss: 8.6133e-04\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.3267e-04 - val_loss: 8.4385e-04\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.1483e-04 - val_loss: 8.2240e-04\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.8821e-04 - val_loss: 7.9458e-04\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.6864e-04 - val_loss: 7.6980e-04\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.4398e-04 - val_loss: 7.4705e-04\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.2692e-04 - val_loss: 7.3223e-04\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.0798e-04 - val_loss: 7.1279e-04\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.9109e-04 - val_loss: 6.9391e-04\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.7679e-04 - val_loss: 6.7622e-04\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.5953e-04 - val_loss: 6.6727e-04\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.4433e-04 - val_loss: 6.5040e-04\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.3316e-04 - val_loss: 6.3295e-04\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.1874e-04 - val_loss: 6.2795e-04\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.0670e-04 - val_loss: 6.1487e-04\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.9435e-04 - val_loss: 5.9709e-04\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.8236e-04 - val_loss: 5.8981e-04\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.7161e-04 - val_loss: 5.7473e-04\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.5976e-04 - val_loss: 5.7023e-04\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.5003e-04 - val_loss: 5.5293e-04\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.4029e-04 - val_loss: 5.4145e-04\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.3009e-04 - val_loss: 5.3437e-04\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.2109e-04 - val_loss: 5.2268e-04\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.1103e-04 - val_loss: 5.2051e-04\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.0353e-04 - val_loss: 5.0792e-04\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.9391e-04 - val_loss: 4.9588e-04\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.8614e-04 - val_loss: 4.8761e-04\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.7980e-04 - val_loss: 4.7931e-04\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.7042e-04 - val_loss: 4.7337e-04\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.6375e-04 - val_loss: 4.6604e-04\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.5513e-04 - val_loss: 4.6048e-04\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.4796e-04 - val_loss: 4.5500e-04\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.4436e-04 - val_loss: 4.4200e-04\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.3519e-04 - val_loss: 4.3779e-04\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.2858e-04 - val_loss: 4.3096e-04\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.2156e-04 - val_loss: 4.2508e-04\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.1560e-04 - val_loss: 4.1795e-04\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.0980e-04 - val_loss: 4.1080e-04\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.0307e-04 - val_loss: 4.0895e-04\n",
      "81/81 [==============================] - 0s 588us/step\n",
      "Accuracy: 98.71%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 30]\n",
    "extreme_data = data[data['rainfall'] > 30]\n",
    "\n",
    "def weighted_loss(normal_weight=0.001, extreme_weight=9):\n",
    "    def loss(y_true, y_pred):\n",
    "        normal_loss = K.mean(K.square(y_true - y_pred))\n",
    "        extreme_loss = K.mean(K.square(y_true - y_pred) * K.cast(y_true > 30, 'float32'))\n",
    "        return K.mean(normal_weight * normal_loss + extreme_weight * extreme_loss)\n",
    "    return loss\n",
    "\n",
    "# Define a function to find the best combo of normal_weight and extreme_weight based on the error threshold\n",
    "def find_best_weights(train_data, normal_weights, extreme_weights, num_epochs, batch_size, threshold):\n",
    "    best_combo = None\n",
    "    best_accuracy = 0\n",
    "    for normal_weight in normal_weights:\n",
    "        for extreme_weight in extreme_weights:\n",
    "            # Define the autoencoder architecture with the given weights\n",
    "            input_layer = Input(shape=(3,))\n",
    "            encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "            decoded = Dense(3, activation=None)(encoded)\n",
    "            autoencoder = Model(input_layer, decoded)\n",
    "            autoencoder.compile(optimizer='adam', loss=weighted_loss(normal_weight, extreme_weight))\n",
    "\n",
    "            # Train the model with the majority class\n",
    "            train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "            autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Use the trained autoencoder to predict the rainfall values for all data points\n",
    "            test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "            predicted_data = autoencoder.predict(test_data)\n",
    "            data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "            # Calculate the error between the predicted and actual rainfall values\n",
    "            data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "            # Classify the rainfall data into normal and extreme based on the error threshold\n",
    "            data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "            # Define the actual rainfall class based on the threshold of 11\n",
    "            data['actual_rainfall_class'] = np.where(data['rainfall'] > 30, 'Extreme', 'Normal')\n",
    "\n",
    "            # Calculate the accuracy\n",
    "            num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "            num_total = len(data)\n",
    "            accuracy = num_correct / num_total * 100\n",
    "\n",
    "            # Update the best combo of weights and accuracy\n",
    "            if accuracy > best_accuracy:\n",
    "                best_combo = (normal_weight, extreme_weight)\n",
    "                best_accuracy = accuracy\n",
    "\n",
    "    print('Best combo of weights:', best_combo)\n",
    "    return best_combo\n",
    "\n",
    "# Find the best regularization strength\n",
    "reg_strengths = [0.01, 0.1, 1, 10]\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "best_reg_strength = find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size)\n",
    "\n",
    "# Define the autoencoder architecture with the best regularization strength\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "# Create the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder with the weighted loss function\n",
    "autoencoder.compile(optimizer='adam', loss=weighted_loss())\n",
    "\n",
    "# Train the model with the majority class\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 9.6\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "data['actual_rainfall_class'] = np.where(data['rainfall'] > 30, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d547e70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall  predicted_rainfall     error  \\\n",
      "41    7/12/2000  23.003010  0.016358  33.846075           33.278931  0.567144   \n",
      "552    8/4/2004  21.769238  0.043448  44.404364           42.520748  1.883616   \n",
      "876   6/23/2007  23.908970  0.331606  49.356381           46.652267  2.704114   \n",
      "900   7/17/2007  19.211468  0.030241  32.941690           32.378540  0.563150   \n",
      "920    8/6/2007  17.366877  0.022628  31.609187           31.090982  0.518205   \n",
      "1033  7/28/2008  20.585272  0.014900  32.559895           32.029484  0.530411   \n",
      "1143  7/16/2009  21.342918  0.037500  38.754120           37.848267  0.905854   \n",
      "1144  7/17/2009  21.451813  0.105873  39.511546           38.501362  1.010184   \n",
      "1145  7/18/2009  16.981620  0.104322  38.342482           37.374252  0.968230   \n",
      "1343   6/2/2011  12.654827  0.550524  34.993438           34.264565  0.728873   \n",
      "1388  7/17/2011  19.214113  0.057870  45.056388           42.946659  2.109729   \n",
      "1409   8/7/2011  15.895408  0.019959  31.262259           30.745380  0.516879   \n",
      "1436   9/3/2011  17.204716  0.025290  32.701293           32.128723  0.572570   \n",
      "1481  6/18/2012  16.490360  0.060908  47.977786           45.217716  2.760069   \n",
      "1482  6/19/2012  12.745975  0.004308  36.064092           35.256836  0.807257   \n",
      "1531   8/7/2012  11.919377  0.026516  33.012284           32.369408  0.642877   \n",
      "1586   6/1/2013  11.123766  0.174906  30.608754           30.074017  0.534738   \n",
      "1619   7/4/2013  16.945307  0.008078  34.218388           33.569862  0.648526   \n",
      "1648   8/2/2013  20.746782  0.006965  30.364948           29.942076  0.422872   \n",
      "1769   8/1/2014  20.457104  0.029008  35.467946           34.795971  0.671975   \n",
      "1980  6/29/2016  21.279696  0.040968  30.296499           29.882627  0.413872   \n",
      "2271  8/15/2018  21.421936  0.086819  41.892355           40.456200  1.436155   \n",
      "2272  8/16/2018  22.673685  0.060061  47.362358           44.972633  2.389725   \n",
      "2384   8/6/2019  18.526226  0.027500  30.351481           29.906094  0.445387   \n",
      "2385   8/7/2019  21.010150  0.027716  44.681781           42.715542  1.966239   \n",
      "2386   8/8/2019  22.589184  0.024273  47.165503           44.808353  2.357150   \n",
      "2387   8/9/2019  21.575483  0.042829  80.636074           72.083847  8.552227   \n",
      "2388  8/10/2019  20.276800  0.043114  58.431061           53.906803  4.524258   \n",
      "2389  8/11/2019  17.905354  0.011177  30.734900           30.264463  0.470436   \n",
      "2505   8/5/2020  27.123528  0.037294  32.042267           31.605326  0.436941   \n",
      "2507   8/7/2020  19.529327  0.108583  42.017340           40.479546  1.537795   \n",
      "2508   8/8/2020  17.372940  0.220405  35.905335           35.180668  0.724667   \n",
      "2551  9/20/2020  18.062815  0.326716  34.073259           33.444565  0.628695   \n",
      "\n",
      "     rainfall_class actual_rainfall_class  \n",
      "41           Normal               Extreme  \n",
      "552          Normal               Extreme  \n",
      "876          Normal               Extreme  \n",
      "900          Normal               Extreme  \n",
      "920          Normal               Extreme  \n",
      "1033         Normal               Extreme  \n",
      "1143         Normal               Extreme  \n",
      "1144         Normal               Extreme  \n",
      "1145         Normal               Extreme  \n",
      "1343         Normal               Extreme  \n",
      "1388         Normal               Extreme  \n",
      "1409         Normal               Extreme  \n",
      "1436         Normal               Extreme  \n",
      "1481         Normal               Extreme  \n",
      "1482         Normal               Extreme  \n",
      "1531         Normal               Extreme  \n",
      "1586         Normal               Extreme  \n",
      "1619         Normal               Extreme  \n",
      "1648         Normal               Extreme  \n",
      "1769         Normal               Extreme  \n",
      "1980         Normal               Extreme  \n",
      "2271         Normal               Extreme  \n",
      "2272         Normal               Extreme  \n",
      "2384         Normal               Extreme  \n",
      "2385         Normal               Extreme  \n",
      "2386         Normal               Extreme  \n",
      "2387         Normal               Extreme  \n",
      "2388         Normal               Extreme  \n",
      "2389         Normal               Extreme  \n",
      "2505         Normal               Extreme  \n",
      "2507         Normal               Extreme  \n",
      "2508         Normal               Extreme  \n",
      "2551         Normal               Extreme  \n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = data[data['rainfall_class'] != data['actual_rainfall_class']]\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5e567ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7/12/2000</td>\n",
       "      <td>23.003010</td>\n",
       "      <td>0.016358</td>\n",
       "      <td>33.846075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>8/4/2004</td>\n",
       "      <td>21.769238</td>\n",
       "      <td>0.043448</td>\n",
       "      <td>44.404364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>6/23/2007</td>\n",
       "      <td>23.908970</td>\n",
       "      <td>0.331606</td>\n",
       "      <td>49.356381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>7/17/2007</td>\n",
       "      <td>19.211468</td>\n",
       "      <td>0.030241</td>\n",
       "      <td>32.941690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>8/6/2007</td>\n",
       "      <td>17.366877</td>\n",
       "      <td>0.022628</td>\n",
       "      <td>31.609187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>7/28/2008</td>\n",
       "      <td>20.585272</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>32.559895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>7/16/2009</td>\n",
       "      <td>21.342918</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>38.754120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>7/17/2009</td>\n",
       "      <td>21.451813</td>\n",
       "      <td>0.105873</td>\n",
       "      <td>39.511546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>7/18/2009</td>\n",
       "      <td>16.981620</td>\n",
       "      <td>0.104322</td>\n",
       "      <td>38.342482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>6/2/2011</td>\n",
       "      <td>12.654827</td>\n",
       "      <td>0.550524</td>\n",
       "      <td>34.993438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>7/17/2011</td>\n",
       "      <td>19.214113</td>\n",
       "      <td>0.057870</td>\n",
       "      <td>45.056388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>8/7/2011</td>\n",
       "      <td>15.895408</td>\n",
       "      <td>0.019959</td>\n",
       "      <td>31.262259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>9/3/2011</td>\n",
       "      <td>17.204716</td>\n",
       "      <td>0.025290</td>\n",
       "      <td>32.701293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>6/18/2012</td>\n",
       "      <td>16.490360</td>\n",
       "      <td>0.060908</td>\n",
       "      <td>47.977786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>6/19/2012</td>\n",
       "      <td>12.745975</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>36.064092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>8/7/2012</td>\n",
       "      <td>11.919377</td>\n",
       "      <td>0.026516</td>\n",
       "      <td>33.012284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>6/1/2013</td>\n",
       "      <td>11.123766</td>\n",
       "      <td>0.174906</td>\n",
       "      <td>30.608754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>7/4/2013</td>\n",
       "      <td>16.945307</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>34.218388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>8/2/2013</td>\n",
       "      <td>20.746782</td>\n",
       "      <td>0.006965</td>\n",
       "      <td>30.364948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>8/1/2014</td>\n",
       "      <td>20.457104</td>\n",
       "      <td>0.029008</td>\n",
       "      <td>35.467946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>6/29/2016</td>\n",
       "      <td>21.279696</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>30.296499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>8/15/2018</td>\n",
       "      <td>21.421936</td>\n",
       "      <td>0.086819</td>\n",
       "      <td>41.892355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>8/16/2018</td>\n",
       "      <td>22.673685</td>\n",
       "      <td>0.060061</td>\n",
       "      <td>47.362358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>8/6/2019</td>\n",
       "      <td>18.526226</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>30.351481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>8/7/2019</td>\n",
       "      <td>21.010150</td>\n",
       "      <td>0.027716</td>\n",
       "      <td>44.681781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>8/8/2019</td>\n",
       "      <td>22.589184</td>\n",
       "      <td>0.024273</td>\n",
       "      <td>47.165503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>21.575483</td>\n",
       "      <td>0.042829</td>\n",
       "      <td>80.636074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>8/10/2019</td>\n",
       "      <td>20.276800</td>\n",
       "      <td>0.043114</td>\n",
       "      <td>58.431061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>8/11/2019</td>\n",
       "      <td>17.905354</td>\n",
       "      <td>0.011177</td>\n",
       "      <td>30.734900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>8/5/2020</td>\n",
       "      <td>27.123528</td>\n",
       "      <td>0.037294</td>\n",
       "      <td>32.042267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>8/7/2020</td>\n",
       "      <td>19.529327</td>\n",
       "      <td>0.108583</td>\n",
       "      <td>42.017340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>8/8/2020</td>\n",
       "      <td>17.372940</td>\n",
       "      <td>0.220405</td>\n",
       "      <td>35.905335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>9/20/2020</td>\n",
       "      <td>18.062815</td>\n",
       "      <td>0.326716</td>\n",
       "      <td>34.073259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall\n",
       "41    7/12/2000  23.003010  0.016358  33.846075\n",
       "552    8/4/2004  21.769238  0.043448  44.404364\n",
       "876   6/23/2007  23.908970  0.331606  49.356381\n",
       "900   7/17/2007  19.211468  0.030241  32.941690\n",
       "920    8/6/2007  17.366877  0.022628  31.609187\n",
       "1033  7/28/2008  20.585272  0.014900  32.559895\n",
       "1143  7/16/2009  21.342918  0.037500  38.754120\n",
       "1144  7/17/2009  21.451813  0.105873  39.511546\n",
       "1145  7/18/2009  16.981620  0.104322  38.342482\n",
       "1343   6/2/2011  12.654827  0.550524  34.993438\n",
       "1388  7/17/2011  19.214113  0.057870  45.056388\n",
       "1409   8/7/2011  15.895408  0.019959  31.262259\n",
       "1436   9/3/2011  17.204716  0.025290  32.701293\n",
       "1481  6/18/2012  16.490360  0.060908  47.977786\n",
       "1482  6/19/2012  12.745975  0.004308  36.064092\n",
       "1531   8/7/2012  11.919377  0.026516  33.012284\n",
       "1586   6/1/2013  11.123766  0.174906  30.608754\n",
       "1619   7/4/2013  16.945307  0.008078  34.218388\n",
       "1648   8/2/2013  20.746782  0.006965  30.364948\n",
       "1769   8/1/2014  20.457104  0.029008  35.467946\n",
       "1980  6/29/2016  21.279696  0.040968  30.296499\n",
       "2271  8/15/2018  21.421936  0.086819  41.892355\n",
       "2272  8/16/2018  22.673685  0.060061  47.362358\n",
       "2384   8/6/2019  18.526226  0.027500  30.351481\n",
       "2385   8/7/2019  21.010150  0.027716  44.681781\n",
       "2386   8/8/2019  22.589184  0.024273  47.165503\n",
       "2387   8/9/2019  21.575483  0.042829  80.636074\n",
       "2388  8/10/2019  20.276800  0.043114  58.431061\n",
       "2389  8/11/2019  17.905354  0.011177  30.734900\n",
       "2505   8/5/2020  27.123528  0.037294  32.042267\n",
       "2507   8/7/2020  19.529327  0.108583  42.017340\n",
       "2508   8/8/2020  17.372940  0.220405  35.905335\n",
       "2551  9/20/2020  18.062815  0.326716  34.073259"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8de5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 30]\n",
    "extreme_data = data[data['rainfall'] > 30]\n",
    "\n",
    "# Define a function to calculate the error threshold\n",
    "def calculate_error_threshold(data):\n",
    "    normal_errors = np.abs(data[data['rainfall'] <= 30]['predicted_rainfall'] - data[data['rainfall'] <= 30]['rainfall'])\n",
    "    extreme_errors = np.abs(data[data['rainfall'] > 30]['predicted_rainfall'] - data[data['rainfall'] > 30]['rainfall'])\n",
    "    return np.mean(normal_errors) + 2 * np.std(normal_errors), np.mean(extreme_errors)\n",
    "\n",
    "# Define a function to calculate the accuracy\n",
    "def calculate_accuracy(data, threshold):\n",
    "    data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "    data['actual_rainfall_class'] = np.where(data['rainfall'] > 30, 'Extreme', 'Normal')\n",
    "    num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "    num_total = len(data)\n",
    "    return num_correct / num_total * 100\n",
    "\n",
    "def weighted_loss(normal_weight=0.001, extreme_weight=9):\n",
    "    def loss(y_true, y_pred):\n",
    "        normal_loss = K.mean(K.square(y_true - y_pred))\n",
    "        extreme_loss = K.mean(K.square(y_true - y_pred) * K.cast(y_true > 30, 'float32'))\n",
    "        return K.mean(normal_weight * normal_loss + extreme_weight * extreme_loss)\n",
    "    return loss\n",
    "\n",
    "# Define a function to find the best weights for normal and extreme data\n",
    "def find_best_weights(train_data, normal_weights, extreme_weights, num_epochs, batch_size):\n",
    "    best_weights = None\n",
    "    best_accuracy = 0\n",
    "    for normal_weight in normal_weights:\n",
    "        for extreme_weight in extreme_weights:\n",
    "            # Define the autoencoder architecture\n",
    "            input_layer = Input(shape=(3,))\n",
    "            encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "            decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "            # Create the autoencoder\n",
    "            autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "            # Compile the autoencoder with the weighted loss function\n",
    "            autoencoder.compile(optimizer='adam', loss=weighted_loss(normal_weight, extreme_weight))\n",
    "\n",
    "            # Train the model\n",
    "            history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Use the trained autoencoder to predict the rainfall values for all data points\n",
    "            test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "            predicted_data = autoencoder.predict(test_data)\n",
    "            data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "            # Calculate the error between the predicted and actual rainfall values\n",
    "            data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "            # Calculate the accuracy\n",
    "            threshold_normal, threshold_extreme = calculate_error_threshold(data)\n",
    "            threshold = (threshold_normal + threshold_extreme) / 2\n",
    "            accuracy = calculate_accuracy(data, threshold)\n",
    "\n",
    "            # Update the best weights and accuracy\n",
    "            if accuracy > best_accuracy:\n",
    "                best_normal_weight = normal_weight\n",
    "                best_extreme_weight = extreme_weight\n",
    "                best_accuracy = accuracy\n",
    "\n",
    "    print('Best normal_weight:', best_normal_weight)\n",
    "    print('Best extreme_weight:', best_extreme_weight)\n",
    "    print('Accuracy: {:.2f}%'.format(best_accuracy))\n",
    "def find_best_combo(data, normal_weight_range, extreme_weight_range, threshold):\n",
    "    best_error = float('inf')\n",
    "    best_combo = None\n",
    "    \n",
    "    for normal_weight in normal_weight_range:\n",
    "        for extreme_weight in extreme_weight_range:\n",
    "            combo_error = 0\n",
    "            for x, y in data:\n",
    "                if y == 0:\n",
    "                    error = abs(normal_weight * x - y)\n",
    "                else:\n",
    "                    error = abs(extreme_weight * x - y)\n",
    "                \n",
    "                if error > threshold:\n",
    "                    combo_error += error\n",
    "            if combo_error < best_error:\n",
    "                best_error = combo_error\n",
    "                best_combo = (normal_weight, extreme_weight)\n",
    "                \n",
    "    return best_combo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4b0c3a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regularization strength: 0.1\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 67.5105 - val_loss: 49.4476\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 26.8979 - val_loss: 20.3905\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 11.4378 - val_loss: 11.4250\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0640 - val_loss: 7.7172\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5517 - val_loss: 4.5271\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5208 - val_loss: 2.4367\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3455 - val_loss: 1.2989\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.7915 - val_loss: 0.8023\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.5661 - val_loss: 0.5928\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.4750 - val_loss: 0.5012\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.4322 - val_loss: 0.4507\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4072 - val_loss: 0.4195\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3905 - val_loss: 0.4004\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3787 - val_loss: 0.3874\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3695 - val_loss: 0.3786\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3625 - val_loss: 0.3716\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3564 - val_loss: 0.3649\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3511 - val_loss: 0.3588\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3465 - val_loss: 0.3536\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3421 - val_loss: 0.3487\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3380 - val_loss: 0.3440\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3342 - val_loss: 0.3395\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3305 - val_loss: 0.3355\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3270 - val_loss: 0.3313\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3234 - val_loss: 0.3275\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3200 - val_loss: 0.3235\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3166 - val_loss: 0.3197\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3131 - val_loss: 0.3159\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3097 - val_loss: 0.3122\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3062 - val_loss: 0.3087\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3028 - val_loss: 0.3050\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2994 - val_loss: 0.3013\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2959 - val_loss: 0.2975\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2925 - val_loss: 0.2939\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2890 - val_loss: 0.2903\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2855 - val_loss: 0.2866\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2820 - val_loss: 0.2830\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2785 - val_loss: 0.2795\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2750 - val_loss: 0.2759\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2715 - val_loss: 0.2721\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2680 - val_loss: 0.2686\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2645 - val_loss: 0.2651\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2609 - val_loss: 0.2613\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2574 - val_loss: 0.2578\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2538 - val_loss: 0.2541\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2503 - val_loss: 0.2505\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2467 - val_loss: 0.2469\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2431 - val_loss: 0.2434\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2396 - val_loss: 0.2397\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2360 - val_loss: 0.2361\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2325 - val_loss: 0.2326\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2289 - val_loss: 0.2290\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2254 - val_loss: 0.2255\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2218 - val_loss: 0.2217\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2175 - val_loss: 0.2162\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2128 - val_loss: 0.2112\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2083 - val_loss: 0.2069\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2041 - val_loss: 0.2026\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1998 - val_loss: 0.1983\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1957 - val_loss: 0.1942\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1915 - val_loss: 0.1901\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1873 - val_loss: 0.1859\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1831 - val_loss: 0.1817\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1789 - val_loss: 0.1775\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1748 - val_loss: 0.1734\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1707 - val_loss: 0.1693\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1665 - val_loss: 0.1652\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1624 - val_loss: 0.1611\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1583 - val_loss: 0.1570\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1542 - val_loss: 0.1528\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1502 - val_loss: 0.1489\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1462 - val_loss: 0.1450\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1422 - val_loss: 0.1410\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1382 - val_loss: 0.1370\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1345 - val_loss: 0.1335\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1305 - val_loss: 0.1295\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1267 - val_loss: 0.1260\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1230 - val_loss: 0.1221\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1194 - val_loss: 0.1184\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1158 - val_loss: 0.1153\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1122 - val_loss: 0.1113\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1086 - val_loss: 0.1079\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1052 - val_loss: 0.1046\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1018 - val_loss: 0.1010\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0986 - val_loss: 0.0978\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0952 - val_loss: 0.0946\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0920 - val_loss: 0.0913\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0889 - val_loss: 0.0884\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0859 - val_loss: 0.0854\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0828 - val_loss: 0.0823\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0799 - val_loss: 0.0796\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0771 - val_loss: 0.0766\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0743 - val_loss: 0.0744\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0716 - val_loss: 0.0713\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0689 - val_loss: 0.0687\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0664 - val_loss: 0.0664\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.0637\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0615 - val_loss: 0.0615\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0592 - val_loss: 0.0589\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0569 - val_loss: 0.0572\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "Accuracy: 99.92%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 50]\n",
    "extreme_data = data[data['rainfall'] > 50]\n",
    "\n",
    "def weighted_loss(normal_weight=1, extreme_weight=0.001):\n",
    "    def loss(y_true, y_pred):\n",
    "        normal_loss = K.mean(K.square(y_true - y_pred))\n",
    "        extreme_loss = K.mean(K.square(y_true - y_pred) * K.cast(y_true > 50, 'float32'))\n",
    "        return K.mean(normal_weight * normal_loss + extreme_weight * extreme_loss)\n",
    "    return loss\n",
    "\n",
    "# Define a function to find the best regularization strength\n",
    "def find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size):\n",
    "    best_reg_strength = None\n",
    "    best_loss = float('inf')\n",
    "    for reg_strength in reg_strengths:\n",
    "        # Define the autoencoder architecture\n",
    "        input_layer = Input(shape=(3,))\n",
    "        encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=reg_strength))(input_layer)\n",
    "        decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "        # Create the autoencoder\n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "        # Compile the autoencoder with the weighted loss function\n",
    "        autoencoder.compile(optimizer='adam', loss=weighted_loss(normal_weight, extreme_weight))\n",
    "\n",
    "        # Train the model\n",
    "        history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Calculate the validation loss\n",
    "        val_loss = np.mean(history.history['val_loss'])\n",
    "\n",
    "        # Update the best regularization strength and loss\n",
    "        if val_loss < best_loss:\n",
    "            best_reg_strength = reg_strength\n",
    "            best_loss = val_loss\n",
    "\n",
    "    print('Best regularization strength:', best_reg_strength)\n",
    "    return best_reg_strength\n",
    "\n",
    "# Find the best regularization strength\n",
    "reg_strengths = [0.01, 0.1, 1, 10]\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "best_reg_strength = find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size)\n",
    "\n",
    "# Define the autoencoder architecture with the best regularization strength\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "# Create the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder with the weighted loss function\n",
    "autoencoder.compile(optimizer='adam', loss=weighted_loss())\n",
    "\n",
    "# Train the model with the majority class\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 9.6\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "data['actual_rainfall_class'] = np.where(data['rainfall'] > 50, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "3987dc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall  predicted_rainfall     error  \\\n",
      "2387   8/9/2019  21.575483  0.042829  80.636074           80.632782  0.003292   \n",
      "2388  8/10/2019  20.276800  0.043114  58.431061           58.430782  0.000279   \n",
      "\n",
      "     rainfall_class actual_rainfall_class  \n",
      "2387         Normal               Extreme  \n",
      "2388         Normal               Extreme  \n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = data[data['rainfall_class'] != data['actual_rainfall_class']]\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "5ba442ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>21.575483</td>\n",
       "      <td>0.042829</td>\n",
       "      <td>80.636074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>8/10/2019</td>\n",
       "      <td>20.276800</td>\n",
       "      <td>0.043114</td>\n",
       "      <td>58.431061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall\n",
       "2387   8/9/2019  21.575483  0.042829  80.636074\n",
       "2388  8/10/2019  20.276800  0.043114  58.431061"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 9\n",
    "best_combination = None\n",
    "best_error = float('inf')\n",
    "\n",
    " # Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 50]\n",
    "extreme_data = data[data['rainfall'] > 50]\n",
    "\n",
    "for normal_weight in range(1, 11):\n",
    "    for extreme_weight in range(1, 11):\n",
    "        # train your model with the current combination of weights\n",
    "       \n",
    "\n",
    "        def weighted_loss(normal_weight, extreme_weight):\n",
    "            def loss(y_true, y_pred):\n",
    "                normal_loss = K.mean(K.square(y_true - y_pred))\n",
    "                extreme_loss = K.mean(K.square(y_true - y_pred) * K.cast(y_true > 50, 'float32'))\n",
    "                return K.mean(normal_weight * normal_loss + extreme_weight * extreme_loss)\n",
    "            return loss\n",
    "\n",
    "        # Define a function to find the best regularization strength\n",
    "        def find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size):\n",
    "            best_reg_strength = None\n",
    "            best_loss = float('inf')\n",
    "            for reg_strength in reg_strengths:\n",
    "                # Define the autoencoder architecture\n",
    "                input_layer = Input(shape=(3,))\n",
    "                encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=reg_strength))(input_layer)\n",
    "                decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "                # Create the autoencoder\n",
    "                autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "                # Compile the autoencoder with the weighted loss function\n",
    "                autoencoder.compile(optimizer='adam', loss=weighted_loss(normal_weight, extreme_weight))\n",
    "\n",
    "                # Train the model\n",
    "                history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "                # Calculate the validation loss\n",
    "                val_loss = np.mean(history.history['val_loss'])\n",
    "\n",
    "                # Update the best regularization strength and loss\n",
    "                if val_loss < best_loss:\n",
    "                    best_reg_strength = reg_strength\n",
    "                    best_loss = val_loss\n",
    "\n",
    "            print('Best regularization strength:', best_reg_strength)\n",
    "            return best_reg_strength\n",
    "\n",
    "        # Find the best regularization strength\n",
    "        reg_strengths = [0.01, 0.1, 1, 10]\n",
    "        num_epochs = 100\n",
    "        batch_size = 32\n",
    "        train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "        best_reg_strength = find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size)\n",
    "\n",
    "        # Define the autoencoder architecture with the best regularization strength\n",
    "        input_layer = Input(shape=(3,))\n",
    "        encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "        decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "        # Create the autoencoder\n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "        # Compile the autoencoder with the weighted loss function\n",
    "        autoencoder.compile(optimizer='adam', loss=weighted_loss())\n",
    "\n",
    "        # and get the error for normal and extreme cases\n",
    "        \n",
    "        \n",
    "        normal_error = get_normal_error(normal_weight, extreme_weight)\n",
    "        extreme_error = get_extreme_error(normal_weight, extreme_weight)\n",
    "\n",
    "        # check if the current combination is better than the previous best combination\n",
    "        if normal_error < threshold and extreme_error > threshold and extreme_error < best_error:\n",
    "            best_combination = (normal_weight, extreme_weight)\n",
    "            best_error = extreme_error\n",
    "\n",
    "print(f\"The best combination of weights is {best_combination} with an extreme error of {best_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(normal_weight, extreme_weight):\n",
    "    # Define the autoencoder architecture with the best regularization strength\n",
    "    input_layer = Input(shape=(3,))\n",
    "    encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "    decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "    # Create the autoencoder\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "    # Compile the autoencoder with the weighted loss function\n",
    "    autoencoder.compile(optimizer='adam', loss=weighted_loss(normal_weight, extreme_weight))\n",
    "\n",
    "    # Train the model with the majority class\n",
    "    train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "    history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "    # Use the trained autoencoder to predict the rainfall values for all data points\n",
    "    test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "    predicted_data = autoencoder.predict(test_data)\n",
    "    data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "    # Calculate the error between the predicted and actual rainfall values\n",
    "    data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "    # Classify the rainfall data into normal and extreme based on the error threshold\n",
    "    normal_error =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(normal_weight, extreme_weight):\n",
    "    def loss(y_true, y_pred):\n",
    "        normal_loss = K.mean(K.square(y_true - y_pred) * K.cast(y_true <= 50, 'float32'))\n",
    "        extreme_loss = K.mean(K.square(y_true - y_pred) * K.cast(y_true > 50, 'float32'))\n",
    "        return K.mean(normal_weight * normal_loss + extreme_weight * extreme_loss)\n",
    "\n",
    "    # Define the autoencoder architecture\n",
    "    input_layer = Input(shape=(3,))\n",
    "    encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "    decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "    # Create the autoencoder\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "    # Compile the autoencoder with the weighted loss function\n",
    "    autoencoder.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "    # Train the model\n",
    "    train_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "    history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "    # Use the trained autoencoder to predict the rainfall values for all data points\n",
    "    test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "    predicted_data = autoencoder.predict(test_data)\n",
    "\n",
    "    # Calculate the error between the predicted and actual rainfall values\n",
    "    error = np.abs(predicted_data[:, 2] - data['rainfall'].values)\n",
    "\n",
    "    # Classify the rainfall data into normal and extreme based on the error threshold\n",
    "    normal_error = np.mean(error[data['rainfall'] <= 50])\n",
    "    extreme_error = np.mean(error[data['rainfall'] > 50])\n",
    "    \n",
    "    if normal_error < threshold and extreme_error > threshold and extreme_error < best_error:\n",
    "          \n",
    "\n",
    "        print(f\"The best combination of weights is {best_combination} with an extreme error of {best_error}\")\n",
    "\n",
    "        return normal_error, extreme_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 9\n",
    "best_combination = None\n",
    "best_error = float('inf')\n",
    "normal_error = 0\n",
    "extreme_error = 0\n",
    "\n",
    "for normal_weight in range(1, 11):\n",
    "    for extreme_weight in range(1, 11):\n",
    "        # train your model with the current combination of weights\n",
    "        # and get the error for normal and extreme cases\n",
    "        normal_loss, extreme_loss = get_losses(normal_weight, extreme_weight)\n",
    "\n",
    "        normal_error = np.mean(normal_loss)\n",
    "        extreme_error = np.mean(extreme_loss)\n",
    "\n",
    "        # check if the current combination is better than the previous best combination\n",
    "        if normal_error < threshold and extreme_error > threshold and extreme_error < best_error:\n",
    "            best_combination = (normal_weight, extreme_weight)\n",
    "            best_error = extreme_error\n",
    "\n",
    "if best_combination is not None:\n",
    "    print(f\"The best combination of weights is {best_combination} with a normal error of {normal_error:.2f} and an extreme error of {extreme_error:.2f}\")\n",
    "else:\n",
    "    print(\"No combination of weights satisfies the given condition\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a98f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 50]\n",
    "extreme_data = data[data['rainfall'] > 50]\n",
    "\n",
    "def weighted_loss(normal_weight=10, extreme_weight=0.001):\n",
    "    def loss(y_true, y_pred):\n",
    "        normal_loss = K.mean(K.square(y_true - y_pred))\n",
    "        extreme_loss = K.mean(K.square(y_true - y_pred) * K.cast(y_true > 50, 'float32'))\n",
    "        return K.mean(normal_weight * normal_loss + extreme_weight * extreme_loss)\n",
    "    return loss\n",
    " \n",
    "\n",
    "# Define a function to find the best regularization strength\n",
    "def find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size):\n",
    "    best_reg_strength = None\n",
    "    best_loss = float('inf')\n",
    "    for reg_strength in reg_strengths:\n",
    "        # Define the autoencoder architecture\n",
    "        input_layer = Input(shape=(3,))\n",
    "        encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=reg_strength))(input_layer)\n",
    "        decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "        # Create the autoencoder\n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "        # Compile the autoencoder with the weighted loss function\n",
    "        autoencoder.compile(optimizer='adam', loss=weighted_loss(normal_weight, extreme_weight))\n",
    "\n",
    "        # Train the model\n",
    "        history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Calculate the validation loss\n",
    "        val_loss = np.mean(history.history['val_loss'])\n",
    "\n",
    "        # Update the best regularization strength and loss\n",
    "        if val_loss < best_loss:\n",
    "            best_reg_strength = reg_strength\n",
    "            best_loss = val_loss\n",
    "\n",
    "    print('Best regularization strength:', best_reg_strength)\n",
    "    return best_reg_strength\n",
    "\n",
    "# Find the best regularization strength\n",
    "reg_strengths = [0.01, 0.1, 1, 10]\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "best_reg_strength = find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size)\n",
    "\n",
    "# Define the autoencoder architecture with the best regularization strength\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "# Create the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder with the weighted loss function\n",
    "autoencoder.compile(optimizer='adam', loss=weighted_loss())\n",
    "\n",
    "# Train the model with the majority class\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 9.6\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "data['actual_rainfall_class'] = np.where(data['rainfall'] > 50, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e27770",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_predictions = data[data['rainfall_class'] != data['actual_rainfall_class']]\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "812f8c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>21.575483</td>\n",
       "      <td>0.042829</td>\n",
       "      <td>80.636074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>8/10/2019</td>\n",
       "      <td>20.276800</td>\n",
       "      <td>0.043114</td>\n",
       "      <td>58.431061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall\n",
       "2387   8/9/2019  21.575483  0.042829  80.636074\n",
       "2388  8/10/2019  20.276800  0.043114  58.431061"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "439fcaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regularization strength: 0.01\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 1s 4ms/step - loss: 0.6438 - val_loss: 0.4517\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2797 - val_loss: 0.2028\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1304 - val_loss: 0.1126\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0808 - val_loss: 0.0758\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0572 - val_loss: 0.0544\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0437 - val_loss: 0.0422\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0361 - val_loss: 0.0350\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0315 - val_loss: 0.0310\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0287 - val_loss: 0.0283\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0267 - val_loss: 0.0264\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0252 - val_loss: 0.0249\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0238 - val_loss: 0.0236\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0223\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0212\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0204 - val_loss: 0.0201\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0194 - val_loss: 0.0192\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0185 - val_loss: 0.0183\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0176 - val_loss: 0.0174\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0168 - val_loss: 0.0165\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0157\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 0.0149\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0143 - val_loss: 0.0141\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0136 - val_loss: 0.0134\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0128\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0121\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0115\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0095\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0092 - val_loss: 0.0091\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0065 - val_loss: 0.0064\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0052 - val_loss: 0.0051\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "81/81 [==============================] - 0s 575us/step\n",
      "Accuracy: 99.92%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "normal_data = data[data['rainfall'] <= 50]\n",
    "extreme_data = data[data['rainfall'] > 50]\n",
    "\n",
    "def weighted_loss(normal_weight=0.009000000000000001, extreme_weight=0.009000000000000001):\n",
    "    def loss(y_true, y_pred):\n",
    "        normal_loss = K.mean(K.square(y_true - y_pred))\n",
    "        extreme_loss = K.mean(K.square(y_true - y_pred) * K.cast(y_true > 50, 'float32'))\n",
    "        return K.mean(normal_weight * normal_loss + extreme_weight * extreme_loss)\n",
    "    return loss\n",
    " \n",
    "\n",
    "# Define a function to find the best regularization strength\n",
    "def find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size):\n",
    "    best_reg_strength = None\n",
    "    best_loss = float('inf')\n",
    "    for reg_strength in reg_strengths:\n",
    "        # Define the autoencoder architecture\n",
    "        input_layer = Input(shape=(3,))\n",
    "        encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=reg_strength))(input_layer)\n",
    "        decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "        # Create the autoencoder\n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "        # Compile the autoencoder with the weighted loss function\n",
    "        autoencoder.compile(optimizer='adam', loss=weighted_loss(normal_weight, extreme_weight))\n",
    "\n",
    "        # Train the model\n",
    "        history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Calculate the validation loss\n",
    "        val_loss = np.mean(history.history['val_loss'])\n",
    "\n",
    "        # Update the best regularization strength and loss\n",
    "        if val_loss < best_loss:\n",
    "            best_reg_strength = reg_strength\n",
    "            best_loss = val_loss\n",
    "\n",
    "    print('Best regularization strength:', best_reg_strength)\n",
    "    return best_reg_strength\n",
    "\n",
    "# Find the best regularization strength\n",
    "reg_strengths = [0.01, 0.1, 1, 10]\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "best_reg_strength = find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size)\n",
    "\n",
    "# Define the autoencoder architecture with the best regularization strength\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "# Create the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder with the weighted loss function\n",
    "autoencoder.compile(optimizer='adam', loss=weighted_loss())\n",
    "\n",
    "# Train the model with the majority class\n",
    "train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold = 9.6\n",
    "data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "data['actual_rainfall_class'] = np.where(data['rainfall'] > 50, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(data[data['rainfall_class'] == data['actual_rainfall_class']])\n",
    "num_total = len(data)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2dff93d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall  predicted_rainfall     error  \\\n",
      "2387   8/9/2019  21.575483  0.042829  80.636074           79.659050  0.977024   \n",
      "2388  8/10/2019  20.276800  0.043114  58.431061           57.826702  0.604359   \n",
      "\n",
      "     rainfall_class actual_rainfall_class  \n",
      "2387         Normal               Extreme  \n",
      "2388         Normal               Extreme  \n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = data[data['rainfall_class'] != data['actual_rainfall_class']]\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1696b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.7280 - val_loss: 0.5550\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3781 - val_loss: 0.2901\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1954 - val_loss: 0.1647\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1157 - val_loss: 0.1074\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0806 - val_loss: 0.0782\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0617 - val_loss: 0.0605\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0496 - val_loss: 0.0489\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0414 - val_loss: 0.0409\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 0.0355\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0320 - val_loss: 0.0317\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0289\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0269\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0252\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0238\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0225\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0214\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0203\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0194\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0184\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0176\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0160\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0153\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0146\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0134\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.0128\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.0122\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0089\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0058\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0052\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0047\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9069e-04 - val_loss: 0.0010\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7438e-04 - val_loss: 9.8245e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6262e-04 - val_loss: 9.7287e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5170e-04 - val_loss: 9.5646e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3810e-04 - val_loss: 9.4748e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2347e-04 - val_loss: 9.3184e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1044e-04 - val_loss: 9.2078e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9780e-04 - val_loss: 9.0777e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8519e-04 - val_loss: 8.9451e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7643e-04 - val_loss: 8.8264e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6268e-04 - val_loss: 8.7057e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5097e-04 - val_loss: 8.6035e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3912e-04 - val_loss: 8.4641e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3053e-04 - val_loss: 8.3986e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1863e-04 - val_loss: 8.2688e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0780e-04 - val_loss: 8.1440e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9765e-04 - val_loss: 8.0375e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8747e-04 - val_loss: 7.9417e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7664e-04 - val_loss: 7.8267e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6813e-04 - val_loss: 7.7307e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5771e-04 - val_loss: 7.6412e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5073e-04 - val_loss: 7.6062e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3931e-04 - val_loss: 7.4555e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "2\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.2242 - val_loss: 0.1489\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0942 - val_loss: 0.0825\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.0522\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0408 - val_loss: 0.0378\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0306\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0267\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0241\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0221\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0204\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0176\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0164\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0153\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0143\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0133\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0125\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0116\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0090\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0064\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8746e-04 - val_loss: 9.8799e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7272e-04 - val_loss: 9.7325e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5863e-04 - val_loss: 9.6479e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4503e-04 - val_loss: 9.5093e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3552e-04 - val_loss: 9.3319e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1831e-04 - val_loss: 9.2282e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0561e-04 - val_loss: 9.1635e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9296e-04 - val_loss: 8.9640e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8121e-04 - val_loss: 8.8492e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6920e-04 - val_loss: 8.7378e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5591e-04 - val_loss: 8.6100e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4718e-04 - val_loss: 8.5290e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3483e-04 - val_loss: 8.3951e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2308e-04 - val_loss: 8.2747e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1201e-04 - val_loss: 8.1843e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0180e-04 - val_loss: 8.0193e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9096e-04 - val_loss: 7.9133e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8282e-04 - val_loss: 7.8833e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7008e-04 - val_loss: 7.7578e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6084e-04 - val_loss: 7.6128e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5123e-04 - val_loss: 7.5566e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4063e-04 - val_loss: 7.4289e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3079e-04 - val_loss: 7.3402e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2388e-04 - val_loss: 7.2683e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1439e-04 - val_loss: 7.1789e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0655e-04 - val_loss: 7.0937e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9712e-04 - val_loss: 7.0191e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8824e-04 - val_loss: 6.8917e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7961e-04 - val_loss: 6.7922e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7094e-04 - val_loss: 6.7324e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6198e-04 - val_loss: 6.6851e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5538e-04 - val_loss: 6.6601e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4768e-04 - val_loss: 6.5559e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3908e-04 - val_loss: 6.4097e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3191e-04 - val_loss: 6.3646e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2372e-04 - val_loss: 6.2732e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1678e-04 - val_loss: 6.2176e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1097e-04 - val_loss: 6.1650e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0187e-04 - val_loss: 6.0374e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.9649e-04 - val_loss: 6.0047e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8880e-04 - val_loss: 5.9049e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8466e-04 - val_loss: 5.9139e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7736e-04 - val_loss: 5.8195e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6950e-04 - val_loss: 5.7047e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6199e-04 - val_loss: 5.6462e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5647e-04 - val_loss: 5.5744e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5151e-04 - val_loss: 5.5123e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4429e-04 - val_loss: 5.4494e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3834e-04 - val_loss: 5.4282e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3254e-04 - val_loss: 5.3342e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2787e-04 - val_loss: 5.4616e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "3\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 0.4363 - val_loss: 0.2588\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1484 - val_loss: 0.0988\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0684 - val_loss: 0.0621\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0490 - val_loss: 0.0467\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0398 - val_loss: 0.0382\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0332\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0300\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0275\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0255\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0238\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0223\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0209\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0196\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0184\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0177 - val_loss: 0.0173\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0163\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0154\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0145\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0137\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0122\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0115\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0097\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0092\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 101/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9669e-04 - val_loss: 0.0010\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8459e-04 - val_loss: 0.0011\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7962e-04 - val_loss: 9.9538e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5678e-04 - val_loss: 0.0010\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5027e-04 - val_loss: 9.7415e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3782e-04 - val_loss: 9.7302e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2841e-04 - val_loss: 9.4050e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1229e-04 - val_loss: 9.4326e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0031e-04 - val_loss: 9.1354e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9745e-04 - val_loss: 9.2772e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8627e-04 - val_loss: 9.0779e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7139e-04 - val_loss: 8.8346e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5983e-04 - val_loss: 9.0524e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5557e-04 - val_loss: 8.7811e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3886e-04 - val_loss: 8.6005e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2954e-04 - val_loss: 8.5435e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2101e-04 - val_loss: 8.5321e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1140e-04 - val_loss: 8.1862e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0268e-04 - val_loss: 8.1509e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9237e-04 - val_loss: 8.0845e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9253e-04 - val_loss: 7.9288e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7297e-04 - val_loss: 7.9751e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6973e-04 - val_loss: 7.8693e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5989e-04 - val_loss: 7.6766e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5001e-04 - val_loss: 7.8278e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4211e-04 - val_loss: 7.5227e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3279e-04 - val_loss: 7.4861e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2680e-04 - val_loss: 7.3630e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1864e-04 - val_loss: 7.3052e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1179e-04 - val_loss: 7.2067e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0501e-04 - val_loss: 7.1483e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9688e-04 - val_loss: 6.9741e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8790e-04 - val_loss: 7.1452e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9061e-04 - val_loss: 7.0975e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8445e-04 - val_loss: 7.6129e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8095e-04 - val_loss: 6.7799e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "4\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.4596 - val_loss: 0.4248\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3291 - val_loss: 0.2802\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1867 - val_loss: 0.1285\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0778 - val_loss: 0.0558\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0411 - val_loss: 0.0390\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.0323\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0272\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0233\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0189\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0164\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0151\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0142\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0135\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0124\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0097\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 0.0093\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0090\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0087\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.0075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0052\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0049\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8167e-04 - val_loss: 9.8796e-04\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6633e-04 - val_loss: 9.6931e-04\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4750e-04 - val_loss: 9.5585e-04\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3143e-04 - val_loss: 9.3901e-04\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1590e-04 - val_loss: 9.2513e-04\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9988e-04 - val_loss: 9.0854e-04\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8511e-04 - val_loss: 8.9577e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7224e-04 - val_loss: 8.7773e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5963e-04 - val_loss: 8.6639e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4458e-04 - val_loss: 8.5288e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3251e-04 - val_loss: 8.4532e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2009e-04 - val_loss: 8.2537e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0786e-04 - val_loss: 8.1448e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9584e-04 - val_loss: 8.0351e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8424e-04 - val_loss: 7.9028e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7433e-04 - val_loss: 7.8301e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6386e-04 - val_loss: 7.6979e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5329e-04 - val_loss: 7.6290e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4329e-04 - val_loss: 7.4819e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3221e-04 - val_loss: 7.4058e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2366e-04 - val_loss: 7.2983e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1350e-04 - val_loss: 7.2017e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0552e-04 - val_loss: 7.1155e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9600e-04 - val_loss: 7.0235e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8651e-04 - val_loss: 6.9403e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7823e-04 - val_loss: 6.8508e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7074e-04 - val_loss: 6.7677e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6294e-04 - val_loss: 6.7042e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5340e-04 - val_loss: 6.5964e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4579e-04 - val_loss: 6.5169e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3814e-04 - val_loss: 6.4681e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3199e-04 - val_loss: 6.3889e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2444e-04 - val_loss: 6.2926e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1659e-04 - val_loss: 6.2168e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1024e-04 - val_loss: 6.1731e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0323e-04 - val_loss: 6.0918e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9584e-04 - val_loss: 6.0229e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8957e-04 - val_loss: 5.9788e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8298e-04 - val_loss: 5.8817e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7737e-04 - val_loss: 5.8303e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7022e-04 - val_loss: 5.7977e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6450e-04 - val_loss: 5.7028e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5879e-04 - val_loss: 5.6545e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5159e-04 - val_loss: 5.5898e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4620e-04 - val_loss: 5.5405e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4162e-04 - val_loss: 5.5098e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3585e-04 - val_loss: 5.4250e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3044e-04 - val_loss: 5.3711e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2405e-04 - val_loss: 5.3099e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1961e-04 - val_loss: 5.2536e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1520e-04 - val_loss: 5.2127e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1050e-04 - val_loss: 5.1885e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0598e-04 - val_loss: 5.1169e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0015e-04 - val_loss: 5.0517e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9487e-04 - val_loss: 5.0633e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9113e-04 - val_loss: 4.9920e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8554e-04 - val_loss: 4.9317e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8095e-04 - val_loss: 4.8798e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7795e-04 - val_loss: 4.8430e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7545e-04 - val_loss: 4.7786e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6821e-04 - val_loss: 4.7530e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6398e-04 - val_loss: 4.6966e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5967e-04 - val_loss: 4.6518e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5659e-04 - val_loss: 4.6187e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5216e-04 - val_loss: 4.5893e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4808e-04 - val_loss: 4.5396e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4420e-04 - val_loss: 4.5048e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4025e-04 - val_loss: 4.4770e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3689e-04 - val_loss: 4.4379e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "5\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.1516 - val_loss: 0.0978\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0655 - val_loss: 0.0630\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0465 - val_loss: 0.0457\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 0.0349\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0285\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0242\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0212\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0189\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0170\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0154\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0141\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0120\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0097\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0091\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0085\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0064\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 35/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8375e-04 - val_loss: 9.8761e-04\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6734e-04 - val_loss: 9.7287e-04\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5379e-04 - val_loss: 9.5606e-04\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3641e-04 - val_loss: 9.4309e-04\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2080e-04 - val_loss: 9.2858e-04\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0698e-04 - val_loss: 9.1173e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9298e-04 - val_loss: 8.9534e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7937e-04 - val_loss: 8.9277e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7064e-04 - val_loss: 8.8407e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5439e-04 - val_loss: 8.5955e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4167e-04 - val_loss: 8.4847e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3061e-04 - val_loss: 8.5503e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2117e-04 - val_loss: 8.2145e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0784e-04 - val_loss: 8.0925e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9453e-04 - val_loss: 8.0726e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8882e-04 - val_loss: 8.0900e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7635e-04 - val_loss: 7.7755e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6233e-04 - val_loss: 7.6778e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5488e-04 - val_loss: 7.5780e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4236e-04 - val_loss: 7.4856e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3755e-04 - val_loss: 7.3543e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2492e-04 - val_loss: 7.2708e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1517e-04 - val_loss: 7.1847e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0508e-04 - val_loss: 7.1244e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9768e-04 - val_loss: 6.9890e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8845e-04 - val_loss: 6.9590e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8075e-04 - val_loss: 6.8246e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7107e-04 - val_loss: 6.7365e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6477e-04 - val_loss: 6.6666e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5687e-04 - val_loss: 6.6236e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5192e-04 - val_loss: 6.5126e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4140e-04 - val_loss: 6.4352e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3306e-04 - val_loss: 6.3967e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2677e-04 - val_loss: 6.2864e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1837e-04 - val_loss: 6.2230e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1140e-04 - val_loss: 6.1464e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0478e-04 - val_loss: 6.0648e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9808e-04 - val_loss: 6.0283e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9054e-04 - val_loss: 5.9316e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8564e-04 - val_loss: 5.8659e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7707e-04 - val_loss: 5.7950e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7146e-04 - val_loss: 5.7322e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6572e-04 - val_loss: 5.6868e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5913e-04 - val_loss: 5.5985e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5300e-04 - val_loss: 5.5573e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4560e-04 - val_loss: 5.4815e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4090e-04 - val_loss: 5.4346e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3564e-04 - val_loss: 5.3630e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3120e-04 - val_loss: 5.5116e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2705e-04 - val_loss: 5.2532e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1747e-04 - val_loss: 5.1974e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1572e-04 - val_loss: 5.2687e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0930e-04 - val_loss: 5.1621e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0861e-04 - val_loss: 5.2828e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0005e-04 - val_loss: 5.0008e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9272e-04 - val_loss: 4.9397e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8825e-04 - val_loss: 4.9025e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8333e-04 - val_loss: 4.8389e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7731e-04 - val_loss: 4.8203e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7284e-04 - val_loss: 4.7748e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6965e-04 - val_loss: 4.7125e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6632e-04 - val_loss: 4.7440e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6024e-04 - val_loss: 4.6303e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5622e-04 - val_loss: 4.5855e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5349e-04 - val_loss: 4.6598e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4984e-04 - val_loss: 4.5150e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4374e-04 - val_loss: 4.4632e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4041e-04 - val_loss: 4.4142e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "6\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.7546 - val_loss: 0.5263\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3082 - val_loss: 0.1735\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0991 - val_loss: 0.0705\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0540 - val_loss: 0.0507\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.0396\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.0327\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0284\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0256\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0235\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0219\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0205\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0193\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0182\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0171\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0162\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0154\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0146\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0138\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0125\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0119\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0096\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0092\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8506e-04 - val_loss: 9.9584e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6976e-04 - val_loss: 9.8209e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5631e-04 - val_loss: 9.6085e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3964e-04 - val_loss: 9.4514e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2449e-04 - val_loss: 9.3104e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1387e-04 - val_loss: 9.1778e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9757e-04 - val_loss: 9.0282e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8518e-04 - val_loss: 8.9496e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7336e-04 - val_loss: 8.7718e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5762e-04 - val_loss: 8.6373e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4709e-04 - val_loss: 8.5172e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3358e-04 - val_loss: 8.3984e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2226e-04 - val_loss: 8.2684e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0981e-04 - val_loss: 8.1966e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9934e-04 - val_loss: 8.0568e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8754e-04 - val_loss: 8.0033e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7727e-04 - val_loss: 7.8755e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6722e-04 - val_loss: 7.7315e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5528e-04 - val_loss: 7.6024e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4499e-04 - val_loss: 7.4888e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3550e-04 - val_loss: 7.3873e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2493e-04 - val_loss: 7.3117e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1612e-04 - val_loss: 7.2278e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0520e-04 - val_loss: 7.0957e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9592e-04 - val_loss: 7.0148e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8737e-04 - val_loss: 6.9081e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7738e-04 - val_loss: 6.8385e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6942e-04 - val_loss: 6.7373e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5984e-04 - val_loss: 6.6357e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5368e-04 - val_loss: 6.5698e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4368e-04 - val_loss: 6.5059e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3573e-04 - val_loss: 6.4198e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2784e-04 - val_loss: 6.3177e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1963e-04 - val_loss: 6.2506e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1251e-04 - val_loss: 6.1787e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0348e-04 - val_loss: 6.0883e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9774e-04 - val_loss: 6.0142e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8901e-04 - val_loss: 5.9439e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8407e-04 - val_loss: 5.8918e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7510e-04 - val_loss: 5.8020e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6920e-04 - val_loss: 5.7303e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6216e-04 - val_loss: 5.6769e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5637e-04 - val_loss: 5.6087e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4853e-04 - val_loss: 5.5232e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4283e-04 - val_loss: 5.4869e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "7\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.5414 - val_loss: 0.4746\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3526 - val_loss: 0.2810\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1806 - val_loss: 0.1273\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0860 - val_loss: 0.0737\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0555 - val_loss: 0.0532\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.0405\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.0314\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0261\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0230\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0210\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0195\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0183\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0172\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0162\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0154\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0146\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0138\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0131\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.0125\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0118\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0113\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0098\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 0.0093\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 50/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9769e-04 - val_loss: 0.0010\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8007e-04 - val_loss: 0.0010\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6507e-04 - val_loss: 0.0010\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4337e-04 - val_loss: 9.8511e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2702e-04 - val_loss: 9.6608e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1151e-04 - val_loss: 9.4866e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9754e-04 - val_loss: 9.3036e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8271e-04 - val_loss: 9.1866e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6614e-04 - val_loss: 9.0971e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5194e-04 - val_loss: 8.8650e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3728e-04 - val_loss: 8.7427e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2544e-04 - val_loss: 8.6750e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1263e-04 - val_loss: 8.4805e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9821e-04 - val_loss: 8.3259e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8603e-04 - val_loss: 8.1615e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7412e-04 - val_loss: 8.1842e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6427e-04 - val_loss: 7.9383e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5030e-04 - val_loss: 7.8417e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4220e-04 - val_loss: 7.8150e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3184e-04 - val_loss: 7.5708e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2020e-04 - val_loss: 7.4915e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0952e-04 - val_loss: 7.3369e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0251e-04 - val_loss: 7.2968e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8991e-04 - val_loss: 7.2150e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8012e-04 - val_loss: 7.0573e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7191e-04 - val_loss: 6.9765e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6181e-04 - val_loss: 6.8766e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5368e-04 - val_loss: 6.7915e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4914e-04 - val_loss: 6.7030e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3921e-04 - val_loss: 6.7495e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2859e-04 - val_loss: 6.6324e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2355e-04 - val_loss: 6.5103e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1491e-04 - val_loss: 6.4000e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0654e-04 - val_loss: 6.3676e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0009e-04 - val_loss: 6.2507e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9185e-04 - val_loss: 6.1703e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8433e-04 - val_loss: 6.1067e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7710e-04 - val_loss: 6.0587e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7055e-04 - val_loss: 5.9353e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6388e-04 - val_loss: 5.8478e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5813e-04 - val_loss: 5.8579e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5463e-04 - val_loss: 5.7715e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4428e-04 - val_loss: 5.7259e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4240e-04 - val_loss: 5.6778e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3496e-04 - val_loss: 5.5196e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2830e-04 - val_loss: 5.5103e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2262e-04 - val_loss: 5.5000e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1761e-04 - val_loss: 5.4125e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1233e-04 - val_loss: 5.2668e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0508e-04 - val_loss: 5.2910e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9965e-04 - val_loss: 5.1763e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9526e-04 - val_loss: 5.1855e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "8\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 7ms/step - loss: 0.3104 - val_loss: 0.2145\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1253 - val_loss: 0.0817\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0554 - val_loss: 0.0500\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 0.0353\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0270\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0223\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0194\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0176\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0163\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0152\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0143\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0135\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0128\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.0121\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0115\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0095\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0091\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 58/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8441e-04 - val_loss: 9.8626e-04\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6474e-04 - val_loss: 9.6949e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4569e-04 - val_loss: 9.6330e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3010e-04 - val_loss: 9.4565e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1133e-04 - val_loss: 9.1452e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9346e-04 - val_loss: 8.9947e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8042e-04 - val_loss: 8.9077e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6385e-04 - val_loss: 8.7064e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4809e-04 - val_loss: 8.5563e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3384e-04 - val_loss: 8.4275e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1858e-04 - val_loss: 8.2553e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0742e-04 - val_loss: 8.1645e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9416e-04 - val_loss: 8.0506e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8043e-04 - val_loss: 7.9509e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7198e-04 - val_loss: 7.8757e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6431e-04 - val_loss: 7.7085e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4728e-04 - val_loss: 7.6256e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3637e-04 - val_loss: 7.5088e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2618e-04 - val_loss: 7.5319e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1904e-04 - val_loss: 7.3777e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0684e-04 - val_loss: 7.2900e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9807e-04 - val_loss: 7.1268e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8790e-04 - val_loss: 7.0165e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7995e-04 - val_loss: 6.9080e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7189e-04 - val_loss: 6.7995e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6206e-04 - val_loss: 6.7751e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5244e-04 - val_loss: 6.8834e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4631e-04 - val_loss: 6.6713e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3713e-04 - val_loss: 6.6031e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3001e-04 - val_loss: 6.5545e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2226e-04 - val_loss: 6.3639e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1803e-04 - val_loss: 6.3221e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0765e-04 - val_loss: 6.3258e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0183e-04 - val_loss: 6.0931e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9523e-04 - val_loss: 6.1268e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9047e-04 - val_loss: 6.0151e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8056e-04 - val_loss: 5.9556e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7611e-04 - val_loss: 5.9323e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6731e-04 - val_loss: 5.8268e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6387e-04 - val_loss: 5.8802e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5632e-04 - val_loss: 5.9180e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5127e-04 - val_loss: 5.6314e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4585e-04 - val_loss: 5.5382e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3981e-04 - val_loss: 5.5403e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3510e-04 - val_loss: 5.5046e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2917e-04 - val_loss: 5.4566e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2358e-04 - val_loss: 5.3269e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1900e-04 - val_loss: 5.3199e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1652e-04 - val_loss: 5.2442e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1080e-04 - val_loss: 5.3247e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0463e-04 - val_loss: 5.2060e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9807e-04 - val_loss: 5.0780e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9279e-04 - val_loss: 5.1105e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8797e-04 - val_loss: 5.0445e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8546e-04 - val_loss: 4.9543e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8314e-04 - val_loss: 4.9229e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7723e-04 - val_loss: 4.8865e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7303e-04 - val_loss: 4.8146e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7020e-04 - val_loss: 4.7904e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6347e-04 - val_loss: 4.8147e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6159e-04 - val_loss: 4.7083e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5440e-04 - val_loss: 4.7353e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5004e-04 - val_loss: 4.5877e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4601e-04 - val_loss: 4.6164e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "9\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.3673 - val_loss: 0.2885\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2232 - val_loss: 0.1702\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1299 - val_loss: 0.1034\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0807 - val_loss: 0.0697\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0559 - val_loss: 0.0502\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0421 - val_loss: 0.0390\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.0325\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0283\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0253\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0230\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0210\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0192\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0177\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0163\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0150\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0139\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0119\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0090\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8762e-04 - val_loss: 0.0010\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6811e-04 - val_loss: 9.9168e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4912e-04 - val_loss: 9.7350e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3320e-04 - val_loss: 9.5864e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1387e-04 - val_loss: 9.3277e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9608e-04 - val_loss: 9.1603e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8018e-04 - val_loss: 8.9957e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6344e-04 - val_loss: 8.8534e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4889e-04 - val_loss: 8.6779e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3244e-04 - val_loss: 8.5347e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2255e-04 - val_loss: 8.3600e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0590e-04 - val_loss: 8.2273e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9075e-04 - val_loss: 8.1033e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7902e-04 - val_loss: 8.0205e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6490e-04 - val_loss: 7.8328e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5176e-04 - val_loss: 7.7071e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4123e-04 - val_loss: 7.5637e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2705e-04 - val_loss: 7.4532e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1708e-04 - val_loss: 7.3364e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0528e-04 - val_loss: 7.2059e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9368e-04 - val_loss: 7.1959e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8486e-04 - val_loss: 7.0250e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7414e-04 - val_loss: 6.9246e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6747e-04 - val_loss: 6.8386e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5332e-04 - val_loss: 6.7023e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4437e-04 - val_loss: 6.5844e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3577e-04 - val_loss: 6.4895e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2833e-04 - val_loss: 6.4411e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1701e-04 - val_loss: 6.3569e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0697e-04 - val_loss: 6.2624e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0044e-04 - val_loss: 6.1750e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9212e-04 - val_loss: 6.1987e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8531e-04 - val_loss: 6.0480e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7720e-04 - val_loss: 5.9173e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6747e-04 - val_loss: 5.8611e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6088e-04 - val_loss: 5.7639e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5316e-04 - val_loss: 5.7036e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4555e-04 - val_loss: 5.5811e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3984e-04 - val_loss: 5.5478e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3162e-04 - val_loss: 5.4924e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2515e-04 - val_loss: 5.3834e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1935e-04 - val_loss: 5.3862e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1476e-04 - val_loss: 5.2521e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0510e-04 - val_loss: 5.2275e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0159e-04 - val_loss: 5.1805e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9438e-04 - val_loss: 5.0740e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8762e-04 - val_loss: 4.9789e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8051e-04 - val_loss: 4.9796e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7698e-04 - val_loss: 4.9235e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7696e-04 - val_loss: 4.8661e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6545e-04 - val_loss: 4.7612e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5941e-04 - val_loss: 4.7219e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5507e-04 - val_loss: 4.6990e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5066e-04 - val_loss: 4.6040e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4437e-04 - val_loss: 4.6202e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4084e-04 - val_loss: 4.5178e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4008e-04 - val_loss: 4.6042e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3076e-04 - val_loss: 4.4522e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2786e-04 - val_loss: 4.3867e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "10\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 4ms/step - loss: 0.2451 - val_loss: 0.1808\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1325 - val_loss: 0.0985\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0740 - val_loss: 0.0647\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0520 - val_loss: 0.0512\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0413 - val_loss: 0.0411\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0335 - val_loss: 0.0332\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0267 - val_loss: 0.0255\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0204\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0179 - val_loss: 0.0174\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0154\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0139\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0128\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0119\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0098\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0085\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 73/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.9307e-04 - val_loss: 0.0010\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.7543e-04 - val_loss: 9.9149e-04\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.6163e-04 - val_loss: 9.9123e-04\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.4524e-04 - val_loss: 9.5599e-04\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.3223e-04 - val_loss: 9.6198e-04\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.1988e-04 - val_loss: 9.6996e-04\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0454e-04 - val_loss: 9.2143e-04\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.9550e-04 - val_loss: 9.1788e-04\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.8593e-04 - val_loss: 9.1828e-04\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7235e-04 - val_loss: 8.9111e-04\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6068e-04 - val_loss: 8.8261e-04\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4799e-04 - val_loss: 8.7426e-04\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3535e-04 - val_loss: 8.6223e-04\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2814e-04 - val_loss: 8.6269e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1668e-04 - val_loss: 8.6020e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0610e-04 - val_loss: 8.2404e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9317e-04 - val_loss: 8.1926e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.8899e-04 - val_loss: 8.1625e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7673e-04 - val_loss: 8.1329e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7027e-04 - val_loss: 7.9312e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.6252e-04 - val_loss: 7.7346e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5013e-04 - val_loss: 7.7131e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.4126e-04 - val_loss: 7.6424e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3382e-04 - val_loss: 7.5669e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.2324e-04 - val_loss: 7.4111e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1403e-04 - val_loss: 7.5210e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0663e-04 - val_loss: 7.5126e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0346e-04 - val_loss: 7.2902e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9344e-04 - val_loss: 7.2253e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8840e-04 - val_loss: 7.0105e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.7632e-04 - val_loss: 7.2451e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7082e-04 - val_loss: 7.1748e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6563e-04 - val_loss: 6.8649e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5970e-04 - val_loss: 6.8119e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4953e-04 - val_loss: 6.5945e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.4225e-04 - val_loss: 6.6607e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3765e-04 - val_loss: 6.5752e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2894e-04 - val_loss: 6.5110e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2574e-04 - val_loss: 6.3136e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1695e-04 - val_loss: 6.3268e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1465e-04 - val_loss: 6.2526e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0738e-04 - val_loss: 6.2071e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0000e-04 - val_loss: 6.2278e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9491e-04 - val_loss: 6.2416e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8824e-04 - val_loss: 6.0857e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8379e-04 - val_loss: 6.0319e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.7626e-04 - val_loss: 5.9311e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7104e-04 - val_loss: 5.9690e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6659e-04 - val_loss: 5.9760e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.6417e-04 - val_loss: 5.7508e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6196e-04 - val_loss: 5.6212e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4644e-04 - val_loss: 5.8376e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.4222e-04 - val_loss: 5.5162e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3823e-04 - val_loss: 5.5744e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3443e-04 - val_loss: 5.5377e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2908e-04 - val_loss: 5.5309e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2844e-04 - val_loss: 5.4362e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.2060e-04 - val_loss: 5.5720e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1765e-04 - val_loss: 5.3337e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1143e-04 - val_loss: 5.3820e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.0716e-04 - val_loss: 5.2244e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.0397e-04 - val_loss: 5.2392e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.9840e-04 - val_loss: 5.2566e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.9531e-04 - val_loss: 5.1955e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9035e-04 - val_loss: 5.0087e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8707e-04 - val_loss: 5.0467e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8139e-04 - val_loss: 5.1388e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8072e-04 - val_loss: 4.9955e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7814e-04 - val_loss: 5.0058e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7112e-04 - val_loss: 4.8414e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6322e-04 - val_loss: 4.9950e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.6549e-04 - val_loss: 4.8125e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.5972e-04 - val_loss: 4.7697e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.6061e-04 - val_loss: 5.0192e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.5286e-04 - val_loss: 4.7590e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.4766e-04 - val_loss: 4.5790e-04\n",
      "81/81 [==============================] - 0s 725us/step\n",
      "11\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.2953 - val_loss: 0.2059\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1360 - val_loss: 0.1030\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0774 - val_loss: 0.0717\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0589 - val_loss: 0.0575\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0487 - val_loss: 0.0475\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0412 - val_loss: 0.0400\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0327\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0284\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0255\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0232\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0213\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0197\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0184\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0172\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0161\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0151\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0143\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0135\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.0128\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0121\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0115\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.0095\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 0.0091\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0087\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8821e-04 - val_loss: 9.9167e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8529e-04 - val_loss: 9.6994e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6470e-04 - val_loss: 9.7260e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.5535e-04 - val_loss: 9.5574e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4065e-04 - val_loss: 9.4030e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2501e-04 - val_loss: 9.4058e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1728e-04 - val_loss: 9.1790e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.0476e-04 - val_loss: 9.0217e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9352e-04 - val_loss: 8.8767e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.8142e-04 - val_loss: 8.8332e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7503e-04 - val_loss: 8.8028e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6602e-04 - val_loss: 8.5242e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5011e-04 - val_loss: 8.4747e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.3778e-04 - val_loss: 8.3589e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3604e-04 - val_loss: 8.2888e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2028e-04 - val_loss: 8.1916e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.1379e-04 - val_loss: 8.0843e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0698e-04 - val_loss: 8.2482e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9920e-04 - val_loss: 8.0491e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.8798e-04 - val_loss: 7.8068e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7552e-04 - val_loss: 7.7262e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7311e-04 - val_loss: 7.6835e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6071e-04 - val_loss: 7.6257e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.4990e-04 - val_loss: 7.4462e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4103e-04 - val_loss: 7.3776e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.3635e-04 - val_loss: 7.4386e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3289e-04 - val_loss: 7.3108e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.1831e-04 - val_loss: 7.1741e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1363e-04 - val_loss: 7.0944e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.0918e-04 - val_loss: 7.2932e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0333e-04 - val_loss: 6.9948e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8964e-04 - val_loss: 6.8156e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.8398e-04 - val_loss: 6.8529e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7554e-04 - val_loss: 6.7131e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7298e-04 - val_loss: 6.6061e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6658e-04 - val_loss: 6.5867e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5884e-04 - val_loss: 6.5101e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4697e-04 - val_loss: 6.4526e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5048e-04 - val_loss: 6.3864e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4149e-04 - val_loss: 6.2570e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2708e-04 - val_loss: 6.2944e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2834e-04 - val_loss: 6.1688e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "12\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.6958 - val_loss: 0.5252\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3653 - val_loss: 0.2627\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1696 - val_loss: 0.1122\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0779 - val_loss: 0.0641\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0510 - val_loss: 0.0489\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0408 - val_loss: 0.0401\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0340\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0298\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0271\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0252\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0237\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0224\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0213\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0203\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0193\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0184\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0175\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0167\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0160\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0152\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0146\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0139\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0133\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.0127\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.0122\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.0100\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0089\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0086\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0060\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0058\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0053\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8762e-04 - val_loss: 9.8730e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7375e-04 - val_loss: 9.7364e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6311e-04 - val_loss: 9.6389e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4773e-04 - val_loss: 9.4707e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3349e-04 - val_loss: 9.3822e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2081e-04 - val_loss: 9.2107e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0937e-04 - val_loss: 9.1637e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9692e-04 - val_loss: 8.9680e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8640e-04 - val_loss: 8.8845e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7618e-04 - val_loss: 8.7576e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6217e-04 - val_loss: 8.6382e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5073e-04 - val_loss: 8.5503e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3931e-04 - val_loss: 8.4124e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2860e-04 - val_loss: 8.3295e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1884e-04 - val_loss: 8.2642e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1124e-04 - val_loss: 8.1099e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9873e-04 - val_loss: 8.0092e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8892e-04 - val_loss: 7.8996e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8162e-04 - val_loss: 7.8410e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7023e-04 - val_loss: 7.7518e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6006e-04 - val_loss: 7.6260e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5257e-04 - val_loss: 7.5515e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4368e-04 - val_loss: 7.4342e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3323e-04 - val_loss: 7.3851e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2588e-04 - val_loss: 7.2497e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1704e-04 - val_loss: 7.1967e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0860e-04 - val_loss: 7.1153e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9991e-04 - val_loss: 7.0788e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9573e-04 - val_loss: 6.9551e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8651e-04 - val_loss: 6.8744e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8048e-04 - val_loss: 6.7951e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6929e-04 - val_loss: 6.7428e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6273e-04 - val_loss: 6.6452e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.5611e-04 - val_loss: 6.6475e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4855e-04 - val_loss: 6.5477e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4158e-04 - val_loss: 6.4409e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3476e-04 - val_loss: 6.3789e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2862e-04 - val_loss: 6.3043e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2170e-04 - val_loss: 6.2505e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1436e-04 - val_loss: 6.1616e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0887e-04 - val_loss: 6.0998e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "13\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 7ms/step - loss: 0.6482 - val_loss: 0.5569\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3860 - val_loss: 0.2759\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1652 - val_loss: 0.1063\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0688 - val_loss: 0.0569\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0445 - val_loss: 0.0418\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 0.0336\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0286\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0256\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0235\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0220\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0208\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0197\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0187\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0178\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0169\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0161\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0153\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0145\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0138\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0131\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.0125\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0119\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0114 - val_loss: 0.0113\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0090\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0087 - val_loss: 0.0086\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0061\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0052\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 97/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9271e-04 - val_loss: 9.9000e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7773e-04 - val_loss: 9.9701e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6261e-04 - val_loss: 9.6196e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4964e-04 - val_loss: 9.4620e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3521e-04 - val_loss: 9.3526e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2147e-04 - val_loss: 9.2177e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1099e-04 - val_loss: 9.0815e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9672e-04 - val_loss: 8.9715e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8313e-04 - val_loss: 8.8391e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7068e-04 - val_loss: 8.7210e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5772e-04 - val_loss: 8.6762e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4846e-04 - val_loss: 8.4537e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3676e-04 - val_loss: 8.4113e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2635e-04 - val_loss: 8.2654e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1418e-04 - val_loss: 8.1832e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0147e-04 - val_loss: 8.0155e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9389e-04 - val_loss: 7.9391e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8270e-04 - val_loss: 7.7971e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7217e-04 - val_loss: 7.7160e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6123e-04 - val_loss: 7.5967e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5187e-04 - val_loss: 7.5195e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4329e-04 - val_loss: 7.5224e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3524e-04 - val_loss: 7.3368e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2411e-04 - val_loss: 7.2693e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1677e-04 - val_loss: 7.2665e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0889e-04 - val_loss: 7.0920e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9800e-04 - val_loss: 6.9555e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8881e-04 - val_loss: 6.9544e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8195e-04 - val_loss: 6.8325e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7251e-04 - val_loss: 6.7907e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6681e-04 - val_loss: 6.6897e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5735e-04 - val_loss: 6.5817e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4929e-04 - val_loss: 6.4884e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4144e-04 - val_loss: 6.4275e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3538e-04 - val_loss: 6.3842e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2737e-04 - val_loss: 6.2843e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2674e-04 - val_loss: 6.3482e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.1715e-04 - val_loss: 6.1387e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0607e-04 - val_loss: 6.0874e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0244e-04 - val_loss: 6.0192e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9250e-04 - val_loss: 5.9316e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8698e-04 - val_loss: 5.8582e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7997e-04 - val_loss: 5.7981e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "14\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.4025 - val_loss: 0.3081\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1956 - val_loss: 0.1395\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0856 - val_loss: 0.0645\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0467 - val_loss: 0.0421\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.0329\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0275\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0240\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0217\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0205 - val_loss: 0.0200\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0187\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0176\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0167\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0159\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0152\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0146\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0134\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0123\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0114\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0097\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0093\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9364e-04 - val_loss: 0.0010\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7616e-04 - val_loss: 9.8157e-04\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5443e-04 - val_loss: 9.6047e-04\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3622e-04 - val_loss: 9.4027e-04\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1967e-04 - val_loss: 9.3666e-04\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.0148e-04 - val_loss: 9.0688e-04\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8660e-04 - val_loss: 8.9017e-04\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.7230e-04 - val_loss: 8.7982e-04\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5823e-04 - val_loss: 8.5938e-04\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4120e-04 - val_loss: 8.4905e-04\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2736e-04 - val_loss: 8.3300e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1523e-04 - val_loss: 8.2934e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0536e-04 - val_loss: 8.0874e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9171e-04 - val_loss: 7.9331e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7932e-04 - val_loss: 7.8456e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6882e-04 - val_loss: 7.7665e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5887e-04 - val_loss: 7.7060e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4729e-04 - val_loss: 7.5100e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.3527e-04 - val_loss: 7.4330e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2937e-04 - val_loss: 7.3583e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1736e-04 - val_loss: 7.2039e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0564e-04 - val_loss: 7.0979e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9685e-04 - val_loss: 7.0225e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8876e-04 - val_loss: 6.9752e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7997e-04 - val_loss: 6.8467e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7091e-04 - val_loss: 6.7728e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6485e-04 - val_loss: 6.6968e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5419e-04 - val_loss: 6.6370e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4750e-04 - val_loss: 6.5020e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3847e-04 - val_loss: 6.4102e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3192e-04 - val_loss: 6.3582e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2391e-04 - val_loss: 6.2894e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1719e-04 - val_loss: 6.2161e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0905e-04 - val_loss: 6.1268e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0228e-04 - val_loss: 6.0719e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9535e-04 - val_loss: 6.0069e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8918e-04 - val_loss: 5.9287e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8207e-04 - val_loss: 5.8332e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7688e-04 - val_loss: 5.8104e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7003e-04 - val_loss: 5.7341e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6378e-04 - val_loss: 5.6564e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5776e-04 - val_loss: 5.5918e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5139e-04 - val_loss: 5.5875e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4803e-04 - val_loss: 5.4928e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4192e-04 - val_loss: 5.4792e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3415e-04 - val_loss: 5.3727e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2796e-04 - val_loss: 5.3203e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2399e-04 - val_loss: 5.3454e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1848e-04 - val_loss: 5.2037e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1220e-04 - val_loss: 5.1623e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0903e-04 - val_loss: 5.1099e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0216e-04 - val_loss: 5.0467e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9840e-04 - val_loss: 5.0643e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9551e-04 - val_loss: 4.9572e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8746e-04 - val_loss: 4.9160e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8290e-04 - val_loss: 4.8728e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7976e-04 - val_loss: 4.8650e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7508e-04 - val_loss: 4.7778e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6934e-04 - val_loss: 4.7534e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6640e-04 - val_loss: 4.6745e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6048e-04 - val_loss: 4.6639e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5726e-04 - val_loss: 4.6435e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5263e-04 - val_loss: 4.5732e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4870e-04 - val_loss: 4.5121e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4591e-04 - val_loss: 4.4955e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4244e-04 - val_loss: 4.4346e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3761e-04 - val_loss: 4.3923e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3403e-04 - val_loss: 4.4213e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.3007e-04 - val_loss: 4.3421e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2773e-04 - val_loss: 4.2751e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2173e-04 - val_loss: 4.2710e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1809e-04 - val_loss: 4.2095e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1604e-04 - val_loss: 4.1771e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "15\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.7002 - val_loss: 0.5581\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3774 - val_loss: 0.2972\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1925 - val_loss: 0.1537\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1034 - val_loss: 0.0946\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0707 - val_loss: 0.0729\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0606\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0495 - val_loss: 0.0517\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.0451\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.0401\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 0.0365\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.0329\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0302\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0282\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0266\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0252\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0239\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0227\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0215\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0205\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0195\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0186\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0176\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0160\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.0145\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0138\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0132\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.0126\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.0101\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0093\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0089\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0082\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.0079\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.0076\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0060\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0058\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0056\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0052\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0051\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0049\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0047\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0045\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8562e-04 - val_loss: 0.0010\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7203e-04 - val_loss: 9.8937e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5682e-04 - val_loss: 9.7337e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4495e-04 - val_loss: 9.6503e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2941e-04 - val_loss: 9.5488e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1770e-04 - val_loss: 9.4314e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0271e-04 - val_loss: 9.1757e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9231e-04 - val_loss: 9.0684e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7677e-04 - val_loss: 9.1394e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6825e-04 - val_loss: 8.8854e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5505e-04 - val_loss: 8.8091e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4086e-04 - val_loss: 8.5930e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3066e-04 - val_loss: 8.4694e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2062e-04 - val_loss: 8.4414e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1140e-04 - val_loss: 8.2785e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9968e-04 - val_loss: 8.2024e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9119e-04 - val_loss: 7.9996e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8050e-04 - val_loss: 7.9462e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6804e-04 - val_loss: 7.8829e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6076e-04 - val_loss: 7.8117e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4702e-04 - val_loss: 7.7067e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3848e-04 - val_loss: 7.6116e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3053e-04 - val_loss: 7.4218e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2181e-04 - val_loss: 7.3645e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1268e-04 - val_loss: 7.3710e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0384e-04 - val_loss: 7.3227e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9525e-04 - val_loss: 7.2001e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8733e-04 - val_loss: 6.9750e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7942e-04 - val_loss: 6.8720e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "16\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.5976 - val_loss: 0.5147\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3714 - val_loss: 0.2947\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1885 - val_loss: 0.1359\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.0719\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0510 - val_loss: 0.0525\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0386 - val_loss: 0.0386\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0281\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0220\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0188\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0169\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0156\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0147\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0139\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0132\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.0126\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0120\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0116\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0096\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0088\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8297e-04 - val_loss: 9.8672e-04\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6313e-04 - val_loss: 9.7014e-04\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4474e-04 - val_loss: 9.5308e-04\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2600e-04 - val_loss: 9.3791e-04\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0839e-04 - val_loss: 9.1457e-04\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8999e-04 - val_loss: 8.9718e-04\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7524e-04 - val_loss: 8.8011e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5949e-04 - val_loss: 8.7604e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4922e-04 - val_loss: 8.4704e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3144e-04 - val_loss: 8.4668e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1746e-04 - val_loss: 8.2054e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0276e-04 - val_loss: 8.1787e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9253e-04 - val_loss: 7.9528e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7838e-04 - val_loss: 7.8636e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6657e-04 - val_loss: 7.7121e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5490e-04 - val_loss: 7.6230e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4493e-04 - val_loss: 7.4756e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3233e-04 - val_loss: 7.3875e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2339e-04 - val_loss: 7.2988e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1196e-04 - val_loss: 7.1597e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0139e-04 - val_loss: 7.0634e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9185e-04 - val_loss: 7.0295e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8209e-04 - val_loss: 6.8874e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7288e-04 - val_loss: 6.8128e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6432e-04 - val_loss: 6.7128e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5599e-04 - val_loss: 6.5932e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4654e-04 - val_loss: 6.5180e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3895e-04 - val_loss: 6.5273e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3085e-04 - val_loss: 6.3560e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2309e-04 - val_loss: 6.2652e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1301e-04 - val_loss: 6.1767e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0573e-04 - val_loss: 6.1542e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0009e-04 - val_loss: 6.0402e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9168e-04 - val_loss: 5.9669e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8445e-04 - val_loss: 5.9158e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7769e-04 - val_loss: 5.8227e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7187e-04 - val_loss: 5.7590e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6493e-04 - val_loss: 5.6866e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5702e-04 - val_loss: 5.6221e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5030e-04 - val_loss: 5.5567e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4657e-04 - val_loss: 5.6034e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4091e-04 - val_loss: 5.4596e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3422e-04 - val_loss: 5.3778e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2783e-04 - val_loss: 5.3533e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2165e-04 - val_loss: 5.2703e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1595e-04 - val_loss: 5.2007e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1124e-04 - val_loss: 5.1585e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0606e-04 - val_loss: 5.1383e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0074e-04 - val_loss: 5.0627e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9510e-04 - val_loss: 4.9899e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8953e-04 - val_loss: 4.9660e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8515e-04 - val_loss: 4.8864e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.8044e-04 - val_loss: 4.8628e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7543e-04 - val_loss: 4.8184e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7037e-04 - val_loss: 4.7448e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6619e-04 - val_loss: 4.7108e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6098e-04 - val_loss: 4.6867e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5679e-04 - val_loss: 4.6088e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5278e-04 - val_loss: 4.5803e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4846e-04 - val_loss: 4.5359e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4473e-04 - val_loss: 4.4724e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4068e-04 - val_loss: 4.4998e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3553e-04 - val_loss: 4.3960e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3177e-04 - val_loss: 4.3524e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2831e-04 - val_loss: 4.3392e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2341e-04 - val_loss: 4.2799e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2255e-04 - val_loss: 4.2714e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1657e-04 - val_loss: 4.2185e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1263e-04 - val_loss: 4.1844e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "17\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.6254 - val_loss: 0.4394\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2582 - val_loss: 0.1656\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1024 - val_loss: 0.0860\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0655 - val_loss: 0.0641\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0520 - val_loss: 0.0508\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0433 - val_loss: 0.0422\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0365\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0327\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0300\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0279\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0262\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0248\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0235\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0224\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0213\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0203\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0194\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0185\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0177\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0169\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0162\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0155\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0149\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0142\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0136\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0131\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.0125\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0120\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0115\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 0.0091\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8467e-04 - val_loss: 0.0010\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7183e-04 - val_loss: 9.9354e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5929e-04 - val_loss: 9.8814e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4410e-04 - val_loss: 9.6315e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2980e-04 - val_loss: 9.5412e-04\n",
      "Epoch 129/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1581e-04 - val_loss: 9.4276e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0395e-04 - val_loss: 9.2811e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9043e-04 - val_loss: 9.0935e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7985e-04 - val_loss: 9.0000e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6726e-04 - val_loss: 8.8992e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5354e-04 - val_loss: 8.7321e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4324e-04 - val_loss: 8.6418e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3478e-04 - val_loss: 8.5119e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2034e-04 - val_loss: 8.4018e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0976e-04 - val_loss: 8.2903e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9899e-04 - val_loss: 8.1837e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8924e-04 - val_loss: 8.0615e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7863e-04 - val_loss: 7.9709e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6866e-04 - val_loss: 7.8879e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5958e-04 - val_loss: 7.7858e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4951e-04 - val_loss: 7.6675e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4052e-04 - val_loss: 7.5733e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3345e-04 - val_loss: 7.5537e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2344e-04 - val_loss: 7.3924e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1299e-04 - val_loss: 7.3769e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0661e-04 - val_loss: 7.2273e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9776e-04 - val_loss: 7.2230e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "18\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.8330 - val_loss: 0.7548\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.6069 - val_loss: 0.5478\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.4012 - val_loss: 0.2882\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1883 - val_loss: 0.1250\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0874 - val_loss: 0.0674\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0543 - val_loss: 0.0515\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0438 - val_loss: 0.0442\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 0.0385\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 0.0338\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.0299\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0268\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0243\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0223\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0207\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0194\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0183\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.0173\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0164\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0142\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0136\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0130\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0125\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0098\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0094\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0091\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0087\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.0076\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0061\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0058\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7865e-04 - val_loss: 9.9751e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5796e-04 - val_loss: 9.8279e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4132e-04 - val_loss: 9.5976e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2234e-04 - val_loss: 9.4309e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0880e-04 - val_loss: 9.3691e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8946e-04 - val_loss: 9.1364e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7478e-04 - val_loss: 8.9678e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5971e-04 - val_loss: 8.8661e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4490e-04 - val_loss: 8.6096e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2906e-04 - val_loss: 8.4862e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1283e-04 - val_loss: 8.4301e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0087e-04 - val_loss: 8.1881e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8699e-04 - val_loss: 8.0807e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7691e-04 - val_loss: 8.0224e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6506e-04 - val_loss: 7.9406e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4848e-04 - val_loss: 7.7089e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3552e-04 - val_loss: 7.6004e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2462e-04 - val_loss: 7.4963e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1477e-04 - val_loss: 7.3861e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0355e-04 - val_loss: 7.2127e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9225e-04 - val_loss: 7.1562e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8047e-04 - val_loss: 7.0867e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6940e-04 - val_loss: 6.9547e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5963e-04 - val_loss: 6.9334e-04\n",
      "Epoch 137/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5068e-04 - val_loss: 6.8634e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4461e-04 - val_loss: 6.6361e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3141e-04 - val_loss: 6.5345e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2210e-04 - val_loss: 6.6155e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1630e-04 - val_loss: 6.3767e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0450e-04 - val_loss: 6.2649e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9522e-04 - val_loss: 6.2511e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8854e-04 - val_loss: 6.1777e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8124e-04 - val_loss: 6.0220e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7162e-04 - val_loss: 5.9642e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6719e-04 - val_loss: 5.8763e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5762e-04 - val_loss: 5.8100e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5075e-04 - val_loss: 5.7231e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4259e-04 - val_loss: 5.6573e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "19\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.2340 - val_loss: 0.1617\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0997 - val_loss: 0.0809\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0566 - val_loss: 0.0529\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0401 - val_loss: 0.0385\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0297\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0243\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0210\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0187\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0170\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0156\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0144\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0134\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0124\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0097\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0091\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0086\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9402e-04 - val_loss: 0.0010\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7264e-04 - val_loss: 9.7949e-04\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5502e-04 - val_loss: 9.6004e-04\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3694e-04 - val_loss: 9.4792e-04\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2142e-04 - val_loss: 9.2812e-04\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0342e-04 - val_loss: 9.1020e-04\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8775e-04 - val_loss: 8.9806e-04\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7383e-04 - val_loss: 8.7930e-04\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5682e-04 - val_loss: 8.6747e-04\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4379e-04 - val_loss: 8.4952e-04\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3001e-04 - val_loss: 8.4809e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1695e-04 - val_loss: 8.2758e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0239e-04 - val_loss: 8.1019e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9167e-04 - val_loss: 8.0128e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8081e-04 - val_loss: 7.9532e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6946e-04 - val_loss: 7.7661e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5499e-04 - val_loss: 7.6876e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4643e-04 - val_loss: 7.5455e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3330e-04 - val_loss: 7.4085e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2323e-04 - val_loss: 7.2816e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1251e-04 - val_loss: 7.1753e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0172e-04 - val_loss: 7.1230e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9383e-04 - val_loss: 6.9948e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8410e-04 - val_loss: 6.8849e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7493e-04 - val_loss: 6.8237e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6727e-04 - val_loss: 6.7260e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5856e-04 - val_loss: 6.6686e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4784e-04 - val_loss: 6.5462e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3959e-04 - val_loss: 6.4577e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3143e-04 - val_loss: 6.3736e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2317e-04 - val_loss: 6.3203e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1704e-04 - val_loss: 6.2268e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0924e-04 - val_loss: 6.1762e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0133e-04 - val_loss: 6.0597e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9365e-04 - val_loss: 6.0026e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8674e-04 - val_loss: 5.9025e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8064e-04 - val_loss: 5.9184e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7251e-04 - val_loss: 5.7814e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6571e-04 - val_loss: 5.7095e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5971e-04 - val_loss: 5.6656e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5338e-04 - val_loss: 5.5912e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4664e-04 - val_loss: 5.5162e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3910e-04 - val_loss: 5.4619e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3378e-04 - val_loss: 5.4306e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2894e-04 - val_loss: 5.3736e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2210e-04 - val_loss: 5.2633e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1551e-04 - val_loss: 5.2044e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1116e-04 - val_loss: 5.2950e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0692e-04 - val_loss: 5.1097e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0031e-04 - val_loss: 5.0776e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9454e-04 - val_loss: 5.0616e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8926e-04 - val_loss: 4.9362e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8343e-04 - val_loss: 4.8910e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7915e-04 - val_loss: 4.8694e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7531e-04 - val_loss: 4.8644e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7132e-04 - val_loss: 4.8160e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6525e-04 - val_loss: 4.6905e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6130e-04 - val_loss: 4.6408e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5554e-04 - val_loss: 4.6366e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5067e-04 - val_loss: 4.9165e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5176e-04 - val_loss: 4.5782e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4482e-04 - val_loss: 4.5345e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3877e-04 - val_loss: 4.5016e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3575e-04 - val_loss: 4.3914e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3073e-04 - val_loss: 4.3724e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2630e-04 - val_loss: 4.3276e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2333e-04 - val_loss: 4.3039e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1836e-04 - val_loss: 4.2716e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1529e-04 - val_loss: 4.2412e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1244e-04 - val_loss: 4.1591e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0783e-04 - val_loss: 4.1658e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0412e-04 - val_loss: 4.1063e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0024e-04 - val_loss: 4.0826e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "20\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.6654 - val_loss: 0.5105\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3618 - val_loss: 0.2536\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1670 - val_loss: 0.1069\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0748 - val_loss: 0.0586\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0465 - val_loss: 0.0443\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 0.0364\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0301\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0229\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0191\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0175\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0163\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0153\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0144\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0136\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0122\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0095\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0091\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 73/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9049e-04 - val_loss: 9.9886e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7732e-04 - val_loss: 9.8035e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6172e-04 - val_loss: 9.6492e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5022e-04 - val_loss: 9.4832e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3277e-04 - val_loss: 9.4304e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2174e-04 - val_loss: 9.3383e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0988e-04 - val_loss: 9.1071e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9904e-04 - val_loss: 8.9759e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8815e-04 - val_loss: 9.0474e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7579e-04 - val_loss: 8.8062e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6446e-04 - val_loss: 8.7434e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5514e-04 - val_loss: 8.5461e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4306e-04 - val_loss: 8.6373e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3173e-04 - val_loss: 8.3723e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2218e-04 - val_loss: 8.2104e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0977e-04 - val_loss: 8.1498e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0362e-04 - val_loss: 8.0906e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9351e-04 - val_loss: 7.9459e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8453e-04 - val_loss: 7.9116e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7571e-04 - val_loss: 7.7695e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6825e-04 - val_loss: 7.7205e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5915e-04 - val_loss: 7.6476e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4869e-04 - val_loss: 7.5270e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4283e-04 - val_loss: 7.3965e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3251e-04 - val_loss: 7.3428e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2512e-04 - val_loss: 7.2931e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1592e-04 - val_loss: 7.2048e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1016e-04 - val_loss: 7.1261e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0735e-04 - val_loss: 7.0666e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9771e-04 - val_loss: 7.0051e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8929e-04 - val_loss: 6.9263e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8360e-04 - val_loss: 6.8820e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8049e-04 - val_loss: 6.7749e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6934e-04 - val_loss: 6.7448e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6340e-04 - val_loss: 6.6097e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5679e-04 - val_loss: 6.5588e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4969e-04 - val_loss: 6.4729e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4335e-04 - val_loss: 6.4280e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3840e-04 - val_loss: 6.3655e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3535e-04 - val_loss: 6.3237e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2596e-04 - val_loss: 6.2820e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1906e-04 - val_loss: 6.1944e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1477e-04 - val_loss: 6.1497e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0910e-04 - val_loss: 6.0762e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0367e-04 - val_loss: 6.0549e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9951e-04 - val_loss: 6.0249e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9371e-04 - val_loss: 5.9106e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8656e-04 - val_loss: 5.8674e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8099e-04 - val_loss: 5.8214e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7974e-04 - val_loss: 5.7634e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7800e-04 - val_loss: 5.7771e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6592e-04 - val_loss: 5.6867e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6649e-04 - val_loss: 5.6227e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5726e-04 - val_loss: 5.5793e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5411e-04 - val_loss: 5.5103e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4880e-04 - val_loss: 5.5008e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4508e-04 - val_loss: 5.5402e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4185e-04 - val_loss: 5.3856e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3596e-04 - val_loss: 5.5623e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3609e-04 - val_loss: 5.3120e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2606e-04 - val_loss: 5.2525e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2122e-04 - val_loss: 5.2074e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1755e-04 - val_loss: 5.2227e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "21\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.3967 - val_loss: 0.2536\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1564 - val_loss: 0.1265\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0962 - val_loss: 0.0967\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0769 - val_loss: 0.0814\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0646 - val_loss: 0.0689\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0549 - val_loss: 0.0585\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0470 - val_loss: 0.0500\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0404 - val_loss: 0.0427\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.0369\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0323\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0285\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0257\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0235\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0219\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0205\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0194\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0185\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0176\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0161\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0155\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0149\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0143\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0137\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.0132\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.0127\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0123\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0118\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.0096\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0093\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0090\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0087\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0085\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0082\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.0080\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0065\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0061\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0059\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0058\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0056\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0053\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0050\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0049\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 80/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9104e-04 - val_loss: 9.9718e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8129e-04 - val_loss: 9.7955e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6546e-04 - val_loss: 9.7054e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5364e-04 - val_loss: 9.6179e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4287e-04 - val_loss: 9.5146e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3769e-04 - val_loss: 9.4249e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1575e-04 - val_loss: 9.1834e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0948e-04 - val_loss: 9.5125e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0030e-04 - val_loss: 8.9929e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8446e-04 - val_loss: 8.8625e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "22\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 11ms/step - loss: 1.2334 - val_loss: 0.9636\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.6431 - val_loss: 0.4673\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2673 - val_loss: 0.1652\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0936 - val_loss: 0.0738\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0550 - val_loss: 0.0556\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.0455\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 0.0386\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.0335\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0272\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0252\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0237\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0225\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0215\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0206\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0198\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0190\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0183\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0176\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0170\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0164\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0158\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0152\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0147\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0142\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0137\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0132\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.0128\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.0124\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0120\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0116\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0099\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0093\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0090\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9036e-04 - val_loss: 9.9736e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7586e-04 - val_loss: 9.8620e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6445e-04 - val_loss: 9.6765e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5030e-04 - val_loss: 9.5462e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3823e-04 - val_loss: 9.4268e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2689e-04 - val_loss: 9.2942e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1371e-04 - val_loss: 9.1761e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0140e-04 - val_loss: 9.0729e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9081e-04 - val_loss: 8.9352e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7846e-04 - val_loss: 8.8138e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6759e-04 - val_loss: 8.7157e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5528e-04 - val_loss: 8.5930e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4487e-04 - val_loss: 8.5007e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3490e-04 - val_loss: 8.3805e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "23\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.6419 - val_loss: 0.4617\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3104 - val_loss: 0.2185\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1446 - val_loss: 0.1014\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0750 - val_loss: 0.0638\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0536 - val_loss: 0.0524\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0455 - val_loss: 0.0458\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0402 - val_loss: 0.0406\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 0.0363\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.0329\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0299\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0261\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0235\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0219\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0207\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0193\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0181\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0171\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0162\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0153\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0145\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0137\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0129\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.0122\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0078\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0066\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0059\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0053\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0050\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8654e-04 - val_loss: 9.8217e-04\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6871e-04 - val_loss: 9.6649e-04\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4879e-04 - val_loss: 9.4652e-04\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3130e-04 - val_loss: 9.3281e-04\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1609e-04 - val_loss: 9.1473e-04\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9945e-04 - val_loss: 9.0047e-04\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8550e-04 - val_loss: 8.8200e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6781e-04 - val_loss: 8.7010e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5510e-04 - val_loss: 8.5276e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4045e-04 - val_loss: 8.3874e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2763e-04 - val_loss: 8.2802e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1550e-04 - val_loss: 8.2050e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0350e-04 - val_loss: 8.0112e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8934e-04 - val_loss: 7.8929e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7779e-04 - val_loss: 7.7681e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6923e-04 - val_loss: 7.7731e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5631e-04 - val_loss: 7.5658e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4654e-04 - val_loss: 7.4705e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3638e-04 - val_loss: 7.3522e-04\n",
      "Epoch 101/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2750e-04 - val_loss: 7.2526e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1480e-04 - val_loss: 7.1532e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0513e-04 - val_loss: 7.0589e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9757e-04 - val_loss: 6.9906e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8768e-04 - val_loss: 6.9208e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7997e-04 - val_loss: 6.8533e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7077e-04 - val_loss: 6.7746e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6367e-04 - val_loss: 6.6742e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5446e-04 - val_loss: 6.5390e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4591e-04 - val_loss: 6.4910e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3753e-04 - val_loss: 6.4267e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3267e-04 - val_loss: 6.3219e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2241e-04 - val_loss: 6.2530e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1488e-04 - val_loss: 6.1922e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0825e-04 - val_loss: 6.0923e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0163e-04 - val_loss: 6.0532e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9517e-04 - val_loss: 6.0078e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8849e-04 - val_loss: 5.9108e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8045e-04 - val_loss: 5.8573e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7598e-04 - val_loss: 5.8123e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6801e-04 - val_loss: 5.7381e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6220e-04 - val_loss: 5.6783e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5657e-04 - val_loss: 5.6895e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5120e-04 - val_loss: 5.5613e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4354e-04 - val_loss: 5.4453e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3788e-04 - val_loss: 5.4071e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3206e-04 - val_loss: 5.3355e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2685e-04 - val_loss: 5.2993e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2147e-04 - val_loss: 5.2582e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1676e-04 - val_loss: 5.1972e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1146e-04 - val_loss: 5.1855e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0878e-04 - val_loss: 5.1131e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9998e-04 - val_loss: 5.0307e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9653e-04 - val_loss: 4.9962e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9153e-04 - val_loss: 4.9713e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8638e-04 - val_loss: 4.8703e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8064e-04 - val_loss: 4.8400e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7733e-04 - val_loss: 4.7960e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7330e-04 - val_loss: 4.7458e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6753e-04 - val_loss: 4.7131e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6473e-04 - val_loss: 4.6718e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6107e-04 - val_loss: 4.6942e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5591e-04 - val_loss: 4.6015e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5345e-04 - val_loss: 4.5656e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4696e-04 - val_loss: 4.5129e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4420e-04 - val_loss: 4.4556e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4056e-04 - val_loss: 4.4916e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3997e-04 - val_loss: 4.4145e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3425e-04 - val_loss: 4.3883e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2819e-04 - val_loss: 4.3095e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "24\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.5407 - val_loss: 0.4308\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2665 - val_loss: 0.1989\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1162 - val_loss: 0.1028\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0686 - val_loss: 0.0690\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.0484\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.0355\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0277\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0230\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0201\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0184\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0170\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0160\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0150\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0141\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0134\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0127\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0120\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 0.0094\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0090\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0086\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0064\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9480e-04 - val_loss: 0.0010\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7700e-04 - val_loss: 9.8410e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6177e-04 - val_loss: 9.7742e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4978e-04 - val_loss: 9.8043e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3574e-04 - val_loss: 9.4054e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1971e-04 - val_loss: 9.4132e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0823e-04 - val_loss: 9.2213e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9447e-04 - val_loss: 9.0218e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8085e-04 - val_loss: 8.9270e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7088e-04 - val_loss: 8.7866e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5875e-04 - val_loss: 8.6358e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4533e-04 - val_loss: 8.5456e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3565e-04 - val_loss: 8.4657e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2901e-04 - val_loss: 8.3192e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1396e-04 - val_loss: 8.2034e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0353e-04 - val_loss: 8.1102e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9406e-04 - val_loss: 7.9970e-04\n",
      "Epoch 107/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8266e-04 - val_loss: 7.9112e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7323e-04 - val_loss: 7.7976e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6695e-04 - val_loss: 7.7076e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5564e-04 - val_loss: 7.6006e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4582e-04 - val_loss: 7.5259e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3892e-04 - val_loss: 7.4249e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3043e-04 - val_loss: 7.3341e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2108e-04 - val_loss: 7.2674e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1297e-04 - val_loss: 7.1570e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0383e-04 - val_loss: 7.0893e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9629e-04 - val_loss: 7.0232e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9017e-04 - val_loss: 6.9665e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8119e-04 - val_loss: 6.8341e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7409e-04 - val_loss: 6.7863e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6715e-04 - val_loss: 6.6973e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6228e-04 - val_loss: 6.6797e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5349e-04 - val_loss: 6.5845e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4625e-04 - val_loss: 6.5003e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3834e-04 - val_loss: 6.4410e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3287e-04 - val_loss: 6.3770e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2516e-04 - val_loss: 6.2948e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1965e-04 - val_loss: 6.2389e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1537e-04 - val_loss: 6.1999e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0780e-04 - val_loss: 6.1476e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0069e-04 - val_loss: 6.1996e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0017e-04 - val_loss: 6.0161e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9382e-04 - val_loss: 5.9381e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8399e-04 - val_loss: 5.8901e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7936e-04 - val_loss: 5.8148e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7245e-04 - val_loss: 5.7687e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6975e-04 - val_loss: 5.7346e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6310e-04 - val_loss: 5.7401e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5871e-04 - val_loss: 5.6013e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5219e-04 - val_loss: 5.6058e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4773e-04 - val_loss: 5.5920e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4306e-04 - val_loss: 5.4680e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3833e-04 - val_loss: 5.4106e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3332e-04 - val_loss: 5.3881e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2907e-04 - val_loss: 5.3118e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2659e-04 - val_loss: 5.2688e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1841e-04 - val_loss: 5.2500e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1510e-04 - val_loss: 5.2336e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1213e-04 - val_loss: 5.1419e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0593e-04 - val_loss: 5.1074e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "25\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.5865 - val_loss: 0.5437\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.4376 - val_loss: 0.3822\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2858 - val_loss: 0.2114\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1481 - val_loss: 0.1088\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0738 - val_loss: 0.0575\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0420 - val_loss: 0.0401\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0298\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0243\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0209\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0189\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0175\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0165\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0137\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0132\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0127\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0122\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0118\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0099\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9377e-04 - val_loss: 9.9063e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7572e-04 - val_loss: 9.8038e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6212e-04 - val_loss: 9.5759e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4618e-04 - val_loss: 9.4454e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3030e-04 - val_loss: 9.2511e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1643e-04 - val_loss: 9.1339e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0343e-04 - val_loss: 8.9701e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9275e-04 - val_loss: 8.8518e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7990e-04 - val_loss: 8.7435e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6509e-04 - val_loss: 8.6684e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5495e-04 - val_loss: 8.5268e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4163e-04 - val_loss: 8.3971e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2860e-04 - val_loss: 8.2875e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1986e-04 - val_loss: 8.1497e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0861e-04 - val_loss: 8.0608e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9754e-04 - val_loss: 7.9322e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8981e-04 - val_loss: 7.8336e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8111e-04 - val_loss: 7.7331e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6748e-04 - val_loss: 7.6545e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6804e-04 - val_loss: 7.6244e-04\n",
      "Epoch 114/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5234e-04 - val_loss: 7.4812e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4364e-04 - val_loss: 7.3757e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3367e-04 - val_loss: 7.3251e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2351e-04 - val_loss: 7.2636e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1889e-04 - val_loss: 7.1235e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0698e-04 - val_loss: 7.1479e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0140e-04 - val_loss: 7.0125e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9438e-04 - val_loss: 6.9194e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8611e-04 - val_loss: 6.8658e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8389e-04 - val_loss: 6.8067e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7047e-04 - val_loss: 6.6703e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6568e-04 - val_loss: 6.6105e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5823e-04 - val_loss: 6.5574e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5106e-04 - val_loss: 6.4746e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4626e-04 - val_loss: 6.4062e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3806e-04 - val_loss: 6.4368e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3387e-04 - val_loss: 6.2700e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2629e-04 - val_loss: 6.2302e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2060e-04 - val_loss: 6.1745e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1286e-04 - val_loss: 6.1276e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0676e-04 - val_loss: 6.0370e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0207e-04 - val_loss: 6.0227e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9713e-04 - val_loss: 6.1610e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9249e-04 - val_loss: 5.9119e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8355e-04 - val_loss: 5.8384e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7925e-04 - val_loss: 5.7892e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7679e-04 - val_loss: 5.8017e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7168e-04 - val_loss: 5.6576e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6490e-04 - val_loss: 5.6264e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6027e-04 - val_loss: 5.5689e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5329e-04 - val_loss: 5.5119e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4878e-04 - val_loss: 5.5067e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4646e-04 - val_loss: 5.4108e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4090e-04 - val_loss: 5.4225e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3517e-04 - val_loss: 5.3426e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3078e-04 - val_loss: 5.2714e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2645e-04 - val_loss: 5.2248e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "26\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.5742 - val_loss: 0.4444\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2752 - val_loss: 0.1806\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0994 - val_loss: 0.0716\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.0556\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0426 - val_loss: 0.0438\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.0345\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0276\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0228\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0195\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0174\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0161\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0150\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0142\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0135\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.0129\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0124\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0097\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 0.0094\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0090\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0087\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9643e-04 - val_loss: 0.0010\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8288e-04 - val_loss: 9.9141e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7009e-04 - val_loss: 9.9094e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.6325e-04 - val_loss: 9.6266e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4926e-04 - val_loss: 9.5894e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3591e-04 - val_loss: 9.4718e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2170e-04 - val_loss: 9.2732e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1575e-04 - val_loss: 9.1503e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9815e-04 - val_loss: 8.9906e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8782e-04 - val_loss: 8.9249e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7852e-04 - val_loss: 8.8199e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.6763e-04 - val_loss: 8.6887e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5825e-04 - val_loss: 8.6054e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4555e-04 - val_loss: 8.4782e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3751e-04 - val_loss: 8.4284e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2761e-04 - val_loss: 8.4843e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1733e-04 - val_loss: 8.1479e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0565e-04 - val_loss: 8.0891e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9962e-04 - val_loss: 8.1383e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9194e-04 - val_loss: 7.9136e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8005e-04 - val_loss: 7.8206e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6909e-04 - val_loss: 7.9826e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6295e-04 - val_loss: 7.6382e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5302e-04 - val_loss: 7.5392e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4492e-04 - val_loss: 7.4665e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3750e-04 - val_loss: 7.5109e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2863e-04 - val_loss: 7.3496e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "27\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.2524 - val_loss: 0.1680\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0990 - val_loss: 0.0747\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0512 - val_loss: 0.0485\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.0359\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0294\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0257\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0231\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0211\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0195\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0180\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0168\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0157\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0148\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0139\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0130\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.0123\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 0.0093\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8835e-04 - val_loss: 9.9615e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7286e-04 - val_loss: 9.8300e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5887e-04 - val_loss: 9.6502e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4771e-04 - val_loss: 9.5563e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2899e-04 - val_loss: 9.3856e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1782e-04 - val_loss: 9.3697e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9923e-04 - val_loss: 9.3556e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0175e-04 - val_loss: 8.9642e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7704e-04 - val_loss: 8.8616e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6291e-04 - val_loss: 8.6841e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5029e-04 - val_loss: 8.5904e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4326e-04 - val_loss: 8.4764e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3084e-04 - val_loss: 8.4013e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1704e-04 - val_loss: 8.3160e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0817e-04 - val_loss: 8.1228e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9658e-04 - val_loss: 8.1190e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8561e-04 - val_loss: 8.0064e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7789e-04 - val_loss: 7.8623e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7530e-04 - val_loss: 7.8295e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5561e-04 - val_loss: 7.6114e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4595e-04 - val_loss: 7.5267e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4782e-04 - val_loss: 7.4132e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2585e-04 - val_loss: 7.4089e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2103e-04 - val_loss: 7.2316e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1080e-04 - val_loss: 7.1895e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0089e-04 - val_loss: 7.3423e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9836e-04 - val_loss: 7.0081e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8626e-04 - val_loss: 6.9692e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7727e-04 - val_loss: 6.8977e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6898e-04 - val_loss: 6.7648e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6057e-04 - val_loss: 6.6583e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5236e-04 - val_loss: 6.8879e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4867e-04 - val_loss: 6.5306e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3828e-04 - val_loss: 6.4387e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3256e-04 - val_loss: 6.3620e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2827e-04 - val_loss: 6.3273e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1786e-04 - val_loss: 6.2647e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1049e-04 - val_loss: 6.1849e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0323e-04 - val_loss: 6.0960e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9676e-04 - val_loss: 6.0982e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9561e-04 - val_loss: 6.2031e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8741e-04 - val_loss: 5.9005e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7919e-04 - val_loss: 5.9318e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7432e-04 - val_loss: 5.7969e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6622e-04 - val_loss: 5.9070e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6113e-04 - val_loss: 5.7039e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5371e-04 - val_loss: 5.5792e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4924e-04 - val_loss: 5.5252e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4228e-04 - val_loss: 5.4770e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3805e-04 - val_loss: 5.4979e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3385e-04 - val_loss: 5.4750e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2772e-04 - val_loss: 5.4809e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2365e-04 - val_loss: 5.2503e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "28\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.2492 - val_loss: 0.1512\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1062 - val_loss: 0.0905\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0705 - val_loss: 0.0671\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0535 - val_loss: 0.0512\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.0403\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.0331\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0284\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0252\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0228\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0209\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0193\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0179\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0167\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0156\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0147\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0138\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0129\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0122\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0115\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9440e-04 - val_loss: 0.0010\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7823e-04 - val_loss: 9.8141e-04\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5657e-04 - val_loss: 9.6124e-04\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3853e-04 - val_loss: 9.4279e-04\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2152e-04 - val_loss: 9.3197e-04\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0776e-04 - val_loss: 9.0941e-04\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8993e-04 - val_loss: 9.0664e-04\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7567e-04 - val_loss: 8.7724e-04\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5826e-04 - val_loss: 8.6598e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4656e-04 - val_loss: 8.5038e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3278e-04 - val_loss: 8.4718e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2105e-04 - val_loss: 8.2316e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0844e-04 - val_loss: 8.1156e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9259e-04 - val_loss: 8.0259e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8107e-04 - val_loss: 7.8527e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6942e-04 - val_loss: 7.7766e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5670e-04 - val_loss: 7.6926e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4584e-04 - val_loss: 7.4950e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3747e-04 - val_loss: 7.5936e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2649e-04 - val_loss: 7.3393e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1534e-04 - val_loss: 7.1717e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0344e-04 - val_loss: 7.0913e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9525e-04 - val_loss: 7.0998e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8861e-04 - val_loss: 7.0078e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7586e-04 - val_loss: 6.7968e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6712e-04 - val_loss: 6.7859e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5708e-04 - val_loss: 6.6615e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4898e-04 - val_loss: 6.5480e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4044e-04 - val_loss: 6.4695e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3103e-04 - val_loss: 6.3794e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2344e-04 - val_loss: 6.3160e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2054e-04 - val_loss: 6.3039e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1484e-04 - val_loss: 6.1630e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0045e-04 - val_loss: 6.0725e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9290e-04 - val_loss: 5.9854e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8584e-04 - val_loss: 5.9862e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8562e-04 - val_loss: 5.8869e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7288e-04 - val_loss: 5.8082e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6493e-04 - val_loss: 5.7254e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5896e-04 - val_loss: 5.6816e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5245e-04 - val_loss: 5.5938e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5016e-04 - val_loss: 5.5918e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4281e-04 - val_loss: 5.5030e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4108e-04 - val_loss: 5.4264e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2773e-04 - val_loss: 5.3515e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2214e-04 - val_loss: 5.2957e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1669e-04 - val_loss: 5.2324e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1031e-04 - val_loss: 5.1715e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0605e-04 - val_loss: 5.3585e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0888e-04 - val_loss: 5.1372e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9418e-04 - val_loss: 4.9969e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8965e-04 - val_loss: 4.9504e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8422e-04 - val_loss: 4.8960e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7895e-04 - val_loss: 4.8440e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7357e-04 - val_loss: 4.8047e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6892e-04 - val_loss: 4.7760e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6426e-04 - val_loss: 4.6953e-04\n",
      "Epoch 137/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6568e-04 - val_loss: 4.7607e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5933e-04 - val_loss: 4.6181e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5040e-04 - val_loss: 4.5620e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4649e-04 - val_loss: 4.5215e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4052e-04 - val_loss: 4.4759e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3717e-04 - val_loss: 4.4466e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4441e-04 - val_loss: 5.3409e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4514e-04 - val_loss: 4.3570e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2550e-04 - val_loss: 4.3224e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2287e-04 - val_loss: 4.2656e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1695e-04 - val_loss: 4.2424e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1303e-04 - val_loss: 4.1855e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0911e-04 - val_loss: 4.1596e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0658e-04 - val_loss: 4.1690e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "29\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.6072 - val_loss: 0.4254\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2730 - val_loss: 0.1825\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1175 - val_loss: 0.0880\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0630 - val_loss: 0.0589\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0453 - val_loss: 0.0443\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.0339\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0268\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0225\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0200\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0185\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0168\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0158\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0151\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0144\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0137\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0131\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.0126\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0120\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0115\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0096\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0061\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 9.9573e-04\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7678e-04 - val_loss: 9.9031e-04\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5399e-04 - val_loss: 9.5247e-04\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3179e-04 - val_loss: 9.3726e-04\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1204e-04 - val_loss: 9.0882e-04\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9275e-04 - val_loss: 8.9201e-04\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7468e-04 - val_loss: 8.9327e-04\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5732e-04 - val_loss: 8.5982e-04\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4349e-04 - val_loss: 8.4542e-04\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2525e-04 - val_loss: 8.2930e-04\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0998e-04 - val_loss: 8.1400e-04\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9639e-04 - val_loss: 8.0015e-04\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8341e-04 - val_loss: 7.8524e-04\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6998e-04 - val_loss: 7.7247e-04\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5671e-04 - val_loss: 7.5877e-04\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4576e-04 - val_loss: 7.5436e-04\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3673e-04 - val_loss: 7.3985e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2242e-04 - val_loss: 7.2635e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1280e-04 - val_loss: 7.2103e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0152e-04 - val_loss: 7.1265e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9224e-04 - val_loss: 6.9700e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8195e-04 - val_loss: 6.8848e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7333e-04 - val_loss: 6.7737e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6609e-04 - val_loss: 6.7499e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5397e-04 - val_loss: 6.5918e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4589e-04 - val_loss: 6.5522e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3825e-04 - val_loss: 6.4743e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2972e-04 - val_loss: 6.3741e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2122e-04 - val_loss: 6.3087e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1434e-04 - val_loss: 6.2225e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0608e-04 - val_loss: 6.1643e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0135e-04 - val_loss: 6.0202e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9276e-04 - val_loss: 5.9812e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8494e-04 - val_loss: 5.8749e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7889e-04 - val_loss: 5.8950e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7147e-04 - val_loss: 5.7748e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6325e-04 - val_loss: 5.6925e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5884e-04 - val_loss: 5.6905e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5171e-04 - val_loss: 5.5659e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4465e-04 - val_loss: 5.5001e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3904e-04 - val_loss: 5.5129e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3231e-04 - val_loss: 5.4668e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2924e-04 - val_loss: 5.4044e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2223e-04 - val_loss: 5.2763e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1782e-04 - val_loss: 5.2579e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1016e-04 - val_loss: 5.1590e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0574e-04 - val_loss: 5.1157e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0297e-04 - val_loss: 5.3394e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9812e-04 - val_loss: 5.0491e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8980e-04 - val_loss: 4.9834e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8698e-04 - val_loss: 5.0471e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8507e-04 - val_loss: 4.9160e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7616e-04 - val_loss: 4.8288e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7151e-04 - val_loss: 4.8263e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6766e-04 - val_loss: 4.7499e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6286e-04 - val_loss: 4.7792e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5885e-04 - val_loss: 4.6542e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5458e-04 - val_loss: 4.6289e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5076e-04 - val_loss: 4.5386e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4539e-04 - val_loss: 4.5240e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4203e-04 - val_loss: 4.4829e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3875e-04 - val_loss: 4.4206e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3605e-04 - val_loss: 4.4904e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3126e-04 - val_loss: 4.5523e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2825e-04 - val_loss: 4.3874e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2328e-04 - val_loss: 4.3274e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1886e-04 - val_loss: 4.2704e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1675e-04 - val_loss: 4.2073e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1158e-04 - val_loss: 4.2866e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0885e-04 - val_loss: 4.1714e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0746e-04 - val_loss: 4.1296e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0357e-04 - val_loss: 4.1346e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0016e-04 - val_loss: 4.1042e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9560e-04 - val_loss: 4.0681e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9974e-04 - val_loss: 4.0070e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9370e-04 - val_loss: 4.0448e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8620e-04 - val_loss: 3.9553e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8405e-04 - val_loss: 3.9596e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8104e-04 - val_loss: 3.8809e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "30\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.2502 - val_loss: 0.1389\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0878 - val_loss: 0.0629\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0526 - val_loss: 0.0488\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0445 - val_loss: 0.0424\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0394 - val_loss: 0.0376\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.0339\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0320 - val_loss: 0.0307\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0280\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0257\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0236\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0218\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0202\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0187\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0174\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0162\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0151\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.0141\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0132\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0124\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0095\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9726e-04 - val_loss: 9.9816e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8344e-04 - val_loss: 9.8198e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6882e-04 - val_loss: 9.6832e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5640e-04 - val_loss: 9.5355e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4272e-04 - val_loss: 9.4223e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2877e-04 - val_loss: 9.2953e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1987e-04 - val_loss: 9.2776e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0754e-04 - val_loss: 9.1233e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9408e-04 - val_loss: 8.9176e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8228e-04 - val_loss: 8.8202e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7203e-04 - val_loss: 8.7325e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6370e-04 - val_loss: 8.6294e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5083e-04 - val_loss: 8.4804e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3689e-04 - val_loss: 8.3697e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2825e-04 - val_loss: 8.2728e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1865e-04 - val_loss: 8.1679e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0802e-04 - val_loss: 8.0547e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9907e-04 - val_loss: 8.0659e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9091e-04 - val_loss: 7.9036e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8022e-04 - val_loss: 7.7738e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7049e-04 - val_loss: 7.7619e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6123e-04 - val_loss: 7.6276e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5243e-04 - val_loss: 7.6986e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4536e-04 - val_loss: 7.4164e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3389e-04 - val_loss: 7.3288e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2743e-04 - val_loss: 7.2588e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2105e-04 - val_loss: 7.3015e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1074e-04 - val_loss: 7.0970e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0328e-04 - val_loss: 7.3233e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0387e-04 - val_loss: 6.9584e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8696e-04 - val_loss: 6.8487e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8014e-04 - val_loss: 6.9010e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7456e-04 - val_loss: 6.7083e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6533e-04 - val_loss: 6.6745e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5834e-04 - val_loss: 6.6161e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5125e-04 - val_loss: 6.4939e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4574e-04 - val_loss: 6.4813e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4013e-04 - val_loss: 6.3678e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3047e-04 - val_loss: 6.3076e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2778e-04 - val_loss: 6.3350e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1993e-04 - val_loss: 6.1943e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1047e-04 - val_loss: 6.0999e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0770e-04 - val_loss: 6.0932e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0095e-04 - val_loss: 6.0176e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9436e-04 - val_loss: 5.9194e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.8842e-04 - val_loss: 5.8711e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8161e-04 - val_loss: 5.8062e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7868e-04 - val_loss: 5.7598e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7169e-04 - val_loss: 5.6811e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6586e-04 - val_loss: 5.6653e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5916e-04 - val_loss: 5.5848e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5434e-04 - val_loss: 5.5308e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4798e-04 - val_loss: 5.4683e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4450e-04 - val_loss: 5.4395e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3944e-04 - val_loss: 5.4047e-04\n",
      "Epoch 150/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3318e-04 - val_loss: 5.3470e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "31\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.5514 - val_loss: 0.3949\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2466 - val_loss: 0.1891\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1288 - val_loss: 0.1226\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0927 - val_loss: 0.0955\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0754 - val_loss: 0.0777\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0633 - val_loss: 0.0650\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0543 - val_loss: 0.0554\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0473 - val_loss: 0.0479\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0418 - val_loss: 0.0421\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0374\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 0.0338\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0307\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0283\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0263\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0245\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0230\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0216\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0204\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0193\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0183\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0174\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0158\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0150\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0144\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0138\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0132\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0127\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0122\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0099\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.0096\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0093\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0090\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0087\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0085\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.0078\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0060\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0057\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0052\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0051\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0050\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "32\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.3261 - val_loss: 0.2010\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1171 - val_loss: 0.0779\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0569 - val_loss: 0.0516\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0426 - val_loss: 0.0408\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.0339\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0291\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 0.0258\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0234\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0216\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0202\n",
      "Epoch 11/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0191\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0181\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0173\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0158\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0152\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0146\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0141\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0136\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.0131\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0126\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0122\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0118\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0100\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0093\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0090\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0087\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0060\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0059\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 93/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9698e-04 - val_loss: 0.0010\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8325e-04 - val_loss: 9.9072e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6893e-04 - val_loss: 9.8383e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4974e-04 - val_loss: 9.6047e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3556e-04 - val_loss: 9.5034e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2450e-04 - val_loss: 9.3111e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0781e-04 - val_loss: 9.1695e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9334e-04 - val_loss: 9.0413e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8285e-04 - val_loss: 8.9712e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6778e-04 - val_loss: 8.7700e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5772e-04 - val_loss: 8.6701e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4445e-04 - val_loss: 8.5097e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3355e-04 - val_loss: 8.4275e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2173e-04 - val_loss: 8.3430e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1064e-04 - val_loss: 8.1864e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0082e-04 - val_loss: 8.1087e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8759e-04 - val_loss: 8.0301e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8103e-04 - val_loss: 7.8773e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6821e-04 - val_loss: 7.7675e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5777e-04 - val_loss: 7.6912e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4999e-04 - val_loss: 7.5614e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.4110e-04 - val_loss: 7.4590e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3045e-04 - val_loss: 7.3564e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2157e-04 - val_loss: 7.2902e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1226e-04 - val_loss: 7.2059e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0502e-04 - val_loss: 7.2122e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9825e-04 - val_loss: 7.0179e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8683e-04 - val_loss: 6.9428e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7989e-04 - val_loss: 6.8956e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7447e-04 - val_loss: 6.7891e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6496e-04 - val_loss: 6.7999e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5874e-04 - val_loss: 6.6716e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4902e-04 - val_loss: 6.5548e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4291e-04 - val_loss: 6.5217e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3682e-04 - val_loss: 6.4849e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3264e-04 - val_loss: 6.5149e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2545e-04 - val_loss: 6.2788e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1580e-04 - val_loss: 6.2465e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1019e-04 - val_loss: 6.1883e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0411e-04 - val_loss: 6.1514e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9742e-04 - val_loss: 6.2258e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9190e-04 - val_loss: 5.9545e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8636e-04 - val_loss: 5.9071e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7829e-04 - val_loss: 5.8636e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7321e-04 - val_loss: 5.8255e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6854e-04 - val_loss: 5.8533e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6458e-04 - val_loss: 5.6927e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5880e-04 - val_loss: 5.6366e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5451e-04 - val_loss: 5.6503e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4780e-04 - val_loss: 5.5421e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4105e-04 - val_loss: 5.6004e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3778e-04 - val_loss: 5.4228e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3292e-04 - val_loss: 5.4067e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2890e-04 - val_loss: 5.3362e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2122e-04 - val_loss: 5.2611e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1764e-04 - val_loss: 5.2656e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "33\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.2559 - val_loss: 0.1514\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0993 - val_loss: 0.0871\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0696 - val_loss: 0.0668\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0557 - val_loss: 0.0536\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0465 - val_loss: 0.0447\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 0.0386\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.0343\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0309\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0281\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0259\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0240\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0222\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0208\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0194\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0182\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0172\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0162\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0154\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0146\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0134\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.0128\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.0123\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0094\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 0.0092\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0089\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0087\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0084 - val_loss: 0.0085\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.0083\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0081\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.0078\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.0076\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0061\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0058\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0057\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 101/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8583e-04 - val_loss: 9.9180e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7103e-04 - val_loss: 9.7556e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5791e-04 - val_loss: 9.6613e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4340e-04 - val_loss: 9.5555e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2957e-04 - val_loss: 9.3346e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1381e-04 - val_loss: 9.2132e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0003e-04 - val_loss: 9.1300e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8769e-04 - val_loss: 9.2171e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7711e-04 - val_loss: 8.8126e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6071e-04 - val_loss: 8.6559e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4987e-04 - val_loss: 8.6273e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3700e-04 - val_loss: 8.4369e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2476e-04 - val_loss: 8.3177e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1465e-04 - val_loss: 8.2885e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0293e-04 - val_loss: 8.1019e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9630e-04 - val_loss: 8.0410e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7942e-04 - val_loss: 7.9204e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6963e-04 - val_loss: 7.8053e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5870e-04 - val_loss: 7.6640e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "34\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.5942 - val_loss: 0.4750\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3084 - val_loss: 0.2226\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1137 - val_loss: 0.0732\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0491 - val_loss: 0.0475\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 0.0404\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.0356\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0320\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0291\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0268\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0250\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0221\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0210\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0200\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0191\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0183\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0175\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0161\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0155\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0137\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0132\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0127\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0122\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0097\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0094\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0090\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0087\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.0079\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0060\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0057\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0056\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0053\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0050\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0049\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0048\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0047\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9290e-04 - val_loss: 0.0010\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7783e-04 - val_loss: 9.8905e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6499e-04 - val_loss: 9.7327e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5170e-04 - val_loss: 9.6476e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3892e-04 - val_loss: 9.5165e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2773e-04 - val_loss: 9.4540e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1515e-04 - val_loss: 9.2148e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0273e-04 - val_loss: 9.0967e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8978e-04 - val_loss: 8.9800e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7831e-04 - val_loss: 8.8821e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6647e-04 - val_loss: 8.7680e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5645e-04 - val_loss: 8.6423e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4519e-04 - val_loss: 8.5455e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3529e-04 - val_loss: 8.4389e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2321e-04 - val_loss: 8.3216e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1720e-04 - val_loss: 8.3024e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0504e-04 - val_loss: 8.1663e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9577e-04 - val_loss: 8.0175e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8438e-04 - val_loss: 7.9225e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7484e-04 - val_loss: 7.8513e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6658e-04 - val_loss: 7.7177e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5672e-04 - val_loss: 7.6507e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4792e-04 - val_loss: 7.5304e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3957e-04 - val_loss: 7.5155e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3105e-04 - val_loss: 7.3482e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2243e-04 - val_loss: 7.2769e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1505e-04 - val_loss: 7.1895e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0599e-04 - val_loss: 7.0954e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9787e-04 - val_loss: 7.0169e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9039e-04 - val_loss: 6.9455e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8237e-04 - val_loss: 6.8646e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7592e-04 - val_loss: 6.7803e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "35\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.3341 - val_loss: 0.2633\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1675 - val_loss: 0.1227\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0775 - val_loss: 0.0646\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0481 - val_loss: 0.0443\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 0.0346\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0291\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0257\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0232\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0212\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0195\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0180\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0167\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0155\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0144\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0134\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.0125\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0117\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0090\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9949e-04 - val_loss: 9.9855e-04\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7380e-04 - val_loss: 9.7101e-04\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4729e-04 - val_loss: 9.4585e-04\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2429e-04 - val_loss: 9.2205e-04\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0246e-04 - val_loss: 9.0698e-04\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8059e-04 - val_loss: 8.7864e-04\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6100e-04 - val_loss: 8.6202e-04\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4336e-04 - val_loss: 8.4290e-04\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2503e-04 - val_loss: 8.3167e-04\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0755e-04 - val_loss: 8.0711e-04\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9125e-04 - val_loss: 7.9221e-04\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7506e-04 - val_loss: 7.7860e-04\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6212e-04 - val_loss: 7.6911e-04\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4800e-04 - val_loss: 7.4785e-04\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3519e-04 - val_loss: 7.3470e-04\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2299e-04 - val_loss: 7.2400e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0749e-04 - val_loss: 7.1240e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9651e-04 - val_loss: 7.0195e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8466e-04 - val_loss: 6.8727e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7361e-04 - val_loss: 6.7550e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6359e-04 - val_loss: 6.6693e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5336e-04 - val_loss: 6.5601e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4296e-04 - val_loss: 6.4501e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3383e-04 - val_loss: 6.3791e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2359e-04 - val_loss: 6.2577e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1564e-04 - val_loss: 6.1679e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0500e-04 - val_loss: 6.1085e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9743e-04 - val_loss: 6.0060e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9172e-04 - val_loss: 5.9285e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7960e-04 - val_loss: 5.8466e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7200e-04 - val_loss: 5.7947e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6536e-04 - val_loss: 5.7194e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5761e-04 - val_loss: 5.6989e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5056e-04 - val_loss: 5.5562e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4396e-04 - val_loss: 5.4894e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4031e-04 - val_loss: 5.4866e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2989e-04 - val_loss: 5.3633e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2261e-04 - val_loss: 5.2791e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1594e-04 - val_loss: 5.2163e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1049e-04 - val_loss: 5.1746e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0499e-04 - val_loss: 5.1162e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9894e-04 - val_loss: 5.0186e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9404e-04 - val_loss: 4.9673e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8738e-04 - val_loss: 4.9037e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8227e-04 - val_loss: 4.8745e-04\n",
      "Epoch 118/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7555e-04 - val_loss: 4.8121e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7266e-04 - val_loss: 4.7526e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6862e-04 - val_loss: 4.6932e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6009e-04 - val_loss: 4.6381e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5727e-04 - val_loss: 4.6153e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5124e-04 - val_loss: 4.5426e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4756e-04 - val_loss: 4.5437e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4562e-04 - val_loss: 4.4689e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3794e-04 - val_loss: 4.4775e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3396e-04 - val_loss: 4.4256e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2986e-04 - val_loss: 4.3371e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2508e-04 - val_loss: 4.3178e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2212e-04 - val_loss: 4.2365e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1622e-04 - val_loss: 4.2269e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1389e-04 - val_loss: 4.1701e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0967e-04 - val_loss: 4.1321e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0479e-04 - val_loss: 4.0824e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0105e-04 - val_loss: 4.0538e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9679e-04 - val_loss: 4.0129e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9413e-04 - val_loss: 3.9718e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8991e-04 - val_loss: 3.9524e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8729e-04 - val_loss: 3.9029e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8373e-04 - val_loss: 3.9334e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8003e-04 - val_loss: 3.8594e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7915e-04 - val_loss: 3.8675e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7250e-04 - val_loss: 3.8133e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7155e-04 - val_loss: 3.7396e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6904e-04 - val_loss: 3.7010e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6963e-04 - val_loss: 3.7054e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6084e-04 - val_loss: 3.6566e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5829e-04 - val_loss: 3.6481e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5476e-04 - val_loss: 3.6008e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5447e-04 - val_loss: 3.5791e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "36\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.3778 - val_loss: 0.2914\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1935 - val_loss: 0.1483\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0933 - val_loss: 0.0765\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0529 - val_loss: 0.0508\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0370\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0288\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0236\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0203\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0181\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0154\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.0145\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0137\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0130\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.0122\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0113\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0064\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9842e-04 - val_loss: 0.0010\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7845e-04 - val_loss: 9.9945e-04\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6225e-04 - val_loss: 9.7996e-04\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4411e-04 - val_loss: 9.6232e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2604e-04 - val_loss: 9.4936e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0972e-04 - val_loss: 9.2860e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9319e-04 - val_loss: 9.1027e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7942e-04 - val_loss: 9.0176e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6358e-04 - val_loss: 8.8663e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4896e-04 - val_loss: 8.6668e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3375e-04 - val_loss: 8.5244e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2059e-04 - val_loss: 8.3964e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0740e-04 - val_loss: 8.2706e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9400e-04 - val_loss: 8.2110e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8363e-04 - val_loss: 7.9979e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7025e-04 - val_loss: 7.9006e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5754e-04 - val_loss: 7.7616e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4699e-04 - val_loss: 7.9385e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3809e-04 - val_loss: 7.5379e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2431e-04 - val_loss: 7.4464e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1524e-04 - val_loss: 7.3287e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0410e-04 - val_loss: 7.2611e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9399e-04 - val_loss: 7.1551e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8330e-04 - val_loss: 7.0864e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7477e-04 - val_loss: 6.9693e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6514e-04 - val_loss: 6.8729e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5612e-04 - val_loss: 6.7218e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4829e-04 - val_loss: 6.6676e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3944e-04 - val_loss: 6.5873e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3275e-04 - val_loss: 6.4650e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2567e-04 - val_loss: 6.4316e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1704e-04 - val_loss: 6.3751e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0837e-04 - val_loss: 6.2332e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0049e-04 - val_loss: 6.1595e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9402e-04 - val_loss: 6.1407e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8886e-04 - val_loss: 6.0413e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8011e-04 - val_loss: 6.0536e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7319e-04 - val_loss: 5.9369e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6675e-04 - val_loss: 5.9256e-04\n",
      "Epoch 124/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6190e-04 - val_loss: 5.7944e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5624e-04 - val_loss: 5.7729e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5129e-04 - val_loss: 5.6716e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4285e-04 - val_loss: 5.5888e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3726e-04 - val_loss: 5.5889e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3239e-04 - val_loss: 5.5700e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2741e-04 - val_loss: 5.4247e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2260e-04 - val_loss: 5.3520e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1478e-04 - val_loss: 5.2996e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1061e-04 - val_loss: 5.2399e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0621e-04 - val_loss: 5.2145e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0050e-04 - val_loss: 5.1537e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9595e-04 - val_loss: 5.0762e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9064e-04 - val_loss: 5.0136e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8530e-04 - val_loss: 5.0070e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8169e-04 - val_loss: 4.9220e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7710e-04 - val_loss: 4.9185e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7872e-04 - val_loss: 4.9178e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6971e-04 - val_loss: 4.8031e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6403e-04 - val_loss: 4.7727e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6021e-04 - val_loss: 4.7733e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5706e-04 - val_loss: 4.6664e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5238e-04 - val_loss: 4.6177e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4826e-04 - val_loss: 4.5903e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4486e-04 - val_loss: 4.5364e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4022e-04 - val_loss: 4.5590e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3725e-04 - val_loss: 4.4905e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "37\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.9768 - val_loss: 0.7496\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.4859 - val_loss: 0.3300\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1708 - val_loss: 0.0826\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0513 - val_loss: 0.0431\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0383 - val_loss: 0.0371\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.0333\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0306\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0286\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0272\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0259\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0249\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0240\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0232\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0223\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0215\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0208\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0201\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0193\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0186\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0179\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0173\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0166\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0160\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0153\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0147\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0141\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0136\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.0130\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.0125\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0120\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0115\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.0101\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0096\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0092\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0066\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0051\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8810e-04 - val_loss: 9.9693e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7383e-04 - val_loss: 9.8291e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5873e-04 - val_loss: 9.7541e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4673e-04 - val_loss: 9.5616e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3049e-04 - val_loss: 9.4081e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1766e-04 - val_loss: 9.2325e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0513e-04 - val_loss: 9.0938e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9118e-04 - val_loss: 9.0026e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8034e-04 - val_loss: 8.9425e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6897e-04 - val_loss: 8.7851e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5569e-04 - val_loss: 8.6309e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4445e-04 - val_loss: 8.5465e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3384e-04 - val_loss: 8.4114e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2470e-04 - val_loss: 8.3180e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1484e-04 - val_loss: 8.2664e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0157e-04 - val_loss: 8.1117e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9274e-04 - val_loss: 8.1048e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8258e-04 - val_loss: 7.9167e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7286e-04 - val_loss: 7.8556e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6458e-04 - val_loss: 7.7014e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5369e-04 - val_loss: 7.6551e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4696e-04 - val_loss: 7.6031e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3774e-04 - val_loss: 7.4542e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2835e-04 - val_loss: 7.3844e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1941e-04 - val_loss: 7.2851e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1262e-04 - val_loss: 7.2400e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0313e-04 - val_loss: 7.1147e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9777e-04 - val_loss: 7.0615e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8925e-04 - val_loss: 6.9668e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8137e-04 - val_loss: 6.8826e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7283e-04 - val_loss: 6.8460e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6732e-04 - val_loss: 6.8054e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5871e-04 - val_loss: 6.6599e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5287e-04 - val_loss: 6.5896e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4491e-04 - val_loss: 6.5625e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3829e-04 - val_loss: 6.4701e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3218e-04 - val_loss: 6.3841e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2551e-04 - val_loss: 6.3765e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1932e-04 - val_loss: 6.2496e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1259e-04 - val_loss: 6.2050e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0780e-04 - val_loss: 6.1949e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0153e-04 - val_loss: 6.0776e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9474e-04 - val_loss: 6.0236e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8764e-04 - val_loss: 5.9557e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8260e-04 - val_loss: 5.9163e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7793e-04 - val_loss: 5.8377e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7160e-04 - val_loss: 5.7912e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6921e-04 - val_loss: 5.7845e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6211e-04 - val_loss: 5.6826e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5490e-04 - val_loss: 5.6563e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4986e-04 - val_loss: 5.5865e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4520e-04 - val_loss: 5.5229e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4007e-04 - val_loss: 5.4978e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3561e-04 - val_loss: 5.5113e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3245e-04 - val_loss: 5.4087e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2493e-04 - val_loss: 5.3625e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "38\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.4268 - val_loss: 0.2728\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1716 - val_loss: 0.1269\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0946 - val_loss: 0.0878\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0688 - val_loss: 0.0654\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0530 - val_loss: 0.0502\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.0407\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 0.0348\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0311\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0284\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0263\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0246\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0230\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0216\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0202\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0191\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0180\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0169\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0160\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0151\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.0143\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0135\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0128\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.0121\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0115\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.0095\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0086\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.8841e-04 - val_loss: 9.9483e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.6960e-04 - val_loss: 9.7661e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.5442e-04 - val_loss: 9.6777e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.3813e-04 - val_loss: 9.4973e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.2422e-04 - val_loss: 9.3173e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.0882e-04 - val_loss: 9.2429e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.9556e-04 - val_loss: 9.0189e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.8099e-04 - val_loss: 8.9402e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.6824e-04 - val_loss: 8.7700e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.5378e-04 - val_loss: 8.6169e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.4745e-04 - val_loss: 8.5121e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.3242e-04 - val_loss: 8.4917e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.1938e-04 - val_loss: 8.2378e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.0664e-04 - val_loss: 8.0965e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.9375e-04 - val_loss: 8.0805e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.8588e-04 - val_loss: 7.8897e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.7113e-04 - val_loss: 7.8396e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.6142e-04 - val_loss: 7.6729e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.4933e-04 - val_loss: 7.5789e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.3994e-04 - val_loss: 7.4675e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.3408e-04 - val_loss: 7.4247e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.1984e-04 - val_loss: 7.2849e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.0957e-04 - val_loss: 7.2011e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.0126e-04 - val_loss: 7.1486e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.9053e-04 - val_loss: 7.0024e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.8232e-04 - val_loss: 6.9136e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.7295e-04 - val_loss: 6.8052e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.6370e-04 - val_loss: 6.7087e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.5670e-04 - val_loss: 6.6092e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.4725e-04 - val_loss: 6.5501e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.3707e-04 - val_loss: 6.5346e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.3018e-04 - val_loss: 6.4443e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.2185e-04 - val_loss: 6.3198e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.1439e-04 - val_loss: 6.2165e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.0566e-04 - val_loss: 6.1663e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.9991e-04 - val_loss: 6.1366e-04\n",
      "81/81 [==============================] - 0s 663us/step\n",
      "39\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 4ms/step - loss: 0.7657 - val_loss: 0.6078\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.4629 - val_loss: 0.3462\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2592 - val_loss: 0.2038\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1624 - val_loss: 0.1519\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1214 - val_loss: 0.1221\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0961 - val_loss: 0.0996\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0780 - val_loss: 0.0820\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0645 - val_loss: 0.0682\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0542 - val_loss: 0.0569\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0444 - val_loss: 0.0439\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 0.0344\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0286\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0254\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0236\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0224\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0213\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0204\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0195\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0187\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0179\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.0170\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0161\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0153\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0146\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0139\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.0126\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0119\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0113\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0078\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 69/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8244e-04 - val_loss: 0.0010\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6229e-04 - val_loss: 0.0010\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.4299e-04 - val_loss: 9.7466e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9.2818e-04 - val_loss: 9.4455e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0933e-04 - val_loss: 9.3041e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.9020e-04 - val_loss: 9.3534e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7568e-04 - val_loss: 9.2480e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5711e-04 - val_loss: 8.9193e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4961e-04 - val_loss: 8.8986e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2799e-04 - val_loss: 8.5644e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8.1621e-04 - val_loss: 8.4289e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0100e-04 - val_loss: 8.6735e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9213e-04 - val_loss: 8.3008e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7.7863e-04 - val_loss: 8.1172e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6532e-04 - val_loss: 8.0456e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5202e-04 - val_loss: 7.8431e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4061e-04 - val_loss: 7.6652e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2888e-04 - val_loss: 7.5347e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1921e-04 - val_loss: 7.7593e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0846e-04 - val_loss: 7.5811e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9837e-04 - val_loss: 7.3233e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9197e-04 - val_loss: 7.0919e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.7776e-04 - val_loss: 7.1842e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6979e-04 - val_loss: 7.0373e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5904e-04 - val_loss: 7.1732e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5595e-04 - val_loss: 6.9364e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4188e-04 - val_loss: 6.8875e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3867e-04 - val_loss: 6.6335e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2816e-04 - val_loss: 6.6092e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2011e-04 - val_loss: 6.3876e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6.1184e-04 - val_loss: 6.4005e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0516e-04 - val_loss: 6.3254e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9930e-04 - val_loss: 6.2852e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9344e-04 - val_loss: 6.2087e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8191e-04 - val_loss: 6.3734e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7733e-04 - val_loss: 6.0691e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6866e-04 - val_loss: 5.9924e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6459e-04 - val_loss: 6.2734e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6028e-04 - val_loss: 5.8791e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5004e-04 - val_loss: 6.0411e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4605e-04 - val_loss: 5.8443e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3977e-04 - val_loss: 5.7210e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3304e-04 - val_loss: 5.7422e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3031e-04 - val_loss: 5.6040e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5.2338e-04 - val_loss: 5.5419e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1820e-04 - val_loss: 5.5724e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1121e-04 - val_loss: 5.3694e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0757e-04 - val_loss: 5.3023e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0262e-04 - val_loss: 5.4294e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9720e-04 - val_loss: 5.2463e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9388e-04 - val_loss: 5.1749e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8837e-04 - val_loss: 5.1770e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8419e-04 - val_loss: 5.1977e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7653e-04 - val_loss: 4.9960e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7058e-04 - val_loss: 5.3403e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6848e-04 - val_loss: 4.9524e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6614e-04 - val_loss: 5.0468e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6218e-04 - val_loss: 4.9730e-04\n",
      "Epoch 146/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5822e-04 - val_loss: 4.8628e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5381e-04 - val_loss: 4.8577e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5121e-04 - val_loss: 4.6943e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.4553e-04 - val_loss: 4.7824e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4.4097e-04 - val_loss: 4.7476e-04\n",
      "81/81 [==============================] - 0s 763us/step\n",
      "40\n",
      "Best regularization strength: 0.01\n",
      "Epoch 1/150\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 0.3992 - val_loss: 0.2593\n",
      "Epoch 2/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1541 - val_loss: 0.1038\n",
      "Epoch 3/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0739 - val_loss: 0.0669\n",
      "Epoch 4/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0525 - val_loss: 0.0484\n",
      "Epoch 5/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0395 - val_loss: 0.0366\n",
      "Epoch 6/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0304\n",
      "Epoch 7/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0266\n",
      "Epoch 8/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0239\n",
      "Epoch 9/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0218\n",
      "Epoch 10/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0200\n",
      "Epoch 11/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0184\n",
      "Epoch 12/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0171\n",
      "Epoch 13/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0159\n",
      "Epoch 14/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0148\n",
      "Epoch 15/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0138\n",
      "Epoch 16/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      "Epoch 17/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0122\n",
      "Epoch 18/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.0115\n",
      "Epoch 19/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 20/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 21/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 22/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 0.0093\n",
      "Epoch 23/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 24/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 25/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 26/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 28/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 29/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 30/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 31/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 32/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 33/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 34/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 35/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 36/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 37/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 38/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 39/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 40/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 41/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 42/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 43/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 44/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 45/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 46/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 47/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 48/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 49/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 50/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 51/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 52/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 53/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 54/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 55/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 56/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 57/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 58/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 59/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 60/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 61/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 62/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 63/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 64/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 65/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 66/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 67/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 68/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 69/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 70/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 71/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 72/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 73/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 74/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 75/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 76/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 78/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 79/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 80/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8598e-04 - val_loss: 0.0010\n",
      "Epoch 81/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6832e-04 - val_loss: 9.7918e-04\n",
      "Epoch 82/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5075e-04 - val_loss: 9.6426e-04\n",
      "Epoch 83/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3740e-04 - val_loss: 9.4341e-04\n",
      "Epoch 84/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1915e-04 - val_loss: 9.2821e-04\n",
      "Epoch 85/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0834e-04 - val_loss: 9.2131e-04\n",
      "Epoch 86/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9070e-04 - val_loss: 9.0224e-04\n",
      "Epoch 87/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7955e-04 - val_loss: 8.8413e-04\n",
      "Epoch 88/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6341e-04 - val_loss: 8.7462e-04\n",
      "Epoch 89/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5156e-04 - val_loss: 8.6035e-04\n",
      "Epoch 90/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3832e-04 - val_loss: 8.4698e-04\n",
      "Epoch 91/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2701e-04 - val_loss: 8.3664e-04\n",
      "Epoch 92/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1701e-04 - val_loss: 8.2883e-04\n",
      "Epoch 93/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0399e-04 - val_loss: 8.1243e-04\n",
      "Epoch 94/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9307e-04 - val_loss: 8.0055e-04\n",
      "Epoch 95/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8446e-04 - val_loss: 7.9080e-04\n",
      "Epoch 96/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7329e-04 - val_loss: 7.7934e-04\n",
      "Epoch 97/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6243e-04 - val_loss: 7.6893e-04\n",
      "Epoch 98/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5433e-04 - val_loss: 7.6572e-04\n",
      "Epoch 99/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4318e-04 - val_loss: 7.5141e-04\n",
      "Epoch 100/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3642e-04 - val_loss: 7.4355e-04\n",
      "Epoch 101/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2611e-04 - val_loss: 7.3241e-04\n",
      "Epoch 102/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1586e-04 - val_loss: 7.2379e-04\n",
      "Epoch 103/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0927e-04 - val_loss: 7.1157e-04\n",
      "Epoch 104/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0027e-04 - val_loss: 7.0473e-04\n",
      "Epoch 105/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.9368e-04 - val_loss: 6.9682e-04\n",
      "Epoch 106/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8418e-04 - val_loss: 6.9407e-04\n",
      "Epoch 107/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7596e-04 - val_loss: 6.8582e-04\n",
      "Epoch 108/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7253e-04 - val_loss: 6.7480e-04\n",
      "Epoch 109/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.6231e-04 - val_loss: 6.6653e-04\n",
      "Epoch 110/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5484e-04 - val_loss: 6.6031e-04\n",
      "Epoch 111/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4778e-04 - val_loss: 6.5246e-04\n",
      "Epoch 112/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4286e-04 - val_loss: 6.4722e-04\n",
      "Epoch 113/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3439e-04 - val_loss: 6.3856e-04\n",
      "Epoch 114/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2706e-04 - val_loss: 6.3279e-04\n",
      "Epoch 115/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2208e-04 - val_loss: 6.2572e-04\n",
      "Epoch 116/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1391e-04 - val_loss: 6.2503e-04\n",
      "Epoch 117/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1077e-04 - val_loss: 6.1917e-04\n",
      "Epoch 118/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0288e-04 - val_loss: 6.0985e-04\n",
      "Epoch 119/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9910e-04 - val_loss: 6.0018e-04\n",
      "Epoch 120/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9014e-04 - val_loss: 5.9424e-04\n",
      "Epoch 121/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8584e-04 - val_loss: 5.9133e-04\n",
      "Epoch 122/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7901e-04 - val_loss: 5.8549e-04\n",
      "Epoch 123/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7523e-04 - val_loss: 5.8191e-04\n",
      "Epoch 124/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6843e-04 - val_loss: 5.7191e-04\n",
      "Epoch 125/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6378e-04 - val_loss: 5.7318e-04\n",
      "Epoch 126/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5891e-04 - val_loss: 5.6387e-04\n",
      "Epoch 127/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5324e-04 - val_loss: 5.5680e-04\n",
      "Epoch 128/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4652e-04 - val_loss: 5.5207e-04\n",
      "Epoch 129/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4340e-04 - val_loss: 5.4932e-04\n",
      "Epoch 130/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3878e-04 - val_loss: 5.4081e-04\n",
      "Epoch 131/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3296e-04 - val_loss: 5.3482e-04\n",
      "Epoch 132/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2837e-04 - val_loss: 5.3217e-04\n",
      "Epoch 133/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2325e-04 - val_loss: 5.2751e-04\n",
      "Epoch 134/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1830e-04 - val_loss: 5.2057e-04\n",
      "Epoch 135/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1321e-04 - val_loss: 5.1917e-04\n",
      "Epoch 136/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0964e-04 - val_loss: 5.1415e-04\n",
      "Epoch 137/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0445e-04 - val_loss: 5.0943e-04\n",
      "Epoch 138/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0083e-04 - val_loss: 5.0783e-04\n",
      "Epoch 139/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9643e-04 - val_loss: 5.0324e-04\n",
      "Epoch 140/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9320e-04 - val_loss: 4.9587e-04\n",
      "Epoch 141/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8830e-04 - val_loss: 4.9054e-04\n",
      "Epoch 142/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8356e-04 - val_loss: 4.8992e-04\n",
      "Epoch 143/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8023e-04 - val_loss: 4.8868e-04\n",
      "Epoch 144/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7645e-04 - val_loss: 4.8068e-04\n",
      "Epoch 145/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7169e-04 - val_loss: 4.7467e-04\n",
      "Epoch 146/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6815e-04 - val_loss: 4.6890e-04\n",
      "Epoch 147/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6277e-04 - val_loss: 4.6723e-04\n",
      "Epoch 148/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6061e-04 - val_loss: 4.6646e-04\n",
      "Epoch 149/150\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.5779e-04 - val_loss: 4.5950e-04\n",
      "Epoch 150/150\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5288e-04 - val_loss: 4.5418e-04\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "0.0059000000000000025 0.004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "threshold = 9\n",
    "best_error = float('inf')\n",
    "normal_error = 0\n",
    "extreme_error = 0\n",
    "import numpy as np\n",
    "count=0\n",
    "flag=False\n",
    "\n",
    "\n",
    "for normal_weight in np.arange(0.005,0.006,0.0001):\n",
    "    for extreme_weight in np.arange(0.001,0.005, 0.001):\n",
    "        # Load the data\n",
    "        data = pd.read_csv('rainfall_data.csv')\n",
    "        count+=1\n",
    "        print(count)\n",
    "        # Split the data into normal and extreme rainfall\n",
    "        normal_data = data[data['rainfall'] <= 50]\n",
    "        extreme_data = data[data['rainfall'] > 50]\n",
    "        def weighted_loss(normal_weight, extreme_weight):\n",
    "            def loss(y_true, y_pred):\n",
    "                normal_loss = K.mean(K.square(y_true - y_pred))\n",
    "                extreme_loss = K.mean(K.square(y_true - y_pred) * K.cast(y_true > 50, 'float32'))\n",
    "                return K.mean(normal_weight * normal_loss + extreme_weight * extreme_loss)\n",
    "            return loss\n",
    "        # train your model with the current combination of weights\n",
    "        # Define a function to find the best regularization strength\n",
    "        def find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size):\n",
    "            best_reg_strength = None\n",
    "            best_loss = float('inf')\n",
    "            for reg_strength in reg_strengths:\n",
    "                # Define the autoencoder architecture\n",
    "                input_layer = Input(shape=(3,))\n",
    "                encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=reg_strength))(input_layer)\n",
    "                decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "                # Create the autoencoder\n",
    "                autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "                # Compile the autoencoder with the weighted loss function\n",
    "                autoencoder.compile(optimizer='adam', loss=weighted_loss(normal_weight, extreme_weight))\n",
    "\n",
    "                # Train the model\n",
    "                history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "\n",
    "                # Calculate the validation loss\n",
    "                val_loss = np.mean(history.history['val_loss'])\n",
    "\n",
    "                # Update the best regularization strength and loss\n",
    "                if val_loss < best_loss:\n",
    "                    best_reg_strength = reg_strength\n",
    "                    best_loss = val_loss\n",
    "\n",
    "            print('Best regularization strength:', best_reg_strength)\n",
    "            return best_reg_strength\n",
    "\n",
    "        # Find the best regularization strength\n",
    "        reg_strengths = [0.01, 0.1, 1, 10]\n",
    "        num_epochs = 150\n",
    "        batch_size = 32\n",
    "        train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "        best_reg_strength = find_best_reg_strength(train_data, reg_strengths, num_epochs, batch_size)\n",
    "\n",
    "        # Define the autoencoder architecture with the best regularization strength\n",
    "        input_layer = Input(shape=(3,))\n",
    "        encoded = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=best_reg_strength))(input_layer)\n",
    "        decoded = Dense(3, activation=None)(encoded)\n",
    "\n",
    "        # Create the autoencoder\n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "        # Compile the autoencoder with the weighted loss function\n",
    "        autoencoder.compile(optimizer='adam', loss=weighted_loss(normal_weight, extreme_weight))\n",
    "\n",
    "        # Train the model with the majority class\n",
    "        train_data = normal_data[['windspeed', 'tpw', 'rainfall']].values\n",
    "        history = autoencoder.fit(train_data, train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "        # Use the trained autoencoder to predict the rainfall values for all data points\n",
    "        test_data = data[['windspeed', 'tpw', 'rainfall']].values\n",
    "        predicted_data = autoencoder.predict(test_data)\n",
    "        data['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "        # Calculate the error between the predicted and actual rainfall values\n",
    "        data['error'] = np.abs(data['predicted_rainfall'] - data['rainfall'])\n",
    "\n",
    "        # Classify the rainfall data into normal and extreme based on the error threshold\n",
    "        # Define a range of threshold values to try\n",
    "        def best_threshold():\n",
    "            thresholds = np.arange(0.5, 10.0, 0.1)\n",
    "\n",
    "            # Initialize variables to store the best threshold and the lowest error\n",
    "            best_threshold = 0.0\n",
    "            lowest_error = float('inf')\n",
    "\n",
    "            # Loop over the threshold values and calculate the error for each\n",
    "            for threshold in thresholds:\n",
    "                # Classify the rainfall data into normal and extreme based on the threshold\n",
    "                pre_df['rainfall_class'] = np.where(data['error'] > threshold, 'extreme', 'normal')\n",
    "\n",
    "                # Calculate the accuracy of the classification\n",
    "                accuracy = sum(pre_df['rainfall_class'] == pre_df['actuall_rainfall_class']) / len(data)\n",
    "\n",
    "                # Calculate the error of the classification\n",
    "                error = 1 - accuracy\n",
    "\n",
    "                # Update the best threshold and lowest error if necessary\n",
    "                if error < lowest_error:\n",
    "                    best_threshold = threshold\n",
    "                    lowest_error = error\n",
    "\n",
    "            return threshold\n",
    "        \n",
    "        data['rainfall_class'] = np.where(data['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "        # Define the actual rainfall class based on the threshold of 11\n",
    "        data['actual_rainfall_class'] = np.where(data['rainfall'] > 50, 'Extreme', 'Normal')\n",
    "\n",
    "        result_list = []\n",
    "        for index, row in data.iterrows():\n",
    "            result_list.append(row['rainfall_class'] == row['actual_rainfall_class'])\n",
    "\n",
    "        if all(result_list):\n",
    "            print(\"found it\")\n",
    "            flag=True\n",
    "            break\n",
    "    if flag:\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print(normal_weight, extreme_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4fbab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rainfall_class: Normal, Actual Rainfall Class: Extreme\n",
      "rainfall_class: Normal, Actual Rainfall Class: Extreme\n"
     ]
    }
   ],
   "source": [
    "for index, row in data.iterrows():\n",
    "    if row['rainfall_class'] != row['actual_rainfall_class']:\n",
    "        print(f\"rainfall_class: {row['rainfall_class']}, Actual Rainfall Class: {row['actual_rainfall_class']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e747d9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for normal_weight in np.arange(1, 10, 1):\n",
    "    for extreme_weight in np.arange(0.01, 0.1, 0.01):\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1b250f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>predicted_rainfall</th>\n",
       "      <th>error</th>\n",
       "      <th>rainfall_class</th>\n",
       "      <th>actual_rainfall_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/1/2000</td>\n",
       "      <td>12.245595</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>4.688589</td>\n",
       "      <td>4.689952</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/2/2000</td>\n",
       "      <td>12.825491</td>\n",
       "      <td>0.044874</td>\n",
       "      <td>11.330589</td>\n",
       "      <td>11.333991</td>\n",
       "      <td>0.003402</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/3/2000</td>\n",
       "      <td>12.921664</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>6.894713</td>\n",
       "      <td>6.897418</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/4/2000</td>\n",
       "      <td>15.149001</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>4.613324</td>\n",
       "      <td>4.614687</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/5/2000</td>\n",
       "      <td>18.495907</td>\n",
       "      <td>0.139491</td>\n",
       "      <td>8.111635</td>\n",
       "      <td>8.113751</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>9/26/2020</td>\n",
       "      <td>5.577215</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>4.467977</td>\n",
       "      <td>4.469275</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>9/27/2020</td>\n",
       "      <td>5.184293</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>1.572182</td>\n",
       "      <td>1.572849</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>9/28/2020</td>\n",
       "      <td>4.469007</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>1.819019</td>\n",
       "      <td>1.819735</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>9/29/2020</td>\n",
       "      <td>4.259090</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>2.198017</td>\n",
       "      <td>2.198763</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>9/30/2020</td>\n",
       "      <td>5.513838</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>12.713634</td>\n",
       "      <td>12.716312</td>\n",
       "      <td>0.002679</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2562 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall  predicted_rainfall     error  \\\n",
       "0      6/1/2000  12.245595  0.033285   4.688589            4.689952  0.001364   \n",
       "1      6/2/2000  12.825491  0.044874  11.330589           11.333991  0.003402   \n",
       "2      6/3/2000  12.921664  0.010243   6.894713            6.897418  0.002705   \n",
       "3      6/4/2000  15.149001  0.036881   4.613324            4.614687  0.001363   \n",
       "4      6/5/2000  18.495907  0.139491   8.111635            8.113751  0.002116   \n",
       "...         ...        ...       ...        ...                 ...       ...   \n",
       "2557  9/26/2020   5.577215  0.009963   4.467977            4.469275  0.001298   \n",
       "2558  9/27/2020   5.184293  0.002341   1.572182            1.572849  0.000668   \n",
       "2559  9/28/2020   4.469007  0.000867   1.819019            1.819735  0.000716   \n",
       "2560  9/29/2020   4.259090  0.001416   2.198017            2.198763  0.000746   \n",
       "2561  9/30/2020   5.513838  0.002474  12.713634           12.716312  0.002679   \n",
       "\n",
       "     rainfall_class actual_rainfall_class  \n",
       "0            Normal                Normal  \n",
       "1            Normal                Normal  \n",
       "2            Normal                Normal  \n",
       "3            Normal                Normal  \n",
       "4            Normal                Normal  \n",
       "...             ...                   ...  \n",
       "2557         Normal                Normal  \n",
       "2558         Normal                Normal  \n",
       "2559         Normal                Normal  \n",
       "2560         Normal                Normal  \n",
       "2561         Normal                Normal  \n",
       "\n",
       "[2562 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1f31132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not all rows have matching rainfall class and actual rainfall class\n"
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "for index, row in data.iterrows():\n",
    "    result_list.append(row['rainfall_class'] == row['actual_rainfall_class'])\n",
    "\n",
    "if all(result_list):\n",
    "    print(\"All rows have matching rainfall class and actual rainfall class\")\n",
    "else:\n",
    "    print(\"Not all rows have matching rainfall class and actual rainfall class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a695e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"rainfall_data.csv\")\n",
    "\n",
    "# Equal frequency discretization for windspeed and tpw\n",
    "num_bins =  6# number of intervals\n",
    "data['windspeed_category'] = pd.qcut(data['windspeed'], q=num_bins, labels=[\"very_Low\",\"low\", \"low_Medium\",\"medium\", \"meduim_High\",\"High\"])\n",
    "data['tpw_category'] = pd.qcut(data['tpw'], q=num_bins, labels=[\"very_Low\",\"low\", \"low_Medium\",\"medium\", \"meduim_High\",\"High\"])\n",
    "\n",
    "# Fixed threshold discretization for rainfall\n",
    "rain_threshold = 30 # threshold value for rainfall\n",
    "data['rainfall_category'] = ['Normal' if rainfall < rain_threshold else 'Extreme' for rainfall in data['rainfall']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15263878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>windspeed_category</th>\n",
       "      <th>tpw_category</th>\n",
       "      <th>rainfall_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/1/2000</td>\n",
       "      <td>12.245595</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>4.688589</td>\n",
       "      <td>medium</td>\n",
       "      <td>meduim_High</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/2/2000</td>\n",
       "      <td>12.825491</td>\n",
       "      <td>0.044874</td>\n",
       "      <td>11.330589</td>\n",
       "      <td>medium</td>\n",
       "      <td>meduim_High</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/3/2000</td>\n",
       "      <td>12.921664</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>6.894713</td>\n",
       "      <td>medium</td>\n",
       "      <td>low_Medium</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/4/2000</td>\n",
       "      <td>15.149001</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>4.613324</td>\n",
       "      <td>meduim_High</td>\n",
       "      <td>meduim_High</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/5/2000</td>\n",
       "      <td>18.495907</td>\n",
       "      <td>0.139491</td>\n",
       "      <td>8.111635</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>9/26/2020</td>\n",
       "      <td>5.577215</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>4.467977</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>low_Medium</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>9/27/2020</td>\n",
       "      <td>5.184293</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>1.572182</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>9/28/2020</td>\n",
       "      <td>4.469007</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>1.819019</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>9/29/2020</td>\n",
       "      <td>4.259090</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>2.198017</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>9/30/2020</td>\n",
       "      <td>5.513838</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>12.713634</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2562 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall windspeed_category  \\\n",
       "0      6/1/2000  12.245595  0.033285   4.688589             medium   \n",
       "1      6/2/2000  12.825491  0.044874  11.330589             medium   \n",
       "2      6/3/2000  12.921664  0.010243   6.894713             medium   \n",
       "3      6/4/2000  15.149001  0.036881   4.613324        meduim_High   \n",
       "4      6/5/2000  18.495907  0.139491   8.111635               High   \n",
       "...         ...        ...       ...        ...                ...   \n",
       "2557  9/26/2020   5.577215  0.009963   4.467977           very_Low   \n",
       "2558  9/27/2020   5.184293  0.002341   1.572182           very_Low   \n",
       "2559  9/28/2020   4.469007  0.000867   1.819019           very_Low   \n",
       "2560  9/29/2020   4.259090  0.001416   2.198017           very_Low   \n",
       "2561  9/30/2020   5.513838  0.002474  12.713634           very_Low   \n",
       "\n",
       "     tpw_category rainfall_category  \n",
       "0     meduim_High            Normal  \n",
       "1     meduim_High            Normal  \n",
       "2      low_Medium            Normal  \n",
       "3     meduim_High            Normal  \n",
       "4            High            Normal  \n",
       "...           ...               ...  \n",
       "2557   low_Medium            Normal  \n",
       "2558     very_Low            Normal  \n",
       "2559     very_Low            Normal  \n",
       "2560     very_Low            Normal  \n",
       "2561     very_Low            Normal  \n",
       "\n",
       "[2562 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff13a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = data[data['rainfall'] <= 50]\n",
    "extreme_data = data[data['rainfall'] > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da11a731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>windspeed_category</th>\n",
       "      <th>tpw_category</th>\n",
       "      <th>rainfall_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>21.575483</td>\n",
       "      <td>0.042829</td>\n",
       "      <td>80.636074</td>\n",
       "      <td>High</td>\n",
       "      <td>meduim_High</td>\n",
       "      <td>Extreme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>8/10/2019</td>\n",
       "      <td>20.276800</td>\n",
       "      <td>0.043114</td>\n",
       "      <td>58.431061</td>\n",
       "      <td>High</td>\n",
       "      <td>meduim_High</td>\n",
       "      <td>Extreme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall windspeed_category  \\\n",
       "2387   8/9/2019  21.575483  0.042829  80.636074               High   \n",
       "2388  8/10/2019  20.276800  0.043114  58.431061               High   \n",
       "\n",
       "     tpw_category rainfall_category  \n",
       "2387  meduim_High           Extreme  \n",
       "2388  meduim_High           Extreme  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d9390dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>windspeed_category</th>\n",
       "      <th>tpw_category</th>\n",
       "      <th>rainfall_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/1/2000</td>\n",
       "      <td>12.245595</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>4.688589</td>\n",
       "      <td>medium</td>\n",
       "      <td>meduim_High</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/2/2000</td>\n",
       "      <td>12.825491</td>\n",
       "      <td>0.044874</td>\n",
       "      <td>11.330589</td>\n",
       "      <td>medium</td>\n",
       "      <td>meduim_High</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/3/2000</td>\n",
       "      <td>12.921664</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>6.894713</td>\n",
       "      <td>medium</td>\n",
       "      <td>low_Medium</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/4/2000</td>\n",
       "      <td>15.149001</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>4.613324</td>\n",
       "      <td>meduim_High</td>\n",
       "      <td>meduim_High</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/5/2000</td>\n",
       "      <td>18.495907</td>\n",
       "      <td>0.139491</td>\n",
       "      <td>8.111635</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>9/26/2020</td>\n",
       "      <td>5.577215</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>4.467977</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>low_Medium</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>9/27/2020</td>\n",
       "      <td>5.184293</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>1.572182</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>9/28/2020</td>\n",
       "      <td>4.469007</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>1.819019</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>9/29/2020</td>\n",
       "      <td>4.259090</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>2.198017</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>9/30/2020</td>\n",
       "      <td>5.513838</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>12.713634</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>very_Low</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2560 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall windspeed_category  \\\n",
       "0      6/1/2000  12.245595  0.033285   4.688589             medium   \n",
       "1      6/2/2000  12.825491  0.044874  11.330589             medium   \n",
       "2      6/3/2000  12.921664  0.010243   6.894713             medium   \n",
       "3      6/4/2000  15.149001  0.036881   4.613324        meduim_High   \n",
       "4      6/5/2000  18.495907  0.139491   8.111635               High   \n",
       "...         ...        ...       ...        ...                ...   \n",
       "2557  9/26/2020   5.577215  0.009963   4.467977           very_Low   \n",
       "2558  9/27/2020   5.184293  0.002341   1.572182           very_Low   \n",
       "2559  9/28/2020   4.469007  0.000867   1.819019           very_Low   \n",
       "2560  9/29/2020   4.259090  0.001416   2.198017           very_Low   \n",
       "2561  9/30/2020   5.513838  0.002474  12.713634           very_Low   \n",
       "\n",
       "     tpw_category rainfall_category  \n",
       "0     meduim_High            Normal  \n",
       "1     meduim_High            Normal  \n",
       "2      low_Medium            Normal  \n",
       "3     meduim_High            Normal  \n",
       "4            High            Normal  \n",
       "...           ...               ...  \n",
       "2557   low_Medium            Normal  \n",
       "2558     very_Low            Normal  \n",
       "2559     very_Low            Normal  \n",
       "2560     very_Low            Normal  \n",
       "2561     very_Low            Normal  \n",
       "\n",
       "[2560 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_data[normal_data[windspeed_category]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "909596cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall windspeed_category  \\\n",
      "7      6/8/2000  18.196522  0.028442  23.246956               High   \n",
      "29    6/30/2000  16.988686  0.027998  10.824606               High   \n",
      "37     7/8/2000  18.840498  0.037964  11.836345               High   \n",
      "70    8/10/2000  16.779938  0.025627   8.766370               High   \n",
      "81    8/21/2000  18.640314  0.043304  11.121415               High   \n",
      "...         ...        ...       ...        ...                ...   \n",
      "2485  7/16/2020  21.259842  0.055283  16.644822               High   \n",
      "2486  7/17/2020  20.645512  0.047109  23.655588               High   \n",
      "2503   8/3/2020  20.900053  0.026320  15.461144               High   \n",
      "2505   8/5/2020  27.123528  0.037294  32.042267               High   \n",
      "2506   8/6/2020  24.605556  0.025061  27.547421               High   \n",
      "\n",
      "     tpw_category rainfall_category  \n",
      "7     meduim_High            Normal  \n",
      "29    meduim_High            Normal  \n",
      "37    meduim_High            Normal  \n",
      "70    meduim_High            Normal  \n",
      "81    meduim_High            Normal  \n",
      "...           ...               ...  \n",
      "2485  meduim_High            Normal  \n",
      "2486  meduim_High            Normal  \n",
      "2503  meduim_High            Normal  \n",
      "2505  meduim_High           Extreme  \n",
      "2506  meduim_High            Normal  \n",
      "\n",
      "[115 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"rainfall_data.csv\")\n",
    "\n",
    "# Filter the rows based on the conditions\n",
    "filtered_data = normal_data[(normal_data[\"windspeed_category\"] == \"High\") & (normal_data[\"tpw_category\"] == \"meduim_High\")]\n",
    "\n",
    "# Print the filtered data\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26a8d493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shine\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shine\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall  windspeed_category  \\\n",
      "0      6/1/2000  12.245595  0.033285   4.688589                   4   \n",
      "1      6/2/2000  12.825491  0.044874  11.330589                   4   \n",
      "2      6/3/2000  12.921664  0.010243   6.894713                   4   \n",
      "3      6/4/2000  15.149001  0.036881   4.613324                   1   \n",
      "4      6/5/2000  18.495907  0.139491   8.111635                   3   \n",
      "...         ...        ...       ...        ...                 ...   \n",
      "2557  9/26/2020   5.577215  0.009963   4.467977                   2   \n",
      "2558  9/27/2020   5.184293  0.002341   1.572182                   2   \n",
      "2559  9/28/2020   4.469007  0.000867   1.819019                   2   \n",
      "2560  9/29/2020   4.259090  0.001416   2.198017                   2   \n",
      "2561  9/30/2020   5.513838  0.002474  12.713634                   2   \n",
      "\n",
      "      tpw_category  rainfall_category  \n",
      "0                1                  4  \n",
      "1                4                  3  \n",
      "2                1                  0  \n",
      "3                4                  4  \n",
      "4                0                  0  \n",
      "...            ...                ...  \n",
      "2557             1                  4  \n",
      "2558             1                  4  \n",
      "2559             1                  4  \n",
      "2560             1                  4  \n",
      "2561             1                  3  \n",
      "\n",
      "[2562 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shine\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# Perform k-means clustering on the 'windspeed' column\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(data[['windspeed']])\n",
    "data['windspeed_category'] = kmeans.labels_\n",
    "\n",
    "# Perform k-means clustering on the 'tpw' column\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(data[['tpw']])\n",
    "data['tpw_category'] = kmeans.labels_\n",
    "\n",
    "# Perform k-means clustering on the 'rainfall' column\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(data[['rainfall']])\n",
    "data['rainfall_category'] = kmeans.labels_\n",
    "\n",
    "# Print the updated data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c095ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = data[data['rainfall'] <= 50]\n",
    "extreme_data = data[data['rainfall'] > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "169d7843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>windspeed_category</th>\n",
       "      <th>tpw_category</th>\n",
       "      <th>rainfall_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/1/2000</td>\n",
       "      <td>12.245595</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>4.688589</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/2/2000</td>\n",
       "      <td>12.825491</td>\n",
       "      <td>0.044874</td>\n",
       "      <td>11.330589</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/3/2000</td>\n",
       "      <td>12.921664</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>6.894713</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/4/2000</td>\n",
       "      <td>15.149001</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>4.613324</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/5/2000</td>\n",
       "      <td>18.495907</td>\n",
       "      <td>0.139491</td>\n",
       "      <td>8.111635</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>9/26/2020</td>\n",
       "      <td>5.577215</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>4.467977</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>9/27/2020</td>\n",
       "      <td>5.184293</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>1.572182</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>9/28/2020</td>\n",
       "      <td>4.469007</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>1.819019</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>9/29/2020</td>\n",
       "      <td>4.259090</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>2.198017</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>9/30/2020</td>\n",
       "      <td>5.513838</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>12.713634</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2560 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall  windspeed_category  \\\n",
       "0      6/1/2000  12.245595  0.033285   4.688589                   4   \n",
       "1      6/2/2000  12.825491  0.044874  11.330589                   4   \n",
       "2      6/3/2000  12.921664  0.010243   6.894713                   4   \n",
       "3      6/4/2000  15.149001  0.036881   4.613324                   1   \n",
       "4      6/5/2000  18.495907  0.139491   8.111635                   3   \n",
       "...         ...        ...       ...        ...                 ...   \n",
       "2557  9/26/2020   5.577215  0.009963   4.467977                   2   \n",
       "2558  9/27/2020   5.184293  0.002341   1.572182                   2   \n",
       "2559  9/28/2020   4.469007  0.000867   1.819019                   2   \n",
       "2560  9/29/2020   4.259090  0.001416   2.198017                   2   \n",
       "2561  9/30/2020   5.513838  0.002474  12.713634                   2   \n",
       "\n",
       "      tpw_category  rainfall_category  \n",
       "0                1                  4  \n",
       "1                4                  3  \n",
       "2                1                  0  \n",
       "3                4                  4  \n",
       "4                0                  0  \n",
       "...            ...                ...  \n",
       "2557             1                  4  \n",
       "2558             1                  4  \n",
       "2559             1                  4  \n",
       "2560             1                  4  \n",
       "2561             1                  3  \n",
       "\n",
       "[2560 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be796afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>windspeed_category</th>\n",
       "      <th>tpw_category</th>\n",
       "      <th>rainfall_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7/12/2000</td>\n",
       "      <td>23.003010</td>\n",
       "      <td>0.016358</td>\n",
       "      <td>33.846075</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>8/4/2004</td>\n",
       "      <td>21.769238</td>\n",
       "      <td>0.043448</td>\n",
       "      <td>44.404364</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>6/23/2007</td>\n",
       "      <td>23.908970</td>\n",
       "      <td>0.331606</td>\n",
       "      <td>49.356381</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>7/17/2007</td>\n",
       "      <td>19.211468</td>\n",
       "      <td>0.030241</td>\n",
       "      <td>32.941690</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>8/6/2007</td>\n",
       "      <td>17.366877</td>\n",
       "      <td>0.022628</td>\n",
       "      <td>31.609187</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>7/28/2008</td>\n",
       "      <td>20.585272</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>32.559895</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>7/16/2009</td>\n",
       "      <td>21.342918</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>38.754120</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>7/17/2009</td>\n",
       "      <td>21.451813</td>\n",
       "      <td>0.105873</td>\n",
       "      <td>39.511546</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>7/18/2009</td>\n",
       "      <td>16.981620</td>\n",
       "      <td>0.104322</td>\n",
       "      <td>38.342482</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>6/2/2011</td>\n",
       "      <td>12.654827</td>\n",
       "      <td>0.550524</td>\n",
       "      <td>34.993438</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>7/17/2011</td>\n",
       "      <td>19.214113</td>\n",
       "      <td>0.057870</td>\n",
       "      <td>45.056388</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>8/7/2011</td>\n",
       "      <td>15.895408</td>\n",
       "      <td>0.019959</td>\n",
       "      <td>31.262259</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>9/3/2011</td>\n",
       "      <td>17.204716</td>\n",
       "      <td>0.025290</td>\n",
       "      <td>32.701293</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>6/18/2012</td>\n",
       "      <td>16.490360</td>\n",
       "      <td>0.060908</td>\n",
       "      <td>47.977786</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>6/19/2012</td>\n",
       "      <td>12.745975</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>36.064092</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>8/7/2012</td>\n",
       "      <td>11.919377</td>\n",
       "      <td>0.026516</td>\n",
       "      <td>33.012284</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>7/4/2013</td>\n",
       "      <td>16.945307</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>34.218388</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>8/1/2014</td>\n",
       "      <td>20.457104</td>\n",
       "      <td>0.029008</td>\n",
       "      <td>35.467946</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>8/15/2018</td>\n",
       "      <td>21.421936</td>\n",
       "      <td>0.086819</td>\n",
       "      <td>41.892355</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>8/16/2018</td>\n",
       "      <td>22.673685</td>\n",
       "      <td>0.060061</td>\n",
       "      <td>47.362358</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>8/7/2019</td>\n",
       "      <td>21.010150</td>\n",
       "      <td>0.027716</td>\n",
       "      <td>44.681781</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>8/8/2019</td>\n",
       "      <td>22.589184</td>\n",
       "      <td>0.024273</td>\n",
       "      <td>47.165503</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>8/5/2020</td>\n",
       "      <td>27.123528</td>\n",
       "      <td>0.037294</td>\n",
       "      <td>32.042267</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>8/7/2020</td>\n",
       "      <td>19.529327</td>\n",
       "      <td>0.108583</td>\n",
       "      <td>42.017340</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>8/8/2020</td>\n",
       "      <td>17.372940</td>\n",
       "      <td>0.220405</td>\n",
       "      <td>35.905335</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>9/20/2020</td>\n",
       "      <td>18.062815</td>\n",
       "      <td>0.326716</td>\n",
       "      <td>34.073259</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall  windspeed_category  \\\n",
       "41    7/12/2000  23.003010  0.016358  33.846075                   3   \n",
       "552    8/4/2004  21.769238  0.043448  44.404364                   3   \n",
       "876   6/23/2007  23.908970  0.331606  49.356381                   3   \n",
       "900   7/17/2007  19.211468  0.030241  32.941690                   3   \n",
       "920    8/6/2007  17.366877  0.022628  31.609187                   1   \n",
       "1033  7/28/2008  20.585272  0.014900  32.559895                   3   \n",
       "1143  7/16/2009  21.342918  0.037500  38.754120                   3   \n",
       "1144  7/17/2009  21.451813  0.105873  39.511546                   3   \n",
       "1145  7/18/2009  16.981620  0.104322  38.342482                   1   \n",
       "1343   6/2/2011  12.654827  0.550524  34.993438                   4   \n",
       "1388  7/17/2011  19.214113  0.057870  45.056388                   3   \n",
       "1409   8/7/2011  15.895408  0.019959  31.262259                   1   \n",
       "1436   9/3/2011  17.204716  0.025290  32.701293                   1   \n",
       "1481  6/18/2012  16.490360  0.060908  47.977786                   1   \n",
       "1482  6/19/2012  12.745975  0.004308  36.064092                   4   \n",
       "1531   8/7/2012  11.919377  0.026516  33.012284                   4   \n",
       "1619   7/4/2013  16.945307  0.008078  34.218388                   1   \n",
       "1769   8/1/2014  20.457104  0.029008  35.467946                   3   \n",
       "2271  8/15/2018  21.421936  0.086819  41.892355                   3   \n",
       "2272  8/16/2018  22.673685  0.060061  47.362358                   3   \n",
       "2385   8/7/2019  21.010150  0.027716  44.681781                   3   \n",
       "2386   8/8/2019  22.589184  0.024273  47.165503                   3   \n",
       "2505   8/5/2020  27.123528  0.037294  32.042267                   3   \n",
       "2507   8/7/2020  19.529327  0.108583  42.017340                   3   \n",
       "2508   8/8/2020  17.372940  0.220405  35.905335                   1   \n",
       "2551  9/20/2020  18.062815  0.326716  34.073259                   3   \n",
       "\n",
       "      tpw_category  rainfall_category  \n",
       "41               1                  1  \n",
       "552              4                  1  \n",
       "876              3                  1  \n",
       "900              1                  1  \n",
       "920              1                  1  \n",
       "1033             1                  1  \n",
       "1143             4                  1  \n",
       "1144             0                  1  \n",
       "1145             0                  1  \n",
       "1343             2                  1  \n",
       "1388             4                  1  \n",
       "1409             1                  1  \n",
       "1436             1                  1  \n",
       "1481             4                  1  \n",
       "1482             1                  1  \n",
       "1531             1                  1  \n",
       "1619             1                  1  \n",
       "1769             1                  1  \n",
       "2271             4                  1  \n",
       "2272             4                  1  \n",
       "2385             1                  1  \n",
       "2386             1                  1  \n",
       "2505             4                  1  \n",
       "2507             0                  1  \n",
       "2508             3                  1  \n",
       "2551             3                  1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_data[normal_data[\"rainfall_category\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6dcade9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>windspeed_category</th>\n",
       "      <th>tpw_category</th>\n",
       "      <th>rainfall_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>21.575483</td>\n",
       "      <td>0.042829</td>\n",
       "      <td>80.636074</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>8/10/2019</td>\n",
       "      <td>20.276800</td>\n",
       "      <td>0.043114</td>\n",
       "      <td>58.431061</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall  windspeed_category  \\\n",
       "2387   8/9/2019  21.575483  0.042829  80.636074                   3   \n",
       "2388  8/10/2019  20.276800  0.043114  58.431061                   3   \n",
       "\n",
       "      tpw_category  rainfall_category  \n",
       "2387             4                  1  \n",
       "2388             4                  1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38f9e46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall  rainfall_tree  \\\n",
      "0      6/1/2000  12.245595  0.033285   4.688589              0   \n",
      "1      6/2/2000  12.825491  0.044874  11.330589              0   \n",
      "2      6/3/2000  12.921664  0.010243   6.894713              0   \n",
      "3      6/4/2000  15.149001  0.036881   4.613324              0   \n",
      "4      6/5/2000  18.495907  0.139491   8.111635              0   \n",
      "...         ...        ...       ...        ...            ...   \n",
      "2557  9/26/2020   5.577215  0.009963   4.467977              0   \n",
      "2558  9/27/2020   5.184293  0.002341   1.572182              0   \n",
      "2559  9/28/2020   4.469007  0.000867   1.819019              0   \n",
      "2560  9/29/2020   4.259090  0.001416   2.198017              0   \n",
      "2561  9/30/2020   5.513838  0.002474  12.713634              0   \n",
      "\n",
      "      windspeed_tree  tpw_tree  \n",
      "0                  0         0  \n",
      "1                  0         0  \n",
      "2                  0         0  \n",
      "3                  0         0  \n",
      "4                  0         0  \n",
      "...              ...       ...  \n",
      "2557               0         0  \n",
      "2558               0         0  \n",
      "2559               0         0  \n",
      "2560               0         0  \n",
      "2561               0         0  \n",
      "\n",
      "[2562 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shine\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\shine\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('rainfall_data.csv')\n",
    "\n",
    "# create decision tree for target variable\n",
    "tree = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "\n",
    "# discretize target variable using decision tree\n",
    "df['rainfall_tree'] = pd.cut(df['rainfall'], bins=6, labels=False)\n",
    "tree.fit(df[['rainfall']], df['rainfall_tree'])\n",
    "df['rainfall_tree'] = tree.predict(df[['rainfall']])\n",
    "\n",
    "# create new columns for the discretized data\n",
    "df['windspeed_tree'] = tree.predict(pd.cut(df['windspeed'], bins=6, labels=False).values.reshape(-1,1))\n",
    "df['tpw_tree'] = tree.predict(pd.cut(df['tpw'], bins=6, labels=False).values.reshape(-1,1))\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c33262f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_data=df[df[\"rainfall\"]>50]\n",
    "normal_data=df[df[\"rainfall\"]<=50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85aac813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>rainfall_tree</th>\n",
       "      <th>windspeed_tree</th>\n",
       "      <th>tpw_tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>21.575483</td>\n",
       "      <td>0.042829</td>\n",
       "      <td>80.636074</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>8/10/2019</td>\n",
       "      <td>20.276800</td>\n",
       "      <td>0.043114</td>\n",
       "      <td>58.431061</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall  rainfall_tree  \\\n",
       "2387   8/9/2019  21.575483  0.042829  80.636074              4   \n",
       "2388  8/10/2019  20.276800  0.043114  58.431061              4   \n",
       "\n",
       "      windspeed_tree  tpw_tree  \n",
       "2387               0         0  \n",
       "2388               0         0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54c0c129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>rainfall_tree</th>\n",
       "      <th>windspeed_tree</th>\n",
       "      <th>tpw_tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, windspeed, tpw, rainfall, rainfall_tree, windspeed_tree, tpw_tree]\n",
       "Index: []"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_data[normal_data[\"rainfall_tree\"]==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "acc10d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shine\\AppData\\Local\\Temp\\ipykernel_16652\\2453966789.py:25: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 5s 2ms/step - loss: 0.3609 - mse: 0.0568\n",
      "Best reg_strength: 0.010 using {'reg_strength': 0.01}\n",
      "Epoch 1/1000\n",
      "64/64 [==============================] - 1s 7ms/step - loss: 0.3182 - val_loss: 0.2918\n",
      "Epoch 2/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2475 - val_loss: 0.2310\n",
      "Epoch 3/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2026 - val_loss: 0.1905\n",
      "Epoch 4/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1707 - val_loss: 0.1613\n",
      "Epoch 5/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1463 - val_loss: 0.1385\n",
      "Epoch 6/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1262 - val_loss: 0.1195\n",
      "Epoch 7/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1091 - val_loss: 0.1033\n",
      "Epoch 8/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0943 - val_loss: 0.0892\n",
      "Epoch 9/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0815 - val_loss: 0.0770\n",
      "Epoch 10/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0703 - val_loss: 0.0664\n",
      "Epoch 11/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0604 - val_loss: 0.0571\n",
      "Epoch 12/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.0490\n",
      "Epoch 13/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.0419\n",
      "Epoch 14/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.0358\n",
      "Epoch 15/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0305\n",
      "Epoch 16/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0259\n",
      "Epoch 17/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0220\n",
      "Epoch 18/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0197 - val_loss: 0.0186\n",
      "Epoch 19/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0167 - val_loss: 0.0157\n",
      "Epoch 20/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0141 - val_loss: 0.0133\n",
      "Epoch 21/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0113\n",
      "Epoch 22/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0100 - val_loss: 0.0095\n",
      "Epoch 23/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.0081\n",
      "Epoch 24/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 25/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0058\n",
      "Epoch 26/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0052 - val_loss: 0.0050\n",
      "Epoch 27/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 28/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 29/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 30/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 31/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 32/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 33/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 34/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 35/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 36/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 37/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 38/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 39/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 40/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 41/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 42/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7883e-04 - val_loss: 0.0010\n",
      "Epoch 43/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1481e-04 - val_loss: 9.5681e-04\n",
      "Epoch 44/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5831e-04 - val_loss: 8.8911e-04\n",
      "Epoch 45/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0891e-04 - val_loss: 8.3662e-04\n",
      "Epoch 46/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6421e-04 - val_loss: 8.0586e-04\n",
      "Epoch 47/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2745e-04 - val_loss: 7.5283e-04\n",
      "Epoch 48/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.8504e-04 - val_loss: 7.0484e-04\n",
      "Epoch 49/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4179e-04 - val_loss: 6.6390e-04\n",
      "Epoch 50/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.0718e-04 - val_loss: 6.3462e-04\n",
      "Epoch 51/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7694e-04 - val_loss: 6.0093e-04\n",
      "Epoch 52/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.5063e-04 - val_loss: 5.7406e-04\n",
      "Epoch 53/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2849e-04 - val_loss: 5.5487e-04\n",
      "Epoch 54/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0827e-04 - val_loss: 5.2944e-04\n",
      "Epoch 55/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8809e-04 - val_loss: 5.1029e-04\n",
      "Epoch 56/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7205e-04 - val_loss: 4.9630e-04\n",
      "Epoch 57/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5657e-04 - val_loss: 4.7703e-04\n",
      "Epoch 58/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4289e-04 - val_loss: 4.6493e-04\n",
      "Epoch 59/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2909e-04 - val_loss: 4.5776e-04\n",
      "Epoch 60/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1863e-04 - val_loss: 4.3753e-04\n",
      "Epoch 61/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0453e-04 - val_loss: 4.2444e-04\n",
      "Epoch 62/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9503e-04 - val_loss: 4.1972e-04\n",
      "Epoch 63/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.8436e-04 - val_loss: 4.0579e-04\n",
      "Epoch 64/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7567e-04 - val_loss: 3.9737e-04\n",
      "Epoch 65/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7083e-04 - val_loss: 3.8489e-04\n",
      "Epoch 66/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5761e-04 - val_loss: 3.7593e-04\n",
      "Epoch 67/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4938e-04 - val_loss: 3.7046e-04\n",
      "Epoch 68/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4240e-04 - val_loss: 3.6018e-04\n",
      "Epoch 69/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3531e-04 - val_loss: 3.5213e-04\n",
      "Epoch 70/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2790e-04 - val_loss: 3.4609e-04\n",
      "Epoch 71/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2184e-04 - val_loss: 3.4281e-04\n",
      "Epoch 72/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1595e-04 - val_loss: 3.3148e-04\n",
      "Epoch 73/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0876e-04 - val_loss: 3.2571e-04\n",
      "Epoch 74/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0325e-04 - val_loss: 3.2079e-04\n",
      "Epoch 75/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9800e-04 - val_loss: 3.1461e-04\n",
      "Epoch 76/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9147e-04 - val_loss: 3.0903e-04\n",
      "Epoch 77/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8654e-04 - val_loss: 3.0180e-04\n",
      "Epoch 78/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8263e-04 - val_loss: 3.0369e-04\n",
      "Epoch 79/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7763e-04 - val_loss: 2.9078e-04\n",
      "Epoch 80/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7141e-04 - val_loss: 2.8702e-04\n",
      "Epoch 81/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6703e-04 - val_loss: 2.8133e-04\n",
      "Epoch 82/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6278e-04 - val_loss: 2.7699e-04\n",
      "Epoch 83/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5960e-04 - val_loss: 2.8370e-04\n",
      "Epoch 84/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5583e-04 - val_loss: 2.7118e-04\n",
      "Epoch 85/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.5008e-04 - val_loss: 2.6449e-04\n",
      "Epoch 86/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4594e-04 - val_loss: 2.5897e-04\n",
      "Epoch 87/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4276e-04 - val_loss: 2.5470e-04\n",
      "Epoch 88/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3906e-04 - val_loss: 2.5184e-04\n",
      "Epoch 89/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3585e-04 - val_loss: 2.4855e-04\n",
      "Epoch 90/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3103e-04 - val_loss: 2.4366e-04\n",
      "Epoch 91/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2775e-04 - val_loss: 2.4004e-04\n",
      "Epoch 92/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.2450e-04 - val_loss: 2.3710e-04\n",
      "Epoch 93/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2169e-04 - val_loss: 2.3315e-04\n",
      "Epoch 94/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1781e-04 - val_loss: 2.2983e-04\n",
      "Epoch 95/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1413e-04 - val_loss: 2.2686e-04\n",
      "Epoch 96/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1161e-04 - val_loss: 2.2283e-04\n",
      "Epoch 97/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.0864e-04 - val_loss: 2.1933e-04\n",
      "Epoch 98/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.0556e-04 - val_loss: 2.1836e-04\n",
      "Epoch 99/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.0291e-04 - val_loss: 2.1421e-04\n",
      "Epoch 100/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.9968e-04 - val_loss: 2.1082e-04\n",
      "Epoch 101/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.9666e-04 - val_loss: 2.0710e-04\n",
      "Epoch 102/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.9323e-04 - val_loss: 2.0359e-04\n",
      "Epoch 103/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.9013e-04 - val_loss: 2.0259e-04\n",
      "Epoch 104/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.8981e-04 - val_loss: 1.9699e-04\n",
      "Epoch 105/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.8436e-04 - val_loss: 1.9462e-04\n",
      "Epoch 106/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.8121e-04 - val_loss: 1.9110e-04\n",
      "Epoch 107/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.7822e-04 - val_loss: 1.8779e-04\n",
      "Epoch 108/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.7539e-04 - val_loss: 1.8457e-04\n",
      "Epoch 109/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.7326e-04 - val_loss: 1.8189e-04\n",
      "Epoch 110/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.7083e-04 - val_loss: 1.7954e-04\n",
      "Epoch 111/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6800e-04 - val_loss: 1.7669e-04\n",
      "Epoch 112/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6603e-04 - val_loss: 1.7650e-04\n",
      "Epoch 113/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6488e-04 - val_loss: 1.7442e-04\n",
      "Epoch 114/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6065e-04 - val_loss: 1.6980e-04\n",
      "Epoch 115/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.5845e-04 - val_loss: 1.6701e-04\n",
      "Epoch 116/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.5622e-04 - val_loss: 1.6749e-04\n",
      "Epoch 117/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.5436e-04 - val_loss: 1.6283e-04\n",
      "Epoch 118/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.5143e-04 - val_loss: 1.6356e-04\n",
      "Epoch 119/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4978e-04 - val_loss: 1.5800e-04\n",
      "Epoch 120/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4744e-04 - val_loss: 1.5596e-04\n",
      "Epoch 121/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4618e-04 - val_loss: 1.5393e-04\n",
      "Epoch 122/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4403e-04 - val_loss: 1.5397e-04\n",
      "Epoch 123/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4406e-04 - val_loss: 1.5484e-04\n",
      "Epoch 124/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.4005e-04 - val_loss: 1.4865e-04\n",
      "Epoch 125/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.3912e-04 - val_loss: 1.4608e-04\n",
      "Epoch 126/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3840e-04 - val_loss: 1.4415e-04\n",
      "Epoch 127/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3493e-04 - val_loss: 1.4267e-04\n",
      "Epoch 128/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.3389e-04 - val_loss: 1.4095e-04\n",
      "Epoch 129/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3112e-04 - val_loss: 1.4119e-04\n",
      "Epoch 130/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3372e-04 - val_loss: 1.3843e-04\n",
      "Epoch 131/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2966e-04 - val_loss: 1.3782e-04\n",
      "Epoch 132/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2709e-04 - val_loss: 1.3427e-04\n",
      "Epoch 133/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2632e-04 - val_loss: 1.3297e-04\n",
      "Epoch 134/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2493e-04 - val_loss: 1.3077e-04\n",
      "Epoch 135/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4404e-04 - val_loss: 1.3325e-04\n",
      "Epoch 136/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2700e-04 - val_loss: 1.2889e-04\n",
      "Epoch 137/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.2013e-04 - val_loss: 1.2800e-04\n",
      "Epoch 138/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1907e-04 - val_loss: 1.2607e-04\n",
      "Epoch 139/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.1819e-04 - val_loss: 1.2540e-04\n",
      "Epoch 140/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.1702e-04 - val_loss: 1.2416e-04\n",
      "Epoch 141/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1657e-04 - val_loss: 1.2296e-04\n",
      "Epoch 142/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1532e-04 - val_loss: 1.2151e-04\n",
      "Epoch 143/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1376e-04 - val_loss: 1.2190e-04\n",
      "Epoch 144/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1320e-04 - val_loss: 1.1957e-04\n",
      "Epoch 145/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1169e-04 - val_loss: 1.1860e-04\n",
      "Epoch 146/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1174e-04 - val_loss: 1.1688e-04\n",
      "Epoch 147/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1008e-04 - val_loss: 1.1618e-04\n",
      "Epoch 148/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0835e-04 - val_loss: 1.1492e-04\n",
      "Epoch 149/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0899e-04 - val_loss: 1.1527e-04\n",
      "Epoch 150/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0790e-04 - val_loss: 1.1690e-04\n",
      "Epoch 151/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0637e-04 - val_loss: 1.1250e-04\n",
      "Epoch 152/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0646e-04 - val_loss: 1.1154e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0646e-04 - val_loss: 1.1095e-04\n",
      "Epoch 154/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0307e-04 - val_loss: 1.0860e-04\n",
      "Epoch 155/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0492e-04 - val_loss: 1.1023e-04\n",
      "Epoch 156/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0206e-04 - val_loss: 1.0706e-04\n",
      "Epoch 157/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0006e-04 - val_loss: 1.0565e-04\n",
      "Epoch 158/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8890e-05 - val_loss: 1.0457e-04\n",
      "Epoch 159/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8200e-05 - val_loss: 1.0431e-04\n",
      "Epoch 160/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7867e-05 - val_loss: 1.0323e-04\n",
      "Epoch 161/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6448e-05 - val_loss: 1.0394e-04\n",
      "Epoch 162/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.5816e-05 - val_loss: 1.0148e-04\n",
      "Epoch 163/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.5264e-05 - val_loss: 1.0188e-04\n",
      "Epoch 164/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4520e-05 - val_loss: 9.9635e-05\n",
      "Epoch 165/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3206e-05 - val_loss: 9.8796e-05\n",
      "Epoch 166/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2932e-05 - val_loss: 9.7687e-05\n",
      "Epoch 167/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1737e-05 - val_loss: 9.8791e-05\n",
      "Epoch 168/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1374e-05 - val_loss: 9.8019e-05\n",
      "Epoch 169/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.9952e-05 - val_loss: 9.6912e-05\n",
      "Epoch 170/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2025e-05 - val_loss: 9.4639e-05\n",
      "Epoch 171/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8429e-05 - val_loss: 9.3274e-05\n",
      "Epoch 172/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8817e-05 - val_loss: 9.4823e-05\n",
      "Epoch 173/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7315e-05 - val_loss: 9.3360e-05\n",
      "Epoch 174/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7241e-05 - val_loss: 9.2595e-05\n",
      "Epoch 175/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7398e-05 - val_loss: 9.1435e-05\n",
      "Epoch 176/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8472e-05 - val_loss: 9.1493e-05\n",
      "Epoch 177/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4785e-05 - val_loss: 8.9992e-05\n",
      "Epoch 178/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4115e-05 - val_loss: 8.9933e-05\n",
      "Epoch 179/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4074e-05 - val_loss: 9.1778e-05\n",
      "Epoch 180/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2776e-05 - val_loss: 8.7684e-05\n",
      "Epoch 181/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2051e-05 - val_loss: 8.6344e-05\n",
      "Epoch 182/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2113e-05 - val_loss: 8.6042e-05\n",
      "Epoch 183/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9909e-05 - val_loss: 8.5770e-05\n",
      "Epoch 184/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9562e-05 - val_loss: 8.5200e-05\n",
      "Epoch 185/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.8898e-05 - val_loss: 8.3897e-05\n",
      "Epoch 186/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8509e-05 - val_loss: 8.4066e-05\n",
      "Epoch 187/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7871e-05 - val_loss: 8.1745e-05\n",
      "Epoch 188/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7239e-05 - val_loss: 8.0979e-05\n",
      "Epoch 189/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7538e-05 - val_loss: 8.2013e-05\n",
      "Epoch 190/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6349e-05 - val_loss: 8.1089e-05\n",
      "Epoch 191/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7690e-05 - val_loss: 8.5081e-05\n",
      "Epoch 192/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7464e-05 - val_loss: 8.5996e-05\n",
      "Epoch 193/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5669e-05 - val_loss: 7.9329e-05\n",
      "Epoch 194/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4224e-05 - val_loss: 7.8368e-05\n",
      "Epoch 195/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3401e-05 - val_loss: 8.0622e-05\n",
      "Epoch 196/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.2840e-05 - val_loss: 8.0482e-05\n",
      "Epoch 197/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2939e-05 - val_loss: 7.9316e-05\n",
      "Epoch 198/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.1977e-05 - val_loss: 7.5838e-05\n",
      "Epoch 199/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1291e-05 - val_loss: 7.5103e-05\n",
      "Epoch 200/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0602e-05 - val_loss: 7.5650e-05\n",
      "Epoch 201/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2719e-05 - val_loss: 7.7474e-05\n",
      "Epoch 202/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4445e-05 - val_loss: 7.4079e-05\n",
      "Epoch 203/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1213e-05 - val_loss: 7.3483e-05\n",
      "Epoch 204/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9269e-05 - val_loss: 7.2527e-05\n",
      "Epoch 205/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8461e-05 - val_loss: 7.2573e-05\n",
      "Epoch 206/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7972e-05 - val_loss: 7.1569e-05\n",
      "Epoch 207/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7564e-05 - val_loss: 7.3029e-05\n",
      "Epoch 208/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7582e-05 - val_loss: 7.2333e-05\n",
      "Epoch 209/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8610e-05 - val_loss: 7.0585e-05\n",
      "Epoch 210/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7913e-05 - val_loss: 7.0627e-05\n",
      "Epoch 211/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6397e-05 - val_loss: 6.9741e-05\n",
      "Epoch 212/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6792e-05 - val_loss: 7.1642e-05\n",
      "Epoch 213/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4850e-05 - val_loss: 6.8888e-05\n",
      "Epoch 214/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5702e-05 - val_loss: 6.8930e-05\n",
      "Epoch 215/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4280e-05 - val_loss: 6.7926e-05\n",
      "Epoch 216/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.3953e-05 - val_loss: 6.7380e-05\n",
      "Epoch 217/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5325e-05 - val_loss: 6.7193e-05\n",
      "Epoch 218/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3738e-05 - val_loss: 6.6023e-05\n",
      "Epoch 219/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3521e-05 - val_loss: 6.6896e-05\n",
      "Epoch 220/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3984e-05 - val_loss: 6.9590e-05\n",
      "Epoch 221/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2685e-05 - val_loss: 6.7475e-05\n",
      "Epoch 222/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2571e-05 - val_loss: 6.5430e-05\n",
      "Epoch 223/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1373e-05 - val_loss: 6.4933e-05\n",
      "Epoch 224/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.1028e-05 - val_loss: 6.5343e-05\n",
      "Epoch 225/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0590e-05 - val_loss: 6.9448e-05\n",
      "Epoch 226/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1386e-05 - val_loss: 6.4222e-05\n",
      "Epoch 227/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.9798e-05 - val_loss: 6.3654e-05\n",
      "Epoch 228/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0526e-05 - val_loss: 6.4204e-05\n",
      "Epoch 229/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9738e-05 - val_loss: 6.5105e-05\n",
      "Epoch 230/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9229e-05 - val_loss: 6.2761e-05\n",
      "Epoch 231/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8904e-05 - val_loss: 6.2703e-05\n",
      "Epoch 232/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8741e-05 - val_loss: 6.1553e-05\n",
      "Epoch 233/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8111e-05 - val_loss: 6.1963e-05\n",
      "Epoch 234/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8814e-05 - val_loss: 6.2937e-05\n",
      "Epoch 235/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6053e-05 - val_loss: 6.3347e-05\n",
      "Epoch 236/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8497e-05 - val_loss: 7.2632e-05\n",
      "Epoch 237/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3064e-05 - val_loss: 6.1969e-05\n",
      "Epoch 238/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0091e-05 - val_loss: 6.2645e-05\n",
      "Epoch 239/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7570e-05 - val_loss: 5.9506e-05\n",
      "Epoch 240/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6096e-05 - val_loss: 5.9480e-05\n",
      "Epoch 241/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6936e-05 - val_loss: 5.9609e-05\n",
      "Epoch 242/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6937e-05 - val_loss: 5.9670e-05\n",
      "Epoch 243/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5296e-05 - val_loss: 5.8132e-05\n",
      "Epoch 244/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6118e-05 - val_loss: 5.8357e-05\n",
      "Epoch 245/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0360e-05 - val_loss: 6.8419e-05\n",
      "Epoch 246/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8697e-05 - val_loss: 5.8080e-05\n",
      "Epoch 247/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.4764e-05 - val_loss: 5.8942e-05\n",
      "Epoch 248/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4221e-05 - val_loss: 5.7037e-05\n",
      "Epoch 249/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3896e-05 - val_loss: 5.9213e-05\n",
      "Epoch 250/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3999e-05 - val_loss: 5.8142e-05\n",
      "Epoch 251/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4436e-05 - val_loss: 5.6222e-05\n",
      "Epoch 252/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4310e-05 - val_loss: 5.7799e-05\n",
      "Epoch 253/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2969e-05 - val_loss: 5.7725e-05\n",
      "Epoch 254/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5802e-05 - val_loss: 5.7011e-05\n",
      "Epoch 255/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2185e-05 - val_loss: 5.5085e-05\n",
      "Epoch 256/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2468e-05 - val_loss: 5.4739e-05\n",
      "Epoch 257/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2143e-05 - val_loss: 5.5248e-05\n",
      "Epoch 258/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3111e-05 - val_loss: 6.0205e-05\n",
      "Epoch 259/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3806e-05 - val_loss: 5.7041e-05\n",
      "Epoch 260/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1904e-05 - val_loss: 5.3912e-05\n",
      "Epoch 261/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0824e-05 - val_loss: 5.6747e-05\n",
      "Epoch 262/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1599e-05 - val_loss: 5.3692e-05\n",
      "Epoch 263/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.1694e-05 - val_loss: 5.7710e-05\n",
      "Epoch 264/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2071e-05 - val_loss: 6.0511e-05\n",
      "Epoch 265/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0109e-05 - val_loss: 5.2610e-05\n",
      "Epoch 266/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9906e-05 - val_loss: 5.2643e-05\n",
      "Epoch 267/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9992e-05 - val_loss: 5.2585e-05\n",
      "Epoch 268/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9269e-05 - val_loss: 5.2471e-05\n",
      "Epoch 269/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9789e-05 - val_loss: 5.1952e-05\n",
      "Epoch 270/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0137e-05 - val_loss: 5.3553e-05\n",
      "Epoch 271/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9103e-05 - val_loss: 5.3630e-05\n",
      "Epoch 272/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8676e-05 - val_loss: 5.1616e-05\n",
      "Epoch 273/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1911e-05 - val_loss: 5.6870e-05\n",
      "Epoch 274/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.0405e-05 - val_loss: 5.3989e-05\n",
      "Epoch 275/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8846e-05 - val_loss: 5.0671e-05\n",
      "Epoch 276/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7628e-05 - val_loss: 5.0723e-05\n",
      "Epoch 277/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1635e-05 - val_loss: 5.6255e-05\n",
      "Epoch 278/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.0764e-05 - val_loss: 5.6187e-05\n",
      "Epoch 279/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8203e-05 - val_loss: 5.2940e-05\n",
      "Epoch 280/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5873e-05 - val_loss: 5.8582e-05\n",
      "Epoch 281/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.9969e-05 - val_loss: 5.0077e-05\n",
      "Epoch 282/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6334e-05 - val_loss: 4.8960e-05\n",
      "Epoch 283/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6170e-05 - val_loss: 4.9534e-05\n",
      "Epoch 284/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6425e-05 - val_loss: 4.9040e-05\n",
      "Epoch 285/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7479e-05 - val_loss: 4.9276e-05\n",
      "Epoch 286/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6224e-05 - val_loss: 4.9255e-05\n",
      "Epoch 287/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5759e-05 - val_loss: 4.8707e-05\n",
      "Epoch 288/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5404e-05 - val_loss: 4.8645e-05\n",
      "Epoch 289/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5461e-05 - val_loss: 4.8164e-05\n",
      "Epoch 290/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5514e-05 - val_loss: 4.8268e-05\n",
      "Epoch 291/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4913e-05 - val_loss: 5.6233e-05\n",
      "Epoch 292/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9688e-05 - val_loss: 5.0332e-05\n",
      "Epoch 293/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5318e-05 - val_loss: 4.7496e-05\n",
      "Epoch 294/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4669e-05 - val_loss: 4.7161e-05\n",
      "Epoch 295/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4461e-05 - val_loss: 4.6706e-05\n",
      "Epoch 296/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.4600e-05 - val_loss: 4.6774e-05\n",
      "Epoch 297/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6880e-05 - val_loss: 4.9582e-05\n",
      "Epoch 298/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4641e-05 - val_loss: 4.7588e-05\n",
      "Epoch 299/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5768e-05 - val_loss: 4.6991e-05\n",
      "Epoch 300/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2121e-05 - val_loss: 5.0547e-05\n",
      "Epoch 301/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4688e-05 - val_loss: 4.6037e-05\n",
      "Epoch 302/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3623e-05 - val_loss: 4.6101e-05\n",
      "Epoch 303/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3849e-05 - val_loss: 4.5744e-05\n",
      "Epoch 304/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.3850e-05 - val_loss: 4.6865e-05\n",
      "Epoch 305/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3793e-05 - val_loss: 4.5604e-05\n",
      "Epoch 306/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3786e-05 - val_loss: 4.5386e-05\n",
      "Epoch 307/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2836e-05 - val_loss: 4.5557e-05\n",
      "Epoch 308/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3080e-05 - val_loss: 4.5671e-05\n",
      "Epoch 309/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1070e-04 - val_loss: 6.8159e-05\n",
      "Epoch 310/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8567e-05 - val_loss: 4.8089e-05\n",
      "Epoch 311/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3001e-05 - val_loss: 4.4940e-05\n",
      "Epoch 312/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2276e-05 - val_loss: 4.5025e-05\n",
      "Epoch 313/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1972e-05 - val_loss: 4.4916e-05\n",
      "Epoch 314/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1722e-05 - val_loss: 4.4669e-05\n",
      "Epoch 315/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1548e-05 - val_loss: 4.4839e-05\n",
      "Epoch 316/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1768e-05 - val_loss: 4.4329e-05\n",
      "Epoch 317/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1656e-05 - val_loss: 4.5259e-05\n",
      "Epoch 318/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1907e-05 - val_loss: 4.3812e-05\n",
      "Epoch 319/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.1308e-05 - val_loss: 4.4557e-05\n",
      "Epoch 320/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1162e-05 - val_loss: 4.3857e-05\n",
      "Epoch 321/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1587e-05 - val_loss: 4.4135e-05\n",
      "Epoch 322/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0999e-05 - val_loss: 4.3603e-05\n",
      "Epoch 323/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1112e-05 - val_loss: 4.3539e-05\n",
      "Epoch 324/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5337e-05 - val_loss: 6.9350e-05\n",
      "Epoch 325/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3369e-05 - val_loss: 4.4688e-05\n",
      "Epoch 326/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1690e-05 - val_loss: 4.5602e-05\n",
      "Epoch 327/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0582e-05 - val_loss: 4.2787e-05\n",
      "Epoch 328/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0519e-05 - val_loss: 4.2862e-05\n",
      "Epoch 329/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.0894e-05 - val_loss: 4.4074e-05\n",
      "Epoch 330/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0611e-05 - val_loss: 4.2555e-05\n",
      "Epoch 331/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0198e-05 - val_loss: 4.2694e-05\n",
      "Epoch 332/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0125e-05 - val_loss: 4.2678e-05\n",
      "Epoch 333/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5304e-05 - val_loss: 4.8936e-05\n",
      "Epoch 334/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1910e-05 - val_loss: 7.0864e-05\n",
      "Epoch 335/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3746e-05 - val_loss: 4.2389e-05\n",
      "Epoch 336/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9804e-05 - val_loss: 4.2221e-05\n",
      "Epoch 337/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.9797e-05 - val_loss: 4.2306e-05\n",
      "Epoch 338/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9360e-05 - val_loss: 4.2456e-05\n",
      "Epoch 339/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9699e-05 - val_loss: 4.2001e-05\n",
      "Epoch 340/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9248e-05 - val_loss: 4.2575e-05\n",
      "Epoch 341/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9226e-05 - val_loss: 4.1552e-05\n",
      "Epoch 342/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9340e-05 - val_loss: 4.1911e-05\n",
      "Epoch 343/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9336e-05 - val_loss: 4.1442e-05\n",
      "Epoch 344/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9297e-05 - val_loss: 4.2155e-05\n",
      "Epoch 345/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.9733e-05 - val_loss: 4.1361e-05\n",
      "Epoch 346/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8937e-05 - val_loss: 4.1132e-05\n",
      "Epoch 347/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8846e-05 - val_loss: 4.2164e-05\n",
      "Epoch 348/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9292e-05 - val_loss: 4.2846e-05\n",
      "Epoch 349/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9265e-05 - val_loss: 4.1585e-05\n",
      "Epoch 350/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0157e-05 - val_loss: 4.2373e-05\n",
      "Epoch 351/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9204e-05 - val_loss: 4.1702e-05\n",
      "Epoch 352/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.8796e-05 - val_loss: 4.1666e-05\n",
      "Epoch 353/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8833e-05 - val_loss: 4.1014e-05\n",
      "Epoch 354/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4182e-05 - val_loss: 5.6777e-05\n",
      "Epoch 355/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9951e-05 - val_loss: 4.3541e-05\n",
      "Epoch 356/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9137e-05 - val_loss: 4.0785e-05\n",
      "Epoch 357/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.8075e-05 - val_loss: 4.0661e-05\n",
      "Epoch 358/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8120e-05 - val_loss: 4.0171e-05\n",
      "Epoch 359/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7983e-05 - val_loss: 4.0400e-05\n",
      "Epoch 360/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7951e-05 - val_loss: 4.0784e-05\n",
      "Epoch 361/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9687e-05 - val_loss: 4.1073e-05\n",
      "Epoch 362/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9289e-05 - val_loss: 4.1172e-05\n",
      "Epoch 363/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8084e-05 - val_loss: 4.2129e-05\n",
      "Epoch 364/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.8566e-05 - val_loss: 4.0603e-05\n",
      "Epoch 365/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8025e-05 - val_loss: 4.0034e-05\n",
      "Epoch 366/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7703e-05 - val_loss: 3.9543e-05\n",
      "Epoch 367/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7244e-05 - val_loss: 3.9361e-05\n",
      "Epoch 368/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8483e-05 - val_loss: 1.5175e-04\n",
      "Epoch 369/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2316e-05 - val_loss: 4.5043e-05\n",
      "Epoch 370/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8770e-05 - val_loss: 3.9622e-05\n",
      "Epoch 371/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7110e-05 - val_loss: 3.9355e-05\n",
      "Epoch 372/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6982e-05 - val_loss: 3.9297e-05\n",
      "Epoch 373/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6881e-05 - val_loss: 3.9195e-05\n",
      "Epoch 374/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6849e-05 - val_loss: 3.9083e-05\n",
      "Epoch 375/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6775e-05 - val_loss: 3.9010e-05\n",
      "Epoch 376/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6736e-05 - val_loss: 3.9482e-05\n",
      "Epoch 377/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6903e-05 - val_loss: 3.8908e-05\n",
      "Epoch 378/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6589e-05 - val_loss: 3.9082e-05\n",
      "Epoch 379/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6637e-05 - val_loss: 3.9472e-05\n",
      "Epoch 380/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6637e-05 - val_loss: 3.9287e-05\n",
      "Epoch 381/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6569e-05 - val_loss: 3.8639e-05\n",
      "Epoch 382/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6251e-05 - val_loss: 3.8548e-05\n",
      "Epoch 383/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6380e-05 - val_loss: 3.8545e-05\n",
      "Epoch 384/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6151e-05 - val_loss: 3.8577e-05\n",
      "Epoch 385/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.6039e-05 - val_loss: 3.8561e-05\n",
      "Epoch 386/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.6171e-05 - val_loss: 3.8518e-05\n",
      "Epoch 387/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6546e-05 - val_loss: 3.8217e-05\n",
      "Epoch 388/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5996e-05 - val_loss: 3.8127e-05\n",
      "Epoch 389/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6067e-05 - val_loss: 3.8033e-05\n",
      "Epoch 390/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6823e-05 - val_loss: 3.8029e-05\n",
      "Epoch 391/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5765e-05 - val_loss: 3.9072e-05\n",
      "Epoch 392/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2567e-05 - val_loss: 8.8685e-05\n",
      "Epoch 393/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.5667e-05 - val_loss: 4.7609e-05\n",
      "Epoch 394/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2450e-05 - val_loss: 3.8359e-05\n",
      "Epoch 395/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5487e-05 - val_loss: 3.7676e-05\n",
      "Epoch 396/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5416e-05 - val_loss: 3.7561e-05\n",
      "Epoch 397/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5394e-05 - val_loss: 3.7438e-05\n",
      "Epoch 398/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5329e-05 - val_loss: 3.7365e-05\n",
      "Epoch 399/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5185e-05 - val_loss: 3.7235e-05\n",
      "Epoch 400/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5076e-05 - val_loss: 3.7173e-05\n",
      "Epoch 401/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5137e-05 - val_loss: 3.8066e-05\n",
      "Epoch 402/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6068e-05 - val_loss: 4.2894e-05\n",
      "Epoch 403/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.7640e-05 - val_loss: 3.7203e-05\n",
      "Epoch 404/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6804e-05 - val_loss: 3.7143e-05\n",
      "Epoch 405/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5432e-05 - val_loss: 3.8399e-05\n",
      "Epoch 406/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8866e-05 - val_loss: 4.2125e-05\n",
      "Epoch 407/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.5612e-05 - val_loss: 3.7216e-05\n",
      "Epoch 408/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1647e-05 - val_loss: 7.4003e-05\n",
      "Epoch 409/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.5979e-05 - val_loss: 3.7454e-05\n",
      "Epoch 410/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4994e-05 - val_loss: 3.6575e-05\n",
      "Epoch 411/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4675e-05 - val_loss: 3.6496e-05\n",
      "Epoch 412/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4355e-05 - val_loss: 3.6418e-05\n",
      "Epoch 413/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4290e-05 - val_loss: 3.6474e-05\n",
      "Epoch 414/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4751e-05 - val_loss: 3.7517e-05\n",
      "Epoch 415/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5140e-05 - val_loss: 3.6465e-05\n",
      "Epoch 416/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4472e-05 - val_loss: 3.7793e-05\n",
      "Epoch 417/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4206e-05 - val_loss: 3.6073e-05\n",
      "Epoch 418/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.4027e-05 - val_loss: 3.6341e-05\n",
      "Epoch 419/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4153e-05 - val_loss: 3.7307e-05\n",
      "Epoch 420/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4791e-05 - val_loss: 3.6712e-05\n",
      "Epoch 421/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6066e-05 - val_loss: 3.6870e-05\n",
      "Epoch 422/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4588e-05 - val_loss: 3.6357e-05\n",
      "Epoch 423/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.5796e-05 - val_loss: 3.9779e-05\n",
      "Epoch 424/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.4607e-05 - val_loss: 3.6272e-05\n",
      "Epoch 425/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4526e-05 - val_loss: 4.4017e-05\n",
      "Epoch 426/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4073e-05 - val_loss: 7.5606e-05\n",
      "Epoch 427/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8434e-05 - val_loss: 5.0031e-05\n",
      "Epoch 428/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9662e-05 - val_loss: 3.6747e-05\n",
      "Epoch 429/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3922e-05 - val_loss: 3.5679e-05\n",
      "Epoch 430/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3482e-05 - val_loss: 3.5787e-05\n",
      "Epoch 431/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3665e-05 - val_loss: 3.5817e-05\n",
      "Epoch 432/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4736e-05 - val_loss: 3.6931e-05\n",
      "Epoch 433/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4041e-05 - val_loss: 3.5159e-05\n",
      "Epoch 434/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3256e-05 - val_loss: 3.5379e-05\n",
      "Epoch 435/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3054e-05 - val_loss: 3.4888e-05\n",
      "Epoch 436/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3173e-05 - val_loss: 3.5666e-05\n",
      "Epoch 437/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4914e-05 - val_loss: 3.7932e-05\n",
      "Epoch 438/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3482e-05 - val_loss: 3.6394e-05\n",
      "Epoch 439/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5249e-05 - val_loss: 3.9246e-05\n",
      "Epoch 440/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8881e-05 - val_loss: 3.6171e-05\n",
      "Epoch 441/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5837e-05 - val_loss: 3.6484e-05\n",
      "Epoch 442/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3233e-05 - val_loss: 3.4873e-05\n",
      "Epoch 443/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3005e-05 - val_loss: 3.5373e-05\n",
      "Epoch 444/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4270e-05 - val_loss: 3.6007e-05\n",
      "Epoch 445/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4005e-05 - val_loss: 3.5078e-05\n",
      "Epoch 446/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4818e-05 - val_loss: 3.8727e-05\n",
      "Epoch 447/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3627e-05 - val_loss: 3.4937e-05\n",
      "Epoch 448/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4864e-05 - val_loss: 4.5421e-05\n",
      "Epoch 449/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6885e-05 - val_loss: 4.3994e-05\n",
      "Epoch 450/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3538e-05 - val_loss: 3.4311e-05\n",
      "Epoch 451/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2486e-05 - val_loss: 3.4885e-05\n",
      "Epoch 452/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0737e-05 - val_loss: 1.4164e-04\n",
      "Epoch 453/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.8384e-04 - val_loss: 4.9133e-05\n",
      "Epoch 454/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4388e-05 - val_loss: 4.2139e-05\n",
      "Epoch 455/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.3045e-05 - val_loss: 3.4124e-05\n",
      "Epoch 456/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2029e-05 - val_loss: 3.4009e-05\n",
      "Epoch 457/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1985e-05 - val_loss: 3.4068e-05\n",
      "Epoch 458/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1960e-05 - val_loss: 3.4024e-05\n",
      "Epoch 459/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1926e-05 - val_loss: 3.3918e-05\n",
      "Epoch 460/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1850e-05 - val_loss: 3.3800e-05\n",
      "Epoch 461/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1782e-05 - val_loss: 3.3821e-05\n",
      "Epoch 462/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1712e-05 - val_loss: 3.3720e-05\n",
      "Epoch 463/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1656e-05 - val_loss: 3.3625e-05\n",
      "Epoch 464/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1615e-05 - val_loss: 3.3687e-05\n",
      "Epoch 465/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1601e-05 - val_loss: 3.3561e-05\n",
      "Epoch 466/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1528e-05 - val_loss: 3.3505e-05\n",
      "Epoch 467/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1504e-05 - val_loss: 3.3558e-05\n",
      "Epoch 468/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1476e-05 - val_loss: 3.3465e-05\n",
      "Epoch 469/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1429e-05 - val_loss: 3.3459e-05\n",
      "Epoch 470/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1386e-05 - val_loss: 3.3366e-05\n",
      "Epoch 471/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1361e-05 - val_loss: 3.3498e-05\n",
      "Epoch 472/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1342e-05 - val_loss: 3.3233e-05\n",
      "Epoch 473/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1250e-05 - val_loss: 3.3214e-05\n",
      "Epoch 474/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.1231e-05 - val_loss: 3.3135e-05\n",
      "Epoch 475/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1180e-05 - val_loss: 3.3110e-05\n",
      "Epoch 476/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1153e-05 - val_loss: 3.3342e-05\n",
      "Epoch 477/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1167e-05 - val_loss: 3.2972e-05\n",
      "Epoch 478/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1361e-05 - val_loss: 3.3755e-05\n",
      "Epoch 479/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1154e-05 - val_loss: 3.3009e-05\n",
      "Epoch 480/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2642e-05 - val_loss: 3.8607e-05\n",
      "Epoch 481/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2400e-05 - val_loss: 3.4267e-05\n",
      "Epoch 482/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1700e-05 - val_loss: 3.2867e-05\n",
      "Epoch 483/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1126e-05 - val_loss: 3.3026e-05\n",
      "Epoch 484/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1356e-05 - val_loss: 3.3114e-05\n",
      "Epoch 485/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1254e-05 - val_loss: 3.3557e-05\n",
      "Epoch 486/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0846e-05 - val_loss: 3.2779e-05\n",
      "Epoch 487/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6184e-05 - val_loss: 5.5422e-05\n",
      "Epoch 488/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3879e-04 - val_loss: 2.4073e-04\n",
      "Epoch 489/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.8169e-04 - val_loss: 9.0226e-05\n",
      "Epoch 490/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9955e-05 - val_loss: 3.2900e-05\n",
      "Epoch 491/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0899e-05 - val_loss: 3.2794e-05\n",
      "Epoch 492/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0786e-05 - val_loss: 3.2744e-05\n",
      "Epoch 493/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0721e-05 - val_loss: 3.2857e-05\n",
      "Epoch 494/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0750e-05 - val_loss: 3.2708e-05\n",
      "Epoch 495/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0792e-05 - val_loss: 3.2895e-05\n",
      "Epoch 496/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0752e-05 - val_loss: 3.2817e-05\n",
      "Epoch 497/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0695e-05 - val_loss: 3.2767e-05\n",
      "Epoch 498/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0660e-05 - val_loss: 3.3569e-05\n",
      "Epoch 499/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1342e-05 - val_loss: 3.2744e-05\n",
      "Epoch 500/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0817e-05 - val_loss: 3.3048e-05\n",
      "Epoch 501/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1859e-05 - val_loss: 3.2639e-05\n",
      "Epoch 502/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0510e-05 - val_loss: 3.2381e-05\n",
      "Epoch 503/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0480e-05 - val_loss: 3.2336e-05\n",
      "Epoch 504/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0539e-05 - val_loss: 3.2276e-05\n",
      "Epoch 505/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0359e-05 - val_loss: 3.2285e-05\n",
      "Epoch 506/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0332e-05 - val_loss: 3.2109e-05\n",
      "Epoch 507/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0265e-05 - val_loss: 3.2472e-05\n",
      "Epoch 508/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0624e-05 - val_loss: 3.2150e-05\n",
      "Epoch 509/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0306e-05 - val_loss: 3.2301e-05\n",
      "Epoch 510/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0231e-05 - val_loss: 3.2096e-05\n",
      "Epoch 511/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0325e-05 - val_loss: 3.2028e-05\n",
      "Epoch 512/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0174e-05 - val_loss: 3.2289e-05\n",
      "Epoch 513/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0205e-05 - val_loss: 3.2293e-05\n",
      "Epoch 514/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0295e-05 - val_loss: 3.3160e-05\n",
      "Epoch 515/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0580e-05 - val_loss: 3.2798e-05\n",
      "Epoch 516/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0666e-05 - val_loss: 3.1834e-05\n",
      "Epoch 517/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1402e-05 - val_loss: 3.5552e-05\n",
      "Epoch 518/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1205e-05 - val_loss: 3.1842e-05\n",
      "Epoch 519/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0167e-05 - val_loss: 3.1756e-05\n",
      "Epoch 520/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0145e-05 - val_loss: 3.2122e-05\n",
      "Epoch 521/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0015e-05 - val_loss: 3.1788e-05\n",
      "Epoch 522/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9770e-05 - val_loss: 3.1759e-05\n",
      "Epoch 523/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0886e-05 - val_loss: 3.2365e-05\n",
      "Epoch 524/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4280e-05 - val_loss: 3.9363e-05\n",
      "Epoch 525/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1764e-05 - val_loss: 3.2174e-05\n",
      "Epoch 526/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0704e-05 - val_loss: 3.1384e-05\n",
      "Epoch 527/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0812e-05 - val_loss: 3.3265e-05\n",
      "Epoch 528/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1439e-05 - val_loss: 3.2526e-05\n",
      "Epoch 529/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0273e-05 - val_loss: 3.3368e-05\n",
      "Epoch 530/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5084e-05 - val_loss: 3.2936e-05\n",
      "Epoch 531/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0498e-05 - val_loss: 3.1545e-05\n",
      "Epoch 532/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6023e-04 - val_loss: 1.0193e-04\n",
      "Epoch 533/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3888e-05 - val_loss: 3.3831e-05\n",
      "Epoch 534/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0794e-05 - val_loss: 3.1803e-05\n",
      "Epoch 535/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9652e-05 - val_loss: 3.1413e-05\n",
      "Epoch 536/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9534e-05 - val_loss: 3.1373e-05\n",
      "Epoch 537/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9479e-05 - val_loss: 3.1358e-05\n",
      "Epoch 538/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9583e-05 - val_loss: 3.1275e-05\n",
      "Epoch 539/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9458e-05 - val_loss: 3.1633e-05\n",
      "Epoch 540/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9439e-05 - val_loss: 3.1451e-05\n",
      "Epoch 541/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9633e-05 - val_loss: 3.1353e-05\n",
      "Epoch 542/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0079e-05 - val_loss: 3.1458e-05\n",
      "Epoch 543/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9390e-05 - val_loss: 3.1215e-05\n",
      "Epoch 544/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9463e-05 - val_loss: 3.1249e-05\n",
      "Epoch 545/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9370e-05 - val_loss: 3.1277e-05\n",
      "Epoch 546/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9257e-05 - val_loss: 3.1033e-05\n",
      "Epoch 547/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9614e-05 - val_loss: 3.1375e-05\n",
      "Epoch 548/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9460e-05 - val_loss: 3.1056e-05\n",
      "Epoch 549/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9131e-05 - val_loss: 3.1262e-05\n",
      "Epoch 550/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9283e-05 - val_loss: 3.0956e-05\n",
      "Epoch 551/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9204e-05 - val_loss: 3.0881e-05\n",
      "Epoch 552/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9127e-05 - val_loss: 3.1544e-05\n",
      "Epoch 553/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9283e-05 - val_loss: 3.1143e-05\n",
      "Epoch 554/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9312e-05 - val_loss: 3.0811e-05\n",
      "Epoch 555/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9404e-05 - val_loss: 3.1222e-05\n",
      "Epoch 556/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9969e-05 - val_loss: 3.0770e-05\n",
      "Epoch 557/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9060e-05 - val_loss: 3.0945e-05\n",
      "Epoch 558/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9157e-05 - val_loss: 3.1640e-05\n",
      "Epoch 559/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0311e-05 - val_loss: 3.1290e-05\n",
      "Epoch 560/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0314e-05 - val_loss: 3.1785e-05\n",
      "Epoch 561/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9714e-05 - val_loss: 3.1298e-05\n",
      "Epoch 562/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9127e-05 - val_loss: 3.1124e-05\n",
      "Epoch 563/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4256e-05 - val_loss: 3.8941e-05\n",
      "Epoch 564/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.4247e-05 - val_loss: 5.2486e-05\n",
      "Epoch 565/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8587e-05 - val_loss: 3.1289e-05\n",
      "Epoch 566/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8852e-05 - val_loss: 3.0149e-05\n",
      "Epoch 567/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8794e-05 - val_loss: 3.1272e-05\n",
      "Epoch 568/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9828e-05 - val_loss: 3.2224e-05\n",
      "Epoch 569/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1983e-05 - val_loss: 4.3442e-05\n",
      "Epoch 570/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1321e-05 - val_loss: 3.1344e-05\n",
      "Epoch 571/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9582e-05 - val_loss: 3.0234e-05\n",
      "Epoch 572/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9223e-05 - val_loss: 3.2052e-05\n",
      "Epoch 573/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9037e-05 - val_loss: 3.0033e-05\n",
      "Epoch 574/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8573e-05 - val_loss: 3.0391e-05\n",
      "Epoch 575/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8563e-05 - val_loss: 3.1490e-05\n",
      "Epoch 576/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8427e-05 - val_loss: 2.9819e-05\n",
      "Epoch 577/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9009e-05 - val_loss: 2.9699e-05\n",
      "Epoch 578/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8360e-05 - val_loss: 2.9705e-05\n",
      "Epoch 579/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8229e-05 - val_loss: 3.0437e-05\n",
      "Epoch 580/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8328e-05 - val_loss: 3.9233e-05\n",
      "Epoch 581/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2006e-05 - val_loss: 1.2736e-04\n",
      "Epoch 582/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6907e-05 - val_loss: 3.8142e-05\n",
      "Epoch 583/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0157e-05 - val_loss: 2.9948e-05\n",
      "Epoch 584/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0505e-05 - val_loss: 3.6048e-05\n",
      "Epoch 585/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2079e-05 - val_loss: 3.3887e-05\n",
      "Epoch 586/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8929e-05 - val_loss: 3.1200e-05\n",
      "Epoch 587/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0800e-05 - val_loss: 3.0872e-05\n",
      "Epoch 588/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8365e-05 - val_loss: 3.0243e-05\n",
      "Epoch 589/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8222e-05 - val_loss: 3.1984e-05\n",
      "Epoch 590/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8934e-05 - val_loss: 2.9768e-05\n",
      "Epoch 591/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7970e-05 - val_loss: 2.9878e-05\n",
      "Epoch 592/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9305e-05 - val_loss: 2.9575e-05\n",
      "Epoch 593/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9357e-05 - val_loss: 2.9774e-05\n",
      "Epoch 594/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7936e-05 - val_loss: 2.9262e-05\n",
      "Epoch 595/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8151e-05 - val_loss: 3.0215e-05\n",
      "Epoch 596/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7765e-05 - val_loss: 2.9603e-05\n",
      "Epoch 597/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7585e-05 - val_loss: 2.8955e-05\n",
      "Epoch 598/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7694e-05 - val_loss: 2.9490e-05\n",
      "Epoch 599/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7903e-05 - val_loss: 2.9336e-05\n",
      "Epoch 600/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7276e-05 - val_loss: 3.1835e-04\n",
      "Epoch 601/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3723e-04 - val_loss: 8.1050e-05\n",
      "Epoch 602/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0550e-05 - val_loss: 2.9952e-05\n",
      "Epoch 603/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7753e-05 - val_loss: 2.8929e-05\n",
      "Epoch 604/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7194e-05 - val_loss: 2.8879e-05\n",
      "Epoch 605/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7164e-05 - val_loss: 2.8901e-05\n",
      "Epoch 606/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7118e-05 - val_loss: 2.8820e-05\n",
      "Epoch 607/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7105e-05 - val_loss: 2.8795e-05\n",
      "Epoch 608/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7074e-05 - val_loss: 2.8761e-05\n",
      "Epoch 609/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7096e-05 - val_loss: 2.8769e-05\n",
      "Epoch 610/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7036e-05 - val_loss: 2.8658e-05\n",
      "Epoch 611/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6941e-05 - val_loss: 2.8665e-05\n",
      "Epoch 612/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7099e-05 - val_loss: 2.8982e-05\n",
      "Epoch 613/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6949e-05 - val_loss: 2.8751e-05\n",
      "Epoch 614/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6877e-05 - val_loss: 2.8540e-05\n",
      "Epoch 615/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.6835e-05 - val_loss: 2.8562e-05\n",
      "Epoch 616/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6906e-05 - val_loss: 2.8475e-05\n",
      "Epoch 617/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6769e-05 - val_loss: 2.8431e-05\n",
      "Epoch 618/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6733e-05 - val_loss: 2.8446e-05\n",
      "Epoch 619/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6780e-05 - val_loss: 2.8408e-05\n",
      "Epoch 620/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.6818e-05 - val_loss: 2.8477e-05\n",
      "Epoch 621/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6755e-05 - val_loss: 2.8364e-05\n",
      "Epoch 622/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6679e-05 - val_loss: 2.8309e-05\n",
      "Epoch 623/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6792e-05 - val_loss: 2.8253e-05\n",
      "Epoch 624/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6604e-05 - val_loss: 2.9434e-05\n",
      "Epoch 625/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8035e-05 - val_loss: 4.7782e-05\n",
      "Epoch 626/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9245e-05 - val_loss: 4.0368e-05\n",
      "Epoch 627/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4802e-05 - val_loss: 3.1265e-05\n",
      "Epoch 628/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7518e-05 - val_loss: 2.8618e-05\n",
      "Epoch 629/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6787e-05 - val_loss: 2.8413e-05\n",
      "Epoch 630/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6769e-05 - val_loss: 2.8746e-05\n",
      "Epoch 631/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.6738e-05 - val_loss: 2.8739e-05\n",
      "Epoch 632/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8168e-05 - val_loss: 2.8188e-05\n",
      "Epoch 633/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6515e-05 - val_loss: 2.8329e-05\n",
      "Epoch 634/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6508e-05 - val_loss: 2.8103e-05\n",
      "Epoch 635/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9328e-05 - val_loss: 4.9350e-05\n",
      "Epoch 636/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9582e-05 - val_loss: 2.8434e-05\n",
      "Epoch 637/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7518e-05 - val_loss: 2.8610e-05\n",
      "Epoch 638/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6621e-05 - val_loss: 2.8630e-05\n",
      "Epoch 639/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6645e-05 - val_loss: 2.7900e-05\n",
      "Epoch 640/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7226e-05 - val_loss: 2.7930e-05\n",
      "Epoch 641/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6582e-05 - val_loss: 2.7911e-05\n",
      "Epoch 642/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6876e-05 - val_loss: 2.7956e-05\n",
      "Epoch 643/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8028e-05 - val_loss: 2.8362e-05\n",
      "Epoch 644/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8167e-05 - val_loss: 2.8796e-05\n",
      "Epoch 645/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0789e-05 - val_loss: 2.8812e-05\n",
      "Epoch 646/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1210e-05 - val_loss: 4.1318e-05\n",
      "Epoch 647/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7500e-05 - val_loss: 2.7677e-05\n",
      "Epoch 648/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5971e-05 - val_loss: 2.7767e-05\n",
      "Epoch 649/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.6625e-05 - val_loss: 2.7998e-05\n",
      "Epoch 650/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6108e-05 - val_loss: 2.9984e-05\n",
      "Epoch 651/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1221e-05 - val_loss: 3.1173e-05\n",
      "Epoch 652/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6845e-05 - val_loss: 2.7520e-05\n",
      "Epoch 653/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5929e-05 - val_loss: 2.8099e-05\n",
      "Epoch 654/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6670e-05 - val_loss: 2.8329e-05\n",
      "Epoch 655/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8921e-05 - val_loss: 2.7845e-05\n",
      "Epoch 656/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5744e-05 - val_loss: 2.7379e-05\n",
      "Epoch 657/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7339e-05 - val_loss: 2.7447e-05\n",
      "Epoch 658/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5977e-05 - val_loss: 2.7136e-05\n",
      "Epoch 659/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6540e-05 - val_loss: 2.8214e-05\n",
      "Epoch 660/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5980e-05 - val_loss: 2.8060e-05\n",
      "Epoch 661/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6699e-05 - val_loss: 2.7354e-05\n",
      "Epoch 662/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6826e-05 - val_loss: 2.8555e-05\n",
      "Epoch 663/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8123e-05 - val_loss: 3.3586e-05\n",
      "Epoch 664/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8725e-05 - val_loss: 3.1319e-05\n",
      "Epoch 665/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.4991e-05 - val_loss: 3.0248e-05\n",
      "Epoch 666/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7149e-05 - val_loss: 3.8451e-05\n",
      "Epoch 667/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6998e-05 - val_loss: 6.9084e-05\n",
      "Epoch 668/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5971e-05 - val_loss: 7.0112e-05\n",
      "Epoch 669/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1322e-05 - val_loss: 4.0340e-05\n",
      "Epoch 670/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7739e-05 - val_loss: 2.7685e-05\n",
      "Epoch 671/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5647e-05 - val_loss: 2.7225e-05\n",
      "Epoch 672/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5584e-05 - val_loss: 2.7189e-05\n",
      "Epoch 673/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5543e-05 - val_loss: 2.7086e-05\n",
      "Epoch 674/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5556e-05 - val_loss: 2.7131e-05\n",
      "Epoch 675/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5534e-05 - val_loss: 2.7054e-05\n",
      "Epoch 676/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.5885e-05 - val_loss: 2.7726e-05\n",
      "Epoch 677/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6548e-05 - val_loss: 2.7228e-05\n",
      "Epoch 678/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5568e-05 - val_loss: 2.7286e-05\n",
      "Epoch 679/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5651e-05 - val_loss: 2.8377e-05\n",
      "Epoch 680/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5556e-05 - val_loss: 2.7039e-05\n",
      "Epoch 681/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6338e-05 - val_loss: 2.7769e-05\n",
      "Epoch 682/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5681e-05 - val_loss: 2.7236e-05\n",
      "Epoch 683/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5298e-05 - val_loss: 2.6718e-05\n",
      "Epoch 684/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6420e-05 - val_loss: 2.7390e-05\n",
      "Epoch 685/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5222e-05 - val_loss: 2.6734e-05\n",
      "Epoch 686/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5113e-05 - val_loss: 2.6645e-05\n",
      "Epoch 687/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5074e-05 - val_loss: 2.6628e-05\n",
      "Epoch 688/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3349e-05 - val_loss: 3.4569e-05\n",
      "Epoch 689/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3482e-05 - val_loss: 4.0314e-05\n",
      "Epoch 690/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7568e-05 - val_loss: 2.6496e-05\n",
      "Epoch 691/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2333e-05 - val_loss: 3.7924e-05\n",
      "Epoch 692/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0185e-05 - val_loss: 2.7549e-05\n",
      "Epoch 693/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5246e-05 - val_loss: 2.7840e-05\n",
      "Epoch 694/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5378e-05 - val_loss: 2.6466e-05\n",
      "Epoch 695/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4945e-05 - val_loss: 2.6377e-05\n",
      "Epoch 696/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6256e-05 - val_loss: 2.7986e-05\n",
      "Epoch 697/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4967e-05 - val_loss: 2.6900e-05\n",
      "Epoch 698/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5212e-05 - val_loss: 2.6500e-05\n",
      "Epoch 699/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6032e-05 - val_loss: 3.4141e-05\n",
      "Epoch 700/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1176e-05 - val_loss: 8.1448e-05\n",
      "Epoch 701/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8234e-05 - val_loss: 2.7268e-05\n",
      "Epoch 702/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4732e-05 - val_loss: 2.6069e-05\n",
      "Epoch 703/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4517e-05 - val_loss: 2.6053e-05\n",
      "Epoch 704/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4600e-05 - val_loss: 2.6095e-05\n",
      "Epoch 705/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5779e-05 - val_loss: 2.7268e-05\n",
      "Epoch 706/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4590e-05 - val_loss: 2.5944e-05\n",
      "Epoch 707/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5384e-05 - val_loss: 2.6862e-05\n",
      "Epoch 708/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6205e-05 - val_loss: 2.8065e-05\n",
      "Epoch 709/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0726e-05 - val_loss: 2.6569e-05\n",
      "Epoch 710/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5347e-05 - val_loss: 2.7543e-05\n",
      "Epoch 711/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7330e-05 - val_loss: 2.6190e-05\n",
      "Epoch 712/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5709e-05 - val_loss: 5.1425e-04\n",
      "Epoch 713/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1780e-05 - val_loss: 2.6856e-05\n",
      "Epoch 714/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4528e-05 - val_loss: 2.5877e-05\n",
      "Epoch 715/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4328e-05 - val_loss: 2.5886e-05\n",
      "Epoch 716/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4289e-05 - val_loss: 2.5820e-05\n",
      "Epoch 717/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4266e-05 - val_loss: 2.5948e-05\n",
      "Epoch 718/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4277e-05 - val_loss: 2.5738e-05\n",
      "Epoch 719/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4361e-05 - val_loss: 2.5927e-05\n",
      "Epoch 720/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4613e-05 - val_loss: 2.5703e-05\n",
      "Epoch 721/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.4138e-05 - val_loss: 2.5595e-05\n",
      "Epoch 722/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4066e-05 - val_loss: 2.5569e-05\n",
      "Epoch 723/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4065e-05 - val_loss: 2.5575e-05\n",
      "Epoch 724/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4140e-05 - val_loss: 2.5630e-05\n",
      "Epoch 725/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4025e-05 - val_loss: 2.5513e-05\n",
      "Epoch 726/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3998e-05 - val_loss: 2.5534e-05\n",
      "Epoch 727/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4512e-05 - val_loss: 2.5533e-05\n",
      "Epoch 728/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3975e-05 - val_loss: 2.5423e-05\n",
      "Epoch 729/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3980e-05 - val_loss: 2.5380e-05\n",
      "Epoch 730/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3922e-05 - val_loss: 2.5512e-05\n",
      "Epoch 731/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3957e-05 - val_loss: 2.5913e-05\n",
      "Epoch 732/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3914e-05 - val_loss: 2.5361e-05\n",
      "Epoch 733/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3889e-05 - val_loss: 2.5425e-05\n",
      "Epoch 734/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4151e-05 - val_loss: 2.5867e-05\n",
      "Epoch 735/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6827e-05 - val_loss: 3.4335e-05\n",
      "Epoch 736/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1524e-05 - val_loss: 3.0372e-05\n",
      "Epoch 737/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4760e-05 - val_loss: 8.0307e-05\n",
      "Epoch 738/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8075e-05 - val_loss: 5.6352e-05\n",
      "Epoch 739/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7969e-05 - val_loss: 2.5872e-05\n",
      "Epoch 740/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3968e-05 - val_loss: 2.5452e-05\n",
      "Epoch 741/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3945e-05 - val_loss: 2.5434e-05\n",
      "Epoch 742/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3886e-05 - val_loss: 2.5409e-05\n",
      "Epoch 743/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3901e-05 - val_loss: 2.5377e-05\n",
      "Epoch 744/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.4062e-05 - val_loss: 2.5509e-05\n",
      "Epoch 745/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3930e-05 - val_loss: 2.5307e-05\n",
      "Epoch 746/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3842e-05 - val_loss: 2.5247e-05\n",
      "Epoch 747/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3761e-05 - val_loss: 2.5345e-05\n",
      "Epoch 748/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3758e-05 - val_loss: 2.5124e-05\n",
      "Epoch 749/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5404e-05 - val_loss: 3.0057e-05\n",
      "Epoch 750/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5223e-05 - val_loss: 2.9384e-05\n",
      "Epoch 751/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8968e-05 - val_loss: 2.9249e-05\n",
      "Epoch 752/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2767e-05 - val_loss: 4.2347e-05\n",
      "Epoch 753/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2889e-05 - val_loss: 2.6506e-05\n",
      "Epoch 754/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3864e-05 - val_loss: 2.5210e-05\n",
      "Epoch 755/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.3640e-05 - val_loss: 2.5121e-05\n",
      "Epoch 756/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3811e-05 - val_loss: 2.5897e-05\n",
      "Epoch 757/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4009e-05 - val_loss: 2.5195e-05\n",
      "Epoch 758/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3846e-05 - val_loss: 2.5155e-05\n",
      "Epoch 759/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6944e-05 - val_loss: 3.2086e-05\n",
      "Epoch 760/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3989e-05 - val_loss: 2.5579e-05\n",
      "Epoch 761/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1174e-05 - val_loss: 3.7056e-05\n",
      "Epoch 762/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5714e-05 - val_loss: 2.6302e-05\n",
      "Epoch 763/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4238e-05 - val_loss: 2.6504e-05\n",
      "Epoch 764/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3834e-05 - val_loss: 2.4979e-05\n",
      "Epoch 765/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5169e-05 - val_loss: 3.3148e-05\n",
      "Epoch 766/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5556e-05 - val_loss: 2.4929e-05\n",
      "Epoch 767/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3554e-05 - val_loss: 2.7025e-05\n",
      "Epoch 768/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6535e-05 - val_loss: 3.7231e-05\n",
      "Epoch 769/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6838e-05 - val_loss: 2.6533e-05\n",
      "Epoch 770/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3929e-05 - val_loss: 2.5271e-05\n",
      "Epoch 771/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7418e-05 - val_loss: 2.8602e-05\n",
      "Epoch 772/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2973e-04 - val_loss: 0.0029\n",
      "Epoch 773/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6552e-04 - val_loss: 1.0489e-04\n",
      "Epoch 774/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3048e-05 - val_loss: 2.5147e-05\n",
      "Epoch 775/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3421e-05 - val_loss: 2.4774e-05\n",
      "Epoch 776/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3285e-05 - val_loss: 2.4779e-05\n",
      "Epoch 777/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3285e-05 - val_loss: 2.4762e-05\n",
      "Epoch 778/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3275e-05 - val_loss: 2.4728e-05\n",
      "Epoch 779/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3264e-05 - val_loss: 2.4724e-05\n",
      "Epoch 780/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3233e-05 - val_loss: 2.4655e-05\n",
      "Epoch 781/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3194e-05 - val_loss: 2.4690e-05\n",
      "Epoch 782/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3187e-05 - val_loss: 2.4642e-05\n",
      "Epoch 783/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3185e-05 - val_loss: 2.4624e-05\n",
      "Epoch 784/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3173e-05 - val_loss: 2.4622e-05\n",
      "Epoch 785/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3165e-05 - val_loss: 2.4622e-05\n",
      "Epoch 786/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3153e-05 - val_loss: 2.4591e-05\n",
      "Epoch 787/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3156e-05 - val_loss: 2.4625e-05\n",
      "Epoch 788/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3100e-05 - val_loss: 2.4504e-05\n",
      "Epoch 789/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3042e-05 - val_loss: 2.4508e-05\n",
      "Epoch 790/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3029e-05 - val_loss: 2.4512e-05\n",
      "Epoch 791/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3026e-05 - val_loss: 2.4495e-05\n",
      "Epoch 792/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3071e-05 - val_loss: 2.4453e-05\n",
      "Epoch 793/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2999e-05 - val_loss: 2.4468e-05\n",
      "Epoch 794/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2984e-05 - val_loss: 2.4464e-05\n",
      "Epoch 795/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2978e-05 - val_loss: 2.4446e-05\n",
      "Epoch 796/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3006e-05 - val_loss: 2.4491e-05\n",
      "Epoch 797/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2986e-05 - val_loss: 2.4412e-05\n",
      "Epoch 798/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2909e-05 - val_loss: 2.4348e-05\n",
      "Epoch 799/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2878e-05 - val_loss: 2.4339e-05\n",
      "Epoch 800/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2911e-05 - val_loss: 2.4336e-05\n",
      "Epoch 801/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2907e-05 - val_loss: 2.4295e-05\n",
      "Epoch 802/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2858e-05 - val_loss: 2.4298e-05\n",
      "Epoch 803/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2843e-05 - val_loss: 2.4494e-05\n",
      "Epoch 804/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2890e-05 - val_loss: 2.4360e-05\n",
      "Epoch 805/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2835e-05 - val_loss: 2.4260e-05\n",
      "Epoch 806/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2792e-05 - val_loss: 2.4211e-05\n",
      "Epoch 807/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2782e-05 - val_loss: 2.4147e-05\n",
      "Epoch 808/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2846e-05 - val_loss: 2.4270e-05\n",
      "Epoch 809/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2782e-05 - val_loss: 2.4174e-05\n",
      "Epoch 810/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2754e-05 - val_loss: 2.4211e-05\n",
      "Epoch 811/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2752e-05 - val_loss: 2.4135e-05\n",
      "Epoch 812/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2801e-05 - val_loss: 2.4122e-05\n",
      "Epoch 813/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2851e-05 - val_loss: 2.4253e-05\n",
      "Epoch 814/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3017e-05 - val_loss: 2.6871e-05\n",
      "Epoch 815/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5864e-05 - val_loss: 3.2657e-05\n",
      "Epoch 816/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1249e-04 - val_loss: 3.2497e-05\n",
      "Epoch 817/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7935e-05 - val_loss: 2.5528e-05\n",
      "Epoch 818/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3623e-05 - val_loss: 2.4401e-05\n",
      "Epoch 819/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3597e-05 - val_loss: 2.6295e-05\n",
      "Epoch 820/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3125e-05 - val_loss: 2.4319e-05\n",
      "Epoch 821/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2733e-05 - val_loss: 2.4140e-05\n",
      "Epoch 822/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2749e-05 - val_loss: 2.4218e-05\n",
      "Epoch 823/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2770e-05 - val_loss: 2.4433e-05\n",
      "Epoch 824/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3216e-05 - val_loss: 2.4618e-05\n",
      "Epoch 825/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2967e-05 - val_loss: 2.4197e-05\n",
      "Epoch 826/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2750e-05 - val_loss: 2.4147e-05\n",
      "Epoch 827/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2884e-05 - val_loss: 2.4376e-05\n",
      "Epoch 828/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3421e-05 - val_loss: 2.4468e-05\n",
      "Epoch 829/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3082e-05 - val_loss: 3.1471e-05\n",
      "Epoch 830/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5205e-05 - val_loss: 2.4685e-05\n",
      "Epoch 831/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3984e-05 - val_loss: 2.6167e-05\n",
      "Epoch 832/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3367e-05 - val_loss: 2.4256e-05\n",
      "Epoch 833/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2870e-05 - val_loss: 2.3916e-05\n",
      "Epoch 834/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3415e-05 - val_loss: 2.4399e-05\n",
      "Epoch 835/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2887e-05 - val_loss: 2.5101e-05\n",
      "Epoch 836/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5047e-05 - val_loss: 3.0605e-05\n",
      "Epoch 837/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0788e-05 - val_loss: 3.2275e-05\n",
      "Epoch 838/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7317e-05 - val_loss: 2.5715e-05\n",
      "Epoch 839/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3441e-05 - val_loss: 2.3956e-05\n",
      "Epoch 840/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2608e-05 - val_loss: 2.4022e-05\n",
      "Epoch 841/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2479e-05 - val_loss: 2.3862e-05\n",
      "Epoch 842/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2471e-05 - val_loss: 2.3890e-05\n",
      "Epoch 843/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2428e-05 - val_loss: 2.4250e-05\n",
      "Epoch 844/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2470e-05 - val_loss: 2.3750e-05\n",
      "Epoch 845/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2510e-05 - val_loss: 2.4244e-05\n",
      "Epoch 846/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3341e-05 - val_loss: 2.6297e-05\n",
      "Epoch 847/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2899e-05 - val_loss: 2.4526e-05\n",
      "Epoch 848/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2561e-05 - val_loss: 2.3828e-05\n",
      "Epoch 849/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5228e-05 - val_loss: 2.6580e-05\n",
      "Epoch 850/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7874e-05 - val_loss: 4.3099e-05\n",
      "Epoch 851/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1843e-05 - val_loss: 9.2837e-05\n",
      "Epoch 852/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2974e-05 - val_loss: 2.8283e-05\n",
      "Epoch 853/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3822e-05 - val_loss: 2.3970e-05\n",
      "Epoch 854/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2448e-05 - val_loss: 2.4018e-05\n",
      "Epoch 855/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2452e-05 - val_loss: 2.3726e-05\n",
      "Epoch 856/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2259e-05 - val_loss: 2.3648e-05\n",
      "Epoch 857/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2241e-05 - val_loss: 2.3695e-05\n",
      "Epoch 858/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2244e-05 - val_loss: 2.3712e-05\n",
      "Epoch 859/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2221e-05 - val_loss: 2.3601e-05\n",
      "Epoch 860/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2252e-05 - val_loss: 2.3626e-05\n",
      "Epoch 861/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2186e-05 - val_loss: 2.3501e-05\n",
      "Epoch 862/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2382e-05 - val_loss: 2.5087e-05\n",
      "Epoch 863/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2500e-05 - val_loss: 2.3624e-05\n",
      "Epoch 864/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2587e-05 - val_loss: 2.4184e-05\n",
      "Epoch 865/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8668e-05 - val_loss: 2.6779e-05\n",
      "Epoch 866/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5449e-05 - val_loss: 2.4213e-05\n",
      "Epoch 867/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2893e-05 - val_loss: 2.3585e-05\n",
      "Epoch 868/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3682e-05 - val_loss: 2.3866e-05\n",
      "Epoch 869/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3128e-05 - val_loss: 2.4796e-05\n",
      "Epoch 870/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2375e-05 - val_loss: 2.3300e-05\n",
      "Epoch 871/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2104e-05 - val_loss: 2.5411e-05\n",
      "Epoch 872/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2560e-05 - val_loss: 2.3352e-05\n",
      "Epoch 873/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3265e-05 - val_loss: 2.3416e-05\n",
      "Epoch 874/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2775e-05 - val_loss: 2.3400e-05\n",
      "Epoch 875/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.2656e-05 - val_loss: 2.4436e-05\n",
      "Epoch 876/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6550e-05 - val_loss: 4.3600e-05\n",
      "Epoch 877/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3023e-05 - val_loss: 4.5111e-05\n",
      "Epoch 878/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1020e-05 - val_loss: 2.5571e-05\n",
      "Epoch 879/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2228e-05 - val_loss: 2.3325e-05\n",
      "Epoch 880/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1928e-05 - val_loss: 2.3317e-05\n",
      "Epoch 881/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2547e-05 - val_loss: 2.3475e-05\n",
      "Epoch 882/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1880e-05 - val_loss: 2.3274e-05\n",
      "Epoch 883/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1919e-05 - val_loss: 2.3263e-05\n",
      "Epoch 884/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2819e-05 - val_loss: 2.4014e-05\n",
      "Epoch 885/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2681e-05 - val_loss: 2.3442e-05\n",
      "Epoch 886/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2074e-05 - val_loss: 2.3155e-05\n",
      "Epoch 887/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2196e-05 - val_loss: 2.3401e-05\n",
      "Epoch 888/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2673e-05 - val_loss: 2.2999e-05\n",
      "Epoch 889/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2051e-05 - val_loss: 2.3157e-05\n",
      "Epoch 890/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.1690e-05 - val_loss: 2.3077e-05\n",
      "Epoch 891/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2364e-05 - val_loss: 2.3793e-05\n",
      "Epoch 892/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2294e-05 - val_loss: 2.3118e-05\n",
      "Epoch 893/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6566e-05 - val_loss: 1.6148e-04\n",
      "Epoch 894/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1759e-04 - val_loss: 0.0027\n",
      "Epoch 895/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6214e-04 - val_loss: 3.6028e-05\n",
      "Epoch 896/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6124e-05 - val_loss: 2.4772e-05\n",
      "Epoch 897/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3025e-05 - val_loss: 2.4168e-05\n",
      "Epoch 898/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2622e-05 - val_loss: 2.3910e-05\n",
      "Epoch 899/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2403e-05 - val_loss: 2.3729e-05\n",
      "Epoch 900/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2276e-05 - val_loss: 2.3608e-05\n",
      "Epoch 901/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2196e-05 - val_loss: 2.3538e-05\n",
      "Epoch 902/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2149e-05 - val_loss: 2.3539e-05\n",
      "Epoch 903/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2118e-05 - val_loss: 2.3502e-05\n",
      "Epoch 904/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2100e-05 - val_loss: 2.3497e-05\n",
      "Epoch 905/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2079e-05 - val_loss: 2.3473e-05\n",
      "Epoch 906/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2067e-05 - val_loss: 2.3450e-05\n",
      "Epoch 907/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2054e-05 - val_loss: 2.3460e-05\n",
      "Epoch 908/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2053e-05 - val_loss: 2.3430e-05\n",
      "Epoch 909/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2041e-05 - val_loss: 2.3417e-05\n",
      "Epoch 910/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2034e-05 - val_loss: 2.3383e-05\n",
      "Epoch 911/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2032e-05 - val_loss: 2.3470e-05\n",
      "Epoch 912/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2027e-05 - val_loss: 2.3400e-05\n",
      "Epoch 913/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2004e-05 - val_loss: 2.3396e-05\n",
      "Epoch 914/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2011e-05 - val_loss: 2.3383e-05\n",
      "Epoch 915/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1988e-05 - val_loss: 2.3371e-05\n",
      "Epoch 916/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1980e-05 - val_loss: 2.3359e-05\n",
      "Epoch 917/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1977e-05 - val_loss: 2.3375e-05\n",
      "Epoch 918/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1968e-05 - val_loss: 2.3366e-05\n",
      "Epoch 919/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1955e-05 - val_loss: 2.3317e-05\n",
      "Epoch 920/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1960e-05 - val_loss: 2.3325e-05\n",
      "Epoch 921/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1942e-05 - val_loss: 2.3348e-05\n",
      "Epoch 922/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1927e-05 - val_loss: 2.3290e-05\n",
      "Epoch 923/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1901e-05 - val_loss: 2.3287e-05\n",
      "Epoch 924/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.2055e-05 - val_loss: 2.3272e-05\n",
      "Epoch 925/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1906e-05 - val_loss: 2.3325e-05\n",
      "Epoch 926/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2064e-05 - val_loss: 2.3272e-05\n",
      "Epoch 927/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1926e-05 - val_loss: 2.3345e-05\n",
      "Epoch 928/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1882e-05 - val_loss: 2.3362e-05\n",
      "Epoch 929/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2122e-05 - val_loss: 2.3792e-05\n",
      "Epoch 930/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2118e-05 - val_loss: 2.3286e-05\n",
      "Epoch 931/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1881e-05 - val_loss: 2.3313e-05\n",
      "Epoch 932/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1903e-05 - val_loss: 2.3206e-05\n",
      "Epoch 933/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2052e-05 - val_loss: 2.3627e-05\n",
      "Epoch 934/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1923e-05 - val_loss: 2.3169e-05\n",
      "Epoch 935/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2099e-05 - val_loss: 2.3296e-05\n",
      "Epoch 936/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1828e-05 - val_loss: 2.3193e-05\n",
      "Epoch 937/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1759e-05 - val_loss: 2.3162e-05\n",
      "Epoch 938/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2172e-05 - val_loss: 2.4243e-05\n",
      "Epoch 939/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2336e-05 - val_loss: 2.7497e-05\n",
      "Epoch 940/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4099e-05 - val_loss: 2.3816e-05\n",
      "Epoch 941/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2116e-05 - val_loss: 2.3247e-05\n",
      "Epoch 942/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3465e-05 - val_loss: 3.1598e-05\n",
      "Epoch 943/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4328e-05 - val_loss: 1.4935e-04\n",
      "Epoch 944/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6752e-05 - val_loss: 2.8031e-05\n",
      "Epoch 945/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0270e-05 - val_loss: 2.4424e-05\n",
      "Epoch 946/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2440e-05 - val_loss: 2.7080e-05\n",
      "Epoch 947/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1394e-05 - val_loss: 2.3950e-05\n",
      "Epoch 948/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0160e-05 - val_loss: 2.8228e-05\n",
      "Epoch 949/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7711e-05 - val_loss: 2.4909e-05\n",
      "Epoch 950/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3730e-05 - val_loss: 1.2358e-04\n",
      "Epoch 951/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2845e-05 - val_loss: 2.3759e-05\n",
      "Epoch 952/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1871e-05 - val_loss: 2.3164e-05\n",
      "Epoch 953/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1632e-05 - val_loss: 2.2916e-05\n",
      "Epoch 954/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1565e-05 - val_loss: 2.2969e-05\n",
      "Epoch 955/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1627e-05 - val_loss: 2.4411e-05\n",
      "Epoch 956/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3266e-05 - val_loss: 2.8113e-05\n",
      "Epoch 957/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2010e-05 - val_loss: 2.3056e-05\n",
      "Epoch 958/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2510e-05 - val_loss: 2.4239e-05\n",
      "Epoch 959/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2315e-05 - val_loss: 2.2653e-05\n",
      "Epoch 960/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1651e-05 - val_loss: 2.3874e-05\n",
      "Epoch 961/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4556e-05 - val_loss: 3.2662e-05\n",
      "Epoch 962/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3015e-05 - val_loss: 3.5663e-04\n",
      "Epoch 963/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7288e-04 - val_loss: 9.9900e-05\n",
      "Epoch 964/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4493e-05 - val_loss: 2.7386e-05\n",
      "Epoch 965/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3119e-05 - val_loss: 2.3316e-05\n",
      "Epoch 966/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1763e-05 - val_loss: 2.3037e-05\n",
      "Epoch 967/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1615e-05 - val_loss: 2.2924e-05\n",
      "Epoch 968/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1576e-05 - val_loss: 2.2940e-05\n",
      "Epoch 969/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.1572e-05 - val_loss: 2.2930e-05\n",
      "Epoch 970/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1569e-05 - val_loss: 2.2916e-05\n",
      "Epoch 971/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1578e-05 - val_loss: 2.2934e-05\n",
      "Epoch 972/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1540e-05 - val_loss: 2.2902e-05\n",
      "Epoch 973/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1549e-05 - val_loss: 2.2892e-05\n",
      "Epoch 974/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1534e-05 - val_loss: 2.2863e-05\n",
      "Epoch 975/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1496e-05 - val_loss: 2.2891e-05\n",
      "Epoch 976/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1524e-05 - val_loss: 2.2912e-05\n",
      "Epoch 977/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1473e-05 - val_loss: 2.2849e-05\n",
      "Epoch 978/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1458e-05 - val_loss: 2.2818e-05\n",
      "Epoch 979/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1459e-05 - val_loss: 2.2797e-05\n",
      "Epoch 980/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1465e-05 - val_loss: 2.3270e-05\n",
      "Epoch 981/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1497e-05 - val_loss: 2.2774e-05\n",
      "Epoch 982/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1536e-05 - val_loss: 2.2784e-05\n",
      "Epoch 983/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1501e-05 - val_loss: 2.2784e-05\n",
      "Epoch 984/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1407e-05 - val_loss: 2.2707e-05\n",
      "Epoch 985/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1385e-05 - val_loss: 2.2727e-05\n",
      "Epoch 986/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1425e-05 - val_loss: 2.2673e-05\n",
      "Epoch 987/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1358e-05 - val_loss: 2.2680e-05\n",
      "Epoch 988/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1409e-05 - val_loss: 2.3288e-05\n",
      "Epoch 989/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.1405e-05 - val_loss: 2.2613e-05\n",
      "Epoch 990/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1276e-05 - val_loss: 2.2633e-05\n",
      "Epoch 991/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1224e-05 - val_loss: 2.2589e-05\n",
      "Epoch 992/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1744e-05 - val_loss: 2.2956e-05\n",
      "Epoch 993/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1324e-05 - val_loss: 2.2436e-05\n",
      "Epoch 994/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1188e-05 - val_loss: 2.2598e-05\n",
      "Epoch 995/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1618e-05 - val_loss: 2.2734e-05\n",
      "Epoch 996/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1524e-05 - val_loss: 2.5493e-05\n",
      "Epoch 997/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8228e-05 - val_loss: 4.5215e-05\n",
      "Epoch 998/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4461e-04 - val_loss: 2.9031e-05\n",
      "Epoch 999/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7325e-05 - val_loss: 3.7707e-05\n",
      "Epoch 1000/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1959e-05 - val_loss: 2.2485e-05\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Load the data\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "extreme_data=df[df[\"rainfall\"]>50]\n",
    "normal_data=df[df[\"rainfall\"]<=50]\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "def create_model(reg_strength):\n",
    "    input_layer = Input(shape=(3,))\n",
    "    encoded = Dense(16, activation='relu', kernel_regularizer='l2', bias_regularizer='l2', activity_regularizer='l2', kernel_initializer='he_uniform')(input_layer)\n",
    "    decoded = Dense(3, activation=None, kernel_initializer='he_uniform')(encoded)\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='logcosh', metrics=['mse'])\n",
    "    return autoencoder\n",
    "\n",
    "# Create the KerasRegressor\n",
    "keras_reg = KerasRegressor(build_fn=create_model)\n",
    "\n",
    "# Define the grid search parameters\n",
    "reg_strengths = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = dict(reg_strength=reg_strengths)\n",
    "\n",
    "# Perform the grid search\n",
    "grid = GridSearchCV(estimator=keras_reg, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_result = grid.fit(normal_data[[\"windspeed_tree\",\"tpw_tree\",\"rainfall_tree\"]].values, normal_data[[\"windspeed_tree\",\"tpw_tree\",\"rainfall_tree\"]].values)\n",
    "\n",
    "# Print the best regularization strength found\n",
    "print(\"Best reg_strength: {:.3f} using {}\".format(grid_result.best_params_['reg_strength'], grid_result.best_params_))\n",
    "\n",
    "# Define the autoencoder architecture with the best regularization strength found by grid search\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu', kernel_regularizer='l2', bias_regularizer='l2', activity_regularizer='l2', kernel_initializer='he_uniform')(input_layer)\n",
    "decoded = Dense(3, activation=None, kernel_initializer='he_uniform')(encoded)\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='logcosh')\n",
    "\n",
    "# Train the model\n",
    "train_data = normal_data[[\"windspeed_tree\",\"tpw_tree\",\"rainfall_tree\"]].values\n",
    "history = autoencoder.fit(train_data, train_data, epochs=1000, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = df[[\"windspeed_tree\",\"tpw_tree\",\"rainfall_tree\"]].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "df['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "df['error'] = np.abs(df['predicted_rainfall'] - df['rainfall'])\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "threshold =50\n",
    "df['rainfall_class'] = np.where(df['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "df['actual_rainfall_class'] = np.where(df['rainfall'] > 50, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(df[df['rainfall_class'] == df['actual_rainfall_class']])\n",
    "num_total = len(df)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc46ee7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>rainfall_tree</th>\n",
       "      <th>windspeed_tree</th>\n",
       "      <th>tpw_tree</th>\n",
       "      <th>predicted_rainfall</th>\n",
       "      <th>error</th>\n",
       "      <th>rainfall_class</th>\n",
       "      <th>actual_rainfall_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/1/2000</td>\n",
       "      <td>12.245595</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>4.688589</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>4.685196</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/2/2000</td>\n",
       "      <td>12.825491</td>\n",
       "      <td>0.044874</td>\n",
       "      <td>11.330589</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>11.327197</td>\n",
       "      <td>Extreme</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/3/2000</td>\n",
       "      <td>12.921664</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>6.894713</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>6.891321</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/4/2000</td>\n",
       "      <td>15.149001</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>4.613324</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>4.609932</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/5/2000</td>\n",
       "      <td>18.495907</td>\n",
       "      <td>0.139491</td>\n",
       "      <td>8.111635</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>8.108243</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>9/26/2020</td>\n",
       "      <td>5.577215</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>4.467977</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>4.464584</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>9/27/2020</td>\n",
       "      <td>5.184293</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>1.572182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>1.568789</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>9/28/2020</td>\n",
       "      <td>4.469007</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>1.819019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>1.815627</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>9/29/2020</td>\n",
       "      <td>4.259090</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>2.198017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>2.194625</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>9/30/2020</td>\n",
       "      <td>5.513838</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>12.713634</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>12.710241</td>\n",
       "      <td>Extreme</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2562 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall  rainfall_tree  \\\n",
       "0      6/1/2000  12.245595  0.033285   4.688589              0   \n",
       "1      6/2/2000  12.825491  0.044874  11.330589              0   \n",
       "2      6/3/2000  12.921664  0.010243   6.894713              0   \n",
       "3      6/4/2000  15.149001  0.036881   4.613324              0   \n",
       "4      6/5/2000  18.495907  0.139491   8.111635              0   \n",
       "...         ...        ...       ...        ...            ...   \n",
       "2557  9/26/2020   5.577215  0.009963   4.467977              0   \n",
       "2558  9/27/2020   5.184293  0.002341   1.572182              0   \n",
       "2559  9/28/2020   4.469007  0.000867   1.819019              0   \n",
       "2560  9/29/2020   4.259090  0.001416   2.198017              0   \n",
       "2561  9/30/2020   5.513838  0.002474  12.713634              0   \n",
       "\n",
       "      windspeed_tree  tpw_tree  predicted_rainfall      error rainfall_class  \\\n",
       "0                  0         0            0.003392   4.685196         Normal   \n",
       "1                  0         0            0.003392  11.327197        Extreme   \n",
       "2                  0         0            0.003392   6.891321         Normal   \n",
       "3                  0         0            0.003392   4.609932         Normal   \n",
       "4                  0         0            0.003392   8.108243         Normal   \n",
       "...              ...       ...                 ...        ...            ...   \n",
       "2557               0         0            0.003392   4.464584         Normal   \n",
       "2558               0         0            0.003392   1.568789         Normal   \n",
       "2559               0         0            0.003392   1.815627         Normal   \n",
       "2560               0         0            0.003392   2.194625         Normal   \n",
       "2561               0         0            0.003392  12.710241        Extreme   \n",
       "\n",
       "     actual_rainfall_class  \n",
       "0                   Normal  \n",
       "1                   Normal  \n",
       "2                   Normal  \n",
       "3                   Normal  \n",
       "4                   Normal  \n",
       "...                    ...  \n",
       "2557                Normal  \n",
       "2558                Normal  \n",
       "2559                Normal  \n",
       "2560                Normal  \n",
       "2561                Normal  \n",
       "\n",
       "[2562 rows x 11 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fc47922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [date, windspeed, tpw, rainfall, rainfall_tree, windspeed_tree, tpw_tree, predicted_rainfall, error, rainfall_class, actual_rainfall_class]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = df[df['rainfall_class'] != df['actual_rainfall_class']]\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "feaf9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df=df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "afb2e09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.5\n",
      "Lowest error: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Define a range of threshold values to try\n",
    "thresholds = np.arange(0.5, 10.0, 0.1)\n",
    "\n",
    "# Initialize variables to store the best threshold and the lowest error\n",
    "best_threshold = 0.0\n",
    "lowest_error = float('inf')\n",
    "\n",
    "# Loop over the threshold values and calculate the error for each\n",
    "for threshold in thresholds:\n",
    "    # Classify the rainfall data into normal and extreme based on the threshold\n",
    "    pre_df['rainfall_class'] = np.where(pre_df['error'] > threshold, 'extreme', 'normal')\n",
    "\n",
    "    # Calculate the accuracy of the classification\n",
    "    accuracy = sum(pre_df['rainfall_class'] == pre_df['actual_rainfall_class']) / len(data)\n",
    "\n",
    "    # Calculate the error of the classification\n",
    "    error = 1 - accuracy\n",
    "\n",
    "    # Update the best threshold and lowest error if necessary\n",
    "    if error < lowest_error:\n",
    "        best_threshold = threshold\n",
    "        lowest_error = error\n",
    "\n",
    "# Print the best threshold and lowest error\n",
    "print('Best threshold:', best_threshold)\n",
    "print('Lowest error:', lowest_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "abaf27aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shine\\AppData\\Local\\Temp\\ipykernel_16652\\2631512346.py:25: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 3ms/step - loss: 0.3473 - mse: 0.1514\n",
      "Best reg_strength: 0.010 using {'reg_strength': 0.01}\n",
      "Epoch 1/1000\n",
      "64/64 [==============================] - 1s 7ms/step - loss: 0.3370 - val_loss: 0.3117\n",
      "Epoch 2/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2874 - val_loss: 0.2704\n",
      "Epoch 3/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2503 - val_loss: 0.2355\n",
      "Epoch 4/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2179 - val_loss: 0.2049\n",
      "Epoch 5/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1895 - val_loss: 0.1779\n",
      "Epoch 6/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1644 - val_loss: 0.1541\n",
      "Epoch 7/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1422 - val_loss: 0.1332\n",
      "Epoch 8/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1228 - val_loss: 0.1148\n",
      "Epoch 9/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1057 - val_loss: 0.0987\n",
      "Epoch 10/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0907 - val_loss: 0.0847\n",
      "Epoch 11/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0777 - val_loss: 0.0724\n",
      "Epoch 12/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0663 - val_loss: 0.0618\n",
      "Epoch 13/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0565 - val_loss: 0.0526\n",
      "Epoch 14/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0480 - val_loss: 0.0447\n",
      "Epoch 15/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.0379\n",
      "Epoch 16/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0321\n",
      "Epoch 17/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0272\n",
      "Epoch 18/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0229\n",
      "Epoch 19/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0194\n",
      "Epoch 20/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0164\n",
      "Epoch 21/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0139\n",
      "Epoch 22/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.0118\n",
      "Epoch 23/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0100\n",
      "Epoch 24/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0085\n",
      "Epoch 25/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 26/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 27/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 28/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0048\n",
      "Epoch 29/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0043\n",
      "Epoch 30/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 31/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0034\n",
      "Epoch 32/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0031\n",
      "Epoch 33/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 34/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 35/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 36/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 37/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 38/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 39/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 40/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 41/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 42/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 43/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 44/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 45/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 46/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 47/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 48/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 49/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 50/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 51/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 52/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 53/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 54/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 55/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 56/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 57/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 58/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 59/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9711e-04 - val_loss: 0.0012\n",
      "Epoch 60/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6937e-04 - val_loss: 0.0012\n",
      "Epoch 61/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.5548e-04 - val_loss: 0.0012\n",
      "Epoch 62/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3550e-04 - val_loss: 0.0012\n",
      "Epoch 63/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1901e-04 - val_loss: 0.0011\n",
      "Epoch 64/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.0394e-04 - val_loss: 0.0011\n",
      "Epoch 65/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8128e-04 - val_loss: 0.0011\n",
      "Epoch 66/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6478e-04 - val_loss: 0.0011\n",
      "Epoch 67/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5303e-04 - val_loss: 0.0011\n",
      "Epoch 68/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.3901e-04 - val_loss: 0.0010\n",
      "Epoch 69/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.2757e-04 - val_loss: 0.0010\n",
      "Epoch 70/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0947e-04 - val_loss: 0.0010\n",
      "Epoch 71/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9455e-04 - val_loss: 9.5065e-04\n",
      "Epoch 72/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7460e-04 - val_loss: 0.0011\n",
      "Epoch 73/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7793e-04 - val_loss: 9.0774e-04\n",
      "Epoch 74/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.5111e-04 - val_loss: 9.2447e-04\n",
      "Epoch 75/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3741e-04 - val_loss: 8.9356e-04\n",
      "Epoch 76/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.3206e-04 - val_loss: 8.7749e-04\n",
      "Epoch 77/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1920e-04 - val_loss: 9.0751e-04\n",
      "Epoch 78/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0567e-04 - val_loss: 8.7600e-04\n",
      "Epoch 79/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9516e-04 - val_loss: 9.1359e-04\n",
      "Epoch 80/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8132e-04 - val_loss: 9.1544e-04\n",
      "Epoch 81/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6670e-04 - val_loss: 7.8427e-04\n",
      "Epoch 82/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6521e-04 - val_loss: 8.1441e-04\n",
      "Epoch 83/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4484e-04 - val_loss: 7.8881e-04\n",
      "Epoch 84/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.3693e-04 - val_loss: 7.9148e-04\n",
      "Epoch 85/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2316e-04 - val_loss: 7.6702e-04\n",
      "Epoch 86/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1245e-04 - val_loss: 7.8479e-04\n",
      "Epoch 87/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9940e-04 - val_loss: 6.9502e-04\n",
      "Epoch 88/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8706e-04 - val_loss: 7.4555e-04\n",
      "Epoch 89/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7809e-04 - val_loss: 6.8410e-04\n",
      "Epoch 90/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7832e-04 - val_loss: 7.2126e-04\n",
      "Epoch 91/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5555e-04 - val_loss: 7.0969e-04\n",
      "Epoch 92/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4674e-04 - val_loss: 6.4396e-04\n",
      "Epoch 93/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4557e-04 - val_loss: 6.5050e-04\n",
      "Epoch 94/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2969e-04 - val_loss: 7.0224e-04\n",
      "Epoch 95/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2621e-04 - val_loss: 6.9063e-04\n",
      "Epoch 96/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.1805e-04 - val_loss: 6.1679e-04\n",
      "Epoch 97/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1274e-04 - val_loss: 6.2840e-04\n",
      "Epoch 98/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0179e-04 - val_loss: 5.8269e-04\n",
      "Epoch 99/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9524e-04 - val_loss: 6.0918e-04\n",
      "Epoch 100/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8415e-04 - val_loss: 6.0187e-04\n",
      "Epoch 101/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7622e-04 - val_loss: 5.9990e-04\n",
      "Epoch 102/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7282e-04 - val_loss: 5.3963e-04\n",
      "Epoch 103/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7160e-04 - val_loss: 5.4712e-04\n",
      "Epoch 104/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5380e-04 - val_loss: 5.7032e-04\n",
      "Epoch 105/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6005e-04 - val_loss: 5.2178e-04\n",
      "Epoch 106/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4624e-04 - val_loss: 5.5325e-04\n",
      "Epoch 107/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.5284e-04 - val_loss: 5.4932e-04\n",
      "Epoch 108/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3077e-04 - val_loss: 5.5629e-04\n",
      "Epoch 109/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2124e-04 - val_loss: 5.5582e-04\n",
      "Epoch 110/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.2205e-04 - val_loss: 5.6682e-04\n",
      "Epoch 111/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0864e-04 - val_loss: 5.7169e-04\n",
      "Epoch 112/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0763e-04 - val_loss: 5.2168e-04\n",
      "Epoch 113/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.9927e-04 - val_loss: 5.3138e-04\n",
      "Epoch 114/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9410e-04 - val_loss: 4.8384e-04\n",
      "Epoch 115/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8751e-04 - val_loss: 4.8112e-04\n",
      "Epoch 116/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.8411e-04 - val_loss: 4.7785e-04\n",
      "Epoch 117/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7615e-04 - val_loss: 4.7407e-04\n",
      "Epoch 118/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7053e-04 - val_loss: 4.2671e-04\n",
      "Epoch 119/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6944e-04 - val_loss: 4.3316e-04\n",
      "Epoch 120/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.6228e-04 - val_loss: 4.5944e-04\n",
      "Epoch 121/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6275e-04 - val_loss: 4.1651e-04\n",
      "Epoch 122/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5661e-04 - val_loss: 4.3145e-04\n",
      "Epoch 123/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5611e-04 - val_loss: 4.0455e-04\n",
      "Epoch 124/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4662e-04 - val_loss: 4.0380e-04\n",
      "Epoch 125/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4648e-04 - val_loss: 4.1394e-04\n",
      "Epoch 126/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3640e-04 - val_loss: 4.7076e-04\n",
      "Epoch 127/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3897e-04 - val_loss: 3.8920e-04\n",
      "Epoch 128/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.3231e-04 - val_loss: 4.1301e-04\n",
      "Epoch 129/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2383e-04 - val_loss: 4.5783e-04\n",
      "Epoch 130/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2144e-04 - val_loss: 3.6647e-04\n",
      "Epoch 131/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2211e-04 - val_loss: 4.3419e-04\n",
      "Epoch 132/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1614e-04 - val_loss: 3.7383e-04\n",
      "Epoch 133/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.2078e-04 - val_loss: 3.8529e-04\n",
      "Epoch 134/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0959e-04 - val_loss: 4.0064e-04\n",
      "Epoch 135/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1013e-04 - val_loss: 3.8108e-04\n",
      "Epoch 136/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0222e-04 - val_loss: 4.0021e-04\n",
      "Epoch 137/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0974e-04 - val_loss: 3.6128e-04\n",
      "Epoch 138/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9719e-04 - val_loss: 3.4860e-04\n",
      "Epoch 139/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9735e-04 - val_loss: 3.5291e-04\n",
      "Epoch 140/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8862e-04 - val_loss: 3.5638e-04\n",
      "Epoch 141/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8299e-04 - val_loss: 4.3807e-04\n",
      "Epoch 142/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8646e-04 - val_loss: 3.3738e-04\n",
      "Epoch 143/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7802e-04 - val_loss: 3.8701e-04\n",
      "Epoch 144/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.7899e-04 - val_loss: 3.1540e-04\n",
      "Epoch 145/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7312e-04 - val_loss: 3.1382e-04\n",
      "Epoch 146/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7968e-04 - val_loss: 3.0324e-04\n",
      "Epoch 147/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6769e-04 - val_loss: 2.9602e-04\n",
      "Epoch 148/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7377e-04 - val_loss: 2.9485e-04\n",
      "Epoch 149/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6330e-04 - val_loss: 3.5355e-04\n",
      "Epoch 150/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.6230e-04 - val_loss: 2.9427e-04\n",
      "Epoch 151/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.7155e-04 - val_loss: 3.0030e-04\n",
      "Epoch 152/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5726e-04 - val_loss: 2.8440e-04\n",
      "Epoch 153/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5349e-04 - val_loss: 3.1157e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5020e-04 - val_loss: 2.7426e-04\n",
      "Epoch 155/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5474e-04 - val_loss: 2.8349e-04\n",
      "Epoch 156/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5889e-04 - val_loss: 2.7271e-04\n",
      "Epoch 157/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.5443e-04 - val_loss: 2.7591e-04\n",
      "Epoch 158/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.4772e-04 - val_loss: 2.8578e-04\n",
      "Epoch 159/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3841e-04 - val_loss: 2.6616e-04\n",
      "Epoch 160/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3835e-04 - val_loss: 2.8414e-04\n",
      "Epoch 161/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3934e-04 - val_loss: 2.7779e-04\n",
      "Epoch 162/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3387e-04 - val_loss: 2.9093e-04\n",
      "Epoch 163/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3222e-04 - val_loss: 2.5351e-04\n",
      "Epoch 164/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3032e-04 - val_loss: 2.6356e-04\n",
      "Epoch 165/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3246e-04 - val_loss: 2.7358e-04\n",
      "Epoch 166/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.3484e-04 - val_loss: 2.7258e-04\n",
      "Epoch 167/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2418e-04 - val_loss: 2.5558e-04\n",
      "Epoch 168/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.2197e-04 - val_loss: 2.4245e-04\n",
      "Epoch 169/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1931e-04 - val_loss: 2.8037e-04\n",
      "Epoch 170/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1645e-04 - val_loss: 2.6474e-04\n",
      "Epoch 171/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1772e-04 - val_loss: 2.4963e-04\n",
      "Epoch 172/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.1254e-04 - val_loss: 2.6806e-04\n",
      "Epoch 173/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.0851e-04 - val_loss: 2.9832e-04\n",
      "Epoch 174/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.0772e-04 - val_loss: 2.6408e-04\n",
      "Epoch 175/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.0663e-04 - val_loss: 3.0902e-04\n",
      "Epoch 176/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1186e-04 - val_loss: 2.4644e-04\n",
      "Epoch 177/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1780e-04 - val_loss: 2.4703e-04\n",
      "Epoch 178/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.0906e-04 - val_loss: 2.3267e-04\n",
      "Epoch 179/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.1718e-04 - val_loss: 2.2978e-04\n",
      "Epoch 180/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.0042e-04 - val_loss: 2.1773e-04\n",
      "Epoch 181/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.9194e-04 - val_loss: 2.7951e-04\n",
      "Epoch 182/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.0289e-04 - val_loss: 2.3359e-04\n",
      "Epoch 183/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.9301e-04 - val_loss: 2.6259e-04\n",
      "Epoch 184/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.9574e-04 - val_loss: 2.2138e-04\n",
      "Epoch 185/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.9198e-04 - val_loss: 2.1075e-04\n",
      "Epoch 186/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.8534e-04 - val_loss: 2.7739e-04\n",
      "Epoch 187/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.9058e-04 - val_loss: 2.2678e-04\n",
      "Epoch 188/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.9406e-04 - val_loss: 2.4625e-04\n",
      "Epoch 189/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.8769e-04 - val_loss: 2.2558e-04\n",
      "Epoch 190/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.9148e-04 - val_loss: 2.1794e-04\n",
      "Epoch 191/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.8721e-04 - val_loss: 1.9466e-04\n",
      "Epoch 192/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.8416e-04 - val_loss: 1.9916e-04\n",
      "Epoch 193/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.8845e-04 - val_loss: 2.0007e-04\n",
      "Epoch 194/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.8115e-04 - val_loss: 2.0108e-04\n",
      "Epoch 195/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.7822e-04 - val_loss: 1.9013e-04\n",
      "Epoch 196/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.8245e-04 - val_loss: 2.2139e-04\n",
      "Epoch 197/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.8138e-04 - val_loss: 1.8577e-04\n",
      "Epoch 198/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.7195e-04 - val_loss: 1.8291e-04\n",
      "Epoch 199/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.7156e-04 - val_loss: 1.9381e-04\n",
      "Epoch 200/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.7991e-04 - val_loss: 1.9548e-04\n",
      "Epoch 201/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6933e-04 - val_loss: 2.0365e-04\n",
      "Epoch 202/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6942e-04 - val_loss: 1.7947e-04\n",
      "Epoch 203/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.6296e-04 - val_loss: 2.3622e-04\n",
      "Epoch 204/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6515e-04 - val_loss: 1.8314e-04\n",
      "Epoch 205/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6405e-04 - val_loss: 2.0901e-04\n",
      "Epoch 206/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.6717e-04 - val_loss: 2.1302e-04\n",
      "Epoch 207/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6391e-04 - val_loss: 1.9899e-04\n",
      "Epoch 208/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.7118e-04 - val_loss: 1.9579e-04\n",
      "Epoch 209/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.6972e-04 - val_loss: 2.1044e-04\n",
      "Epoch 210/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6120e-04 - val_loss: 1.6951e-04\n",
      "Epoch 211/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6314e-04 - val_loss: 1.7137e-04\n",
      "Epoch 212/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.5352e-04 - val_loss: 2.2558e-04\n",
      "Epoch 213/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.5943e-04 - val_loss: 1.8621e-04\n",
      "Epoch 214/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6861e-04 - val_loss: 1.8218e-04\n",
      "Epoch 215/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6198e-04 - val_loss: 1.6650e-04\n",
      "Epoch 216/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.4735e-04 - val_loss: 1.8863e-04\n",
      "Epoch 217/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.5825e-04 - val_loss: 1.9278e-04\n",
      "Epoch 218/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.7403e-04 - val_loss: 1.6785e-04\n",
      "Epoch 219/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4638e-04 - val_loss: 1.6269e-04\n",
      "Epoch 220/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4989e-04 - val_loss: 1.5671e-04\n",
      "Epoch 221/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4626e-04 - val_loss: 1.5937e-04\n",
      "Epoch 222/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.6011e-04 - val_loss: 1.8045e-04\n",
      "Epoch 223/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.5931e-04 - val_loss: 1.5423e-04\n",
      "Epoch 224/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4241e-04 - val_loss: 1.7339e-04\n",
      "Epoch 225/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.4286e-04 - val_loss: 1.6687e-04\n",
      "Epoch 226/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4336e-04 - val_loss: 1.5116e-04\n",
      "Epoch 227/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4521e-04 - val_loss: 1.5143e-04\n",
      "Epoch 228/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4140e-04 - val_loss: 1.5147e-04\n",
      "Epoch 229/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.3912e-04 - val_loss: 1.6277e-04\n",
      "Epoch 230/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.5504e-04 - val_loss: 1.6862e-04\n",
      "Epoch 231/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.4257e-04 - val_loss: 1.5644e-04\n",
      "Epoch 232/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3876e-04 - val_loss: 1.4594e-04\n",
      "Epoch 233/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3631e-04 - val_loss: 1.4294e-04\n",
      "Epoch 234/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3836e-04 - val_loss: 1.4461e-04\n",
      "Epoch 235/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3509e-04 - val_loss: 1.4433e-04\n",
      "Epoch 236/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3285e-04 - val_loss: 1.9147e-04\n",
      "Epoch 237/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3778e-04 - val_loss: 1.4292e-04\n",
      "Epoch 238/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3398e-04 - val_loss: 1.4001e-04\n",
      "Epoch 239/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2760e-04 - val_loss: 1.8018e-04\n",
      "Epoch 240/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.4621e-04 - val_loss: 1.6855e-04\n",
      "Epoch 241/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3348e-04 - val_loss: 1.5623e-04\n",
      "Epoch 242/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.3161e-04 - val_loss: 1.4589e-04\n",
      "Epoch 243/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2973e-04 - val_loss: 1.3926e-04\n",
      "Epoch 244/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3017e-04 - val_loss: 1.4560e-04\n",
      "Epoch 245/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3020e-04 - val_loss: 1.4300e-04\n",
      "Epoch 246/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.2614e-04 - val_loss: 1.4121e-04\n",
      "Epoch 247/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2957e-04 - val_loss: 1.3685e-04\n",
      "Epoch 248/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.2204e-04 - val_loss: 1.5359e-04\n",
      "Epoch 249/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2189e-04 - val_loss: 1.5531e-04\n",
      "Epoch 250/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2422e-04 - val_loss: 1.2942e-04\n",
      "Epoch 251/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3577e-04 - val_loss: 1.6121e-04\n",
      "Epoch 252/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.2529e-04 - val_loss: 1.3950e-04\n",
      "Epoch 253/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.2121e-04 - val_loss: 1.2954e-04\n",
      "Epoch 254/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3791e-04 - val_loss: 1.3206e-04\n",
      "Epoch 255/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2417e-04 - val_loss: 1.2833e-04\n",
      "Epoch 256/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1998e-04 - val_loss: 1.5497e-04\n",
      "Epoch 257/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1994e-04 - val_loss: 1.2608e-04\n",
      "Epoch 258/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2599e-04 - val_loss: 1.4255e-04\n",
      "Epoch 259/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1830e-04 - val_loss: 1.3461e-04\n",
      "Epoch 260/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.1890e-04 - val_loss: 1.3651e-04\n",
      "Epoch 261/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2328e-04 - val_loss: 1.7469e-04\n",
      "Epoch 262/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.3593e-04 - val_loss: 1.4013e-04\n",
      "Epoch 263/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1767e-04 - val_loss: 1.3290e-04\n",
      "Epoch 264/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1484e-04 - val_loss: 1.2814e-04\n",
      "Epoch 265/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1233e-04 - val_loss: 1.4173e-04\n",
      "Epoch 266/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.2110e-04 - val_loss: 1.4031e-04\n",
      "Epoch 267/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.2446e-04 - val_loss: 1.3434e-04\n",
      "Epoch 268/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.1532e-04 - val_loss: 1.2465e-04\n",
      "Epoch 269/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1808e-04 - val_loss: 1.4187e-04\n",
      "Epoch 270/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1602e-04 - val_loss: 1.3220e-04\n",
      "Epoch 271/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.1247e-04 - val_loss: 1.1666e-04\n",
      "Epoch 272/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0744e-04 - val_loss: 1.3285e-04\n",
      "Epoch 273/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.1365e-04 - val_loss: 1.3028e-04\n",
      "Epoch 274/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0978e-04 - val_loss: 1.2022e-04\n",
      "Epoch 275/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.1382e-04 - val_loss: 1.2367e-04\n",
      "Epoch 276/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.1391e-04 - val_loss: 1.1652e-04\n",
      "Epoch 277/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.1322e-04 - val_loss: 1.3562e-04\n",
      "Epoch 278/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.1163e-04 - val_loss: 1.5066e-04\n",
      "Epoch 279/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.1626e-04 - val_loss: 1.1679e-04\n",
      "Epoch 280/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0666e-04 - val_loss: 1.1500e-04\n",
      "Epoch 281/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0302e-04 - val_loss: 1.4221e-04\n",
      "Epoch 282/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0747e-04 - val_loss: 1.2484e-04\n",
      "Epoch 283/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0390e-04 - val_loss: 1.4075e-04\n",
      "Epoch 284/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0786e-04 - val_loss: 1.6208e-04\n",
      "Epoch 285/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0840e-04 - val_loss: 1.0854e-04\n",
      "Epoch 286/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0330e-04 - val_loss: 1.1884e-04\n",
      "Epoch 287/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0867e-04 - val_loss: 1.1166e-04\n",
      "Epoch 288/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0392e-04 - val_loss: 1.2289e-04\n",
      "Epoch 289/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0600e-04 - val_loss: 1.1471e-04\n",
      "Epoch 290/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0259e-04 - val_loss: 1.0381e-04\n",
      "Epoch 291/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.8896e-05 - val_loss: 1.0654e-04\n",
      "Epoch 292/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0185e-04 - val_loss: 1.0589e-04\n",
      "Epoch 293/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0241e-04 - val_loss: 1.0631e-04\n",
      "Epoch 294/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0018e-04 - val_loss: 1.3759e-04\n",
      "Epoch 295/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0176e-04 - val_loss: 1.1051e-04\n",
      "Epoch 296/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0011e-04 - val_loss: 1.0449e-04\n",
      "Epoch 297/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0300e-04 - val_loss: 1.0821e-04\n",
      "Epoch 298/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.8999e-05 - val_loss: 1.1127e-04\n",
      "Epoch 299/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6799e-05 - val_loss: 1.0937e-04\n",
      "Epoch 300/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.5597e-05 - val_loss: 1.0060e-04\n",
      "Epoch 301/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.8324e-05 - val_loss: 1.3715e-04\n",
      "Epoch 302/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9206e-05 - val_loss: 1.1001e-04\n",
      "Epoch 303/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.4215e-05 - val_loss: 1.0787e-04\n",
      "Epoch 304/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.8996e-05 - val_loss: 1.0842e-04\n",
      "Epoch 305/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6236e-05 - val_loss: 1.1062e-04\n",
      "Epoch 306/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0120e-04 - val_loss: 1.2490e-04\n",
      "Epoch 307/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0749e-04 - val_loss: 1.2746e-04\n",
      "Epoch 308/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.1150e-04 - val_loss: 1.0357e-04\n",
      "Epoch 309/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.3531e-05 - val_loss: 9.8146e-05\n",
      "Epoch 310/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3977e-05 - val_loss: 1.1845e-04\n",
      "Epoch 311/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.3937e-05 - val_loss: 9.4417e-05\n",
      "Epoch 312/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.7861e-05 - val_loss: 1.0274e-04\n",
      "Epoch 313/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.7717e-05 - val_loss: 9.7601e-05\n",
      "Epoch 314/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.1155e-05 - val_loss: 9.6910e-05\n",
      "Epoch 315/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.5607e-05 - val_loss: 9.7608e-05\n",
      "Epoch 316/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6768e-05 - val_loss: 1.4941e-04\n",
      "Epoch 317/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0134e-04 - val_loss: 1.1600e-04\n",
      "Epoch 318/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8765e-05 - val_loss: 1.1555e-04\n",
      "Epoch 319/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7963e-05 - val_loss: 1.1042e-04\n",
      "Epoch 320/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.4584e-05 - val_loss: 1.0120e-04\n",
      "Epoch 321/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.6542e-05 - val_loss: 1.0544e-04\n",
      "Epoch 322/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.9994e-05 - val_loss: 1.0142e-04\n",
      "Epoch 323/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.0518e-05 - val_loss: 1.0267e-04\n",
      "Epoch 324/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9001e-05 - val_loss: 1.0769e-04\n",
      "Epoch 325/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2250e-05 - val_loss: 1.0404e-04\n",
      "Epoch 326/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4978e-05 - val_loss: 9.5740e-05\n",
      "Epoch 327/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5375e-05 - val_loss: 9.3317e-05\n",
      "Epoch 328/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.7341e-05 - val_loss: 8.9956e-05\n",
      "Epoch 329/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7447e-05 - val_loss: 9.9980e-05\n",
      "Epoch 330/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.2186e-05 - val_loss: 1.1511e-04\n",
      "Epoch 331/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8691e-05 - val_loss: 1.0566e-04\n",
      "Epoch 332/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2411e-05 - val_loss: 8.7191e-05\n",
      "Epoch 333/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.2940e-05 - val_loss: 8.5945e-05\n",
      "Epoch 334/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8004e-05 - val_loss: 1.0816e-04\n",
      "Epoch 335/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.0515e-04 - val_loss: 9.5949e-05\n",
      "Epoch 336/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.7877e-05 - val_loss: 9.1439e-05\n",
      "Epoch 337/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8201e-05 - val_loss: 9.9087e-05\n",
      "Epoch 338/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.6006e-05 - val_loss: 8.9506e-05\n",
      "Epoch 339/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2564e-05 - val_loss: 8.7847e-05\n",
      "Epoch 340/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3032e-05 - val_loss: 8.5986e-05\n",
      "Epoch 341/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.1910e-05 - val_loss: 8.8147e-05\n",
      "Epoch 342/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1161e-05 - val_loss: 8.6377e-05\n",
      "Epoch 343/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0864e-05 - val_loss: 8.7886e-05\n",
      "Epoch 344/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.4066e-05 - val_loss: 8.6178e-05\n",
      "Epoch 345/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0591e-05 - val_loss: 8.4042e-05\n",
      "Epoch 346/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8775e-05 - val_loss: 8.3138e-05\n",
      "Epoch 347/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7866e-05 - val_loss: 8.2656e-05\n",
      "Epoch 348/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7758e-05 - val_loss: 8.2638e-05\n",
      "Epoch 349/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5117e-05 - val_loss: 8.4347e-05\n",
      "Epoch 350/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.0702e-05 - val_loss: 9.3333e-05\n",
      "Epoch 351/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1954e-05 - val_loss: 8.5151e-05\n",
      "Epoch 352/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9215e-05 - val_loss: 8.1043e-05\n",
      "Epoch 353/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.3184e-05 - val_loss: 9.2614e-05\n",
      "Epoch 354/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.9235e-05 - val_loss: 8.1375e-05\n",
      "Epoch 355/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6760e-05 - val_loss: 8.4688e-05\n",
      "Epoch 356/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 9.0123e-05 - val_loss: 8.9139e-05\n",
      "Epoch 357/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8601e-05 - val_loss: 8.9424e-05\n",
      "Epoch 358/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6958e-05 - val_loss: 9.2582e-05\n",
      "Epoch 359/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.2246e-05 - val_loss: 8.2186e-05\n",
      "Epoch 360/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0809e-05 - val_loss: 1.4698e-04\n",
      "Epoch 361/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.0858e-05 - val_loss: 8.2332e-05\n",
      "Epoch 362/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4570e-05 - val_loss: 9.9871e-05\n",
      "Epoch 363/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6735e-05 - val_loss: 1.0692e-04\n",
      "Epoch 364/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9523e-05 - val_loss: 8.9151e-05\n",
      "Epoch 365/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6208e-05 - val_loss: 8.0298e-05\n",
      "Epoch 366/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5142e-05 - val_loss: 8.1126e-05\n",
      "Epoch 367/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2385e-05 - val_loss: 8.8869e-05\n",
      "Epoch 368/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.9742e-05 - val_loss: 1.5829e-04\n",
      "Epoch 369/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.8930e-05 - val_loss: 8.6782e-05\n",
      "Epoch 370/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5483e-05 - val_loss: 8.3289e-05\n",
      "Epoch 371/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2966e-05 - val_loss: 8.8229e-05\n",
      "Epoch 372/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.5660e-05 - val_loss: 8.9316e-05\n",
      "Epoch 373/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3206e-05 - val_loss: 7.7205e-05\n",
      "Epoch 374/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2983e-05 - val_loss: 7.5924e-05\n",
      "Epoch 375/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1157e-05 - val_loss: 7.5635e-05\n",
      "Epoch 376/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4927e-05 - val_loss: 7.9359e-05\n",
      "Epoch 377/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.8121e-05 - val_loss: 8.2001e-05\n",
      "Epoch 378/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4021e-05 - val_loss: 7.8863e-05\n",
      "Epoch 379/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.3486e-05 - val_loss: 8.2218e-05\n",
      "Epoch 380/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4120e-05 - val_loss: 8.6949e-05\n",
      "Epoch 381/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2535e-05 - val_loss: 8.6402e-05\n",
      "Epoch 382/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4205e-05 - val_loss: 7.7729e-05\n",
      "Epoch 383/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.3318e-05 - val_loss: 8.0546e-05\n",
      "Epoch 384/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2669e-05 - val_loss: 7.5484e-05\n",
      "Epoch 385/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9126e-05 - val_loss: 8.1335e-05\n",
      "Epoch 386/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2037e-05 - val_loss: 7.7175e-05\n",
      "Epoch 387/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.8136e-05 - val_loss: 7.2037e-05\n",
      "Epoch 388/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.8506e-05 - val_loss: 8.8022e-05\n",
      "Epoch 389/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1658e-05 - val_loss: 7.3249e-05\n",
      "Epoch 390/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.2625e-05 - val_loss: 7.1195e-05\n",
      "Epoch 391/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7567e-05 - val_loss: 8.7668e-05\n",
      "Epoch 392/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.8885e-05 - val_loss: 8.5595e-05\n",
      "Epoch 393/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.0535e-05 - val_loss: 7.2119e-05\n",
      "Epoch 394/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5712e-05 - val_loss: 8.0487e-05\n",
      "Epoch 395/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.5278e-05 - val_loss: 7.2660e-05\n",
      "Epoch 396/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2198e-05 - val_loss: 7.3319e-05\n",
      "Epoch 397/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5756e-05 - val_loss: 8.0435e-05\n",
      "Epoch 398/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2956e-05 - val_loss: 8.7002e-05\n",
      "Epoch 399/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.2300e-05 - val_loss: 7.8804e-05\n",
      "Epoch 400/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.1119e-05 - val_loss: 7.2383e-05\n",
      "Epoch 401/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9528e-05 - val_loss: 7.5130e-05\n",
      "Epoch 402/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5977e-05 - val_loss: 7.1399e-05\n",
      "Epoch 403/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7817e-05 - val_loss: 7.3033e-05\n",
      "Epoch 404/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.7509e-05 - val_loss: 7.1794e-05\n",
      "Epoch 405/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6877e-05 - val_loss: 7.2983e-05\n",
      "Epoch 406/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6919e-05 - val_loss: 6.9601e-05\n",
      "Epoch 407/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7434e-05 - val_loss: 7.7294e-05\n",
      "Epoch 408/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6337e-05 - val_loss: 7.8115e-05\n",
      "Epoch 409/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.6880e-05 - val_loss: 7.8396e-05\n",
      "Epoch 410/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5530e-05 - val_loss: 6.7471e-05\n",
      "Epoch 411/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6461e-05 - val_loss: 6.9667e-05\n",
      "Epoch 412/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1599e-05 - val_loss: 7.8152e-05\n",
      "Epoch 413/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4299e-05 - val_loss: 7.4679e-05\n",
      "Epoch 414/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0549e-05 - val_loss: 1.1123e-04\n",
      "Epoch 415/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.2367e-05 - val_loss: 7.0518e-05\n",
      "Epoch 416/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5267e-05 - val_loss: 6.5648e-05\n",
      "Epoch 417/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4346e-05 - val_loss: 7.1168e-05\n",
      "Epoch 418/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6860e-05 - val_loss: 8.2520e-05\n",
      "Epoch 419/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3746e-05 - val_loss: 6.6294e-05\n",
      "Epoch 420/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5748e-05 - val_loss: 6.9911e-05\n",
      "Epoch 421/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7339e-05 - val_loss: 7.0363e-05\n",
      "Epoch 422/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6696e-05 - val_loss: 7.1266e-05\n",
      "Epoch 423/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7583e-05 - val_loss: 6.7254e-05\n",
      "Epoch 424/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6206e-05 - val_loss: 6.6514e-05\n",
      "Epoch 425/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.4492e-05 - val_loss: 6.8726e-05\n",
      "Epoch 426/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3189e-05 - val_loss: 6.6795e-05\n",
      "Epoch 427/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4769e-05 - val_loss: 6.6658e-05\n",
      "Epoch 428/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1841e-05 - val_loss: 7.7110e-05\n",
      "Epoch 429/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2009e-05 - val_loss: 6.4460e-05\n",
      "Epoch 430/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2362e-05 - val_loss: 7.0708e-05\n",
      "Epoch 431/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6068e-05 - val_loss: 7.4311e-05\n",
      "Epoch 432/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.5060e-05 - val_loss: 7.5198e-05\n",
      "Epoch 433/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.9253e-05 - val_loss: 8.6908e-05\n",
      "Epoch 434/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7723e-05 - val_loss: 8.3165e-05\n",
      "Epoch 435/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2294e-05 - val_loss: 7.2791e-05\n",
      "Epoch 436/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3552e-05 - val_loss: 7.9361e-05\n",
      "Epoch 437/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2482e-05 - val_loss: 6.3100e-05\n",
      "Epoch 438/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6125e-05 - val_loss: 7.6456e-05\n",
      "Epoch 439/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0058e-05 - val_loss: 7.0319e-05\n",
      "Epoch 440/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0764e-05 - val_loss: 6.3766e-05\n",
      "Epoch 441/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0499e-05 - val_loss: 6.5957e-05\n",
      "Epoch 442/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5857e-05 - val_loss: 6.3532e-05\n",
      "Epoch 443/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8903e-05 - val_loss: 6.8794e-05\n",
      "Epoch 444/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9234e-05 - val_loss: 6.3831e-05\n",
      "Epoch 445/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2528e-05 - val_loss: 9.1394e-05\n",
      "Epoch 446/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.2302e-05 - val_loss: 6.5566e-05\n",
      "Epoch 447/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0556e-05 - val_loss: 7.2612e-05\n",
      "Epoch 448/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8391e-05 - val_loss: 6.7799e-05\n",
      "Epoch 449/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9579e-05 - val_loss: 6.6991e-05\n",
      "Epoch 450/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7850e-05 - val_loss: 7.3252e-05\n",
      "Epoch 451/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0950e-05 - val_loss: 6.8606e-05\n",
      "Epoch 452/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5250e-05 - val_loss: 7.1739e-05\n",
      "Epoch 453/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.3322e-05 - val_loss: 6.3660e-05\n",
      "Epoch 454/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.4357e-05 - val_loss: 1.1075e-04\n",
      "Epoch 455/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0942e-05 - val_loss: 6.8482e-05\n",
      "Epoch 456/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6224e-05 - val_loss: 6.5733e-05\n",
      "Epoch 457/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2094e-05 - val_loss: 7.0722e-05\n",
      "Epoch 458/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7960e-05 - val_loss: 8.0449e-05\n",
      "Epoch 459/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7544e-05 - val_loss: 6.2876e-05\n",
      "Epoch 460/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8900e-05 - val_loss: 7.5566e-05\n",
      "Epoch 461/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.8023e-05 - val_loss: 6.5034e-05\n",
      "Epoch 462/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6778e-05 - val_loss: 6.7100e-05\n",
      "Epoch 463/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7120e-05 - val_loss: 6.1265e-05\n",
      "Epoch 464/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8756e-05 - val_loss: 7.1684e-05\n",
      "Epoch 465/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.6181e-05 - val_loss: 6.5088e-05\n",
      "Epoch 466/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7246e-05 - val_loss: 6.1236e-05\n",
      "Epoch 467/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6179e-05 - val_loss: 5.7469e-05\n",
      "Epoch 468/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6264e-05 - val_loss: 7.1169e-05\n",
      "Epoch 469/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1543e-05 - val_loss: 7.2335e-05\n",
      "Epoch 470/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8605e-05 - val_loss: 6.0681e-05\n",
      "Epoch 471/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7919e-05 - val_loss: 6.0762e-05\n",
      "Epoch 472/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4290e-05 - val_loss: 5.6166e-05\n",
      "Epoch 473/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8802e-05 - val_loss: 7.8884e-05\n",
      "Epoch 474/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6498e-05 - val_loss: 5.6711e-05\n",
      "Epoch 475/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9429e-05 - val_loss: 6.2686e-05\n",
      "Epoch 476/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.3839e-05 - val_loss: 6.9795e-05\n",
      "Epoch 477/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4095e-05 - val_loss: 5.8340e-05\n",
      "Epoch 478/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4569e-05 - val_loss: 5.8425e-05\n",
      "Epoch 479/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4845e-05 - val_loss: 6.4131e-05\n",
      "Epoch 480/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4168e-05 - val_loss: 5.9059e-05\n",
      "Epoch 481/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.7573e-05 - val_loss: 6.0788e-05\n",
      "Epoch 482/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3669e-05 - val_loss: 6.0680e-05\n",
      "Epoch 483/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.9672e-05 - val_loss: 6.1651e-05\n",
      "Epoch 484/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.3241e-05 - val_loss: 6.4397e-05\n",
      "Epoch 485/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2657e-05 - val_loss: 5.5649e-05\n",
      "Epoch 486/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1655e-05 - val_loss: 5.4253e-05\n",
      "Epoch 487/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.3474e-05 - val_loss: 5.5437e-05\n",
      "Epoch 488/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.2496e-05 - val_loss: 5.5904e-05\n",
      "Epoch 489/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3811e-05 - val_loss: 5.6542e-05\n",
      "Epoch 490/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.1306e-05 - val_loss: 6.1891e-05\n",
      "Epoch 491/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0599e-05 - val_loss: 6.1472e-05\n",
      "Epoch 492/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2947e-05 - val_loss: 5.5606e-05\n",
      "Epoch 493/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.2234e-05 - val_loss: 6.9998e-05\n",
      "Epoch 494/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.5927e-05 - val_loss: 7.0726e-05\n",
      "Epoch 495/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2220e-05 - val_loss: 1.3287e-04\n",
      "Epoch 496/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6640e-05 - val_loss: 5.9430e-05\n",
      "Epoch 497/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.7678e-05 - val_loss: 6.0905e-05\n",
      "Epoch 498/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6695e-05 - val_loss: 5.5308e-05\n",
      "Epoch 499/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4698e-05 - val_loss: 5.4031e-05\n",
      "Epoch 500/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.6031e-05 - val_loss: 6.0875e-05\n",
      "Epoch 501/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.4856e-05 - val_loss: 6.1737e-05\n",
      "Epoch 502/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0934e-05 - val_loss: 5.7112e-05\n",
      "Epoch 503/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5026e-05 - val_loss: 7.6740e-05\n",
      "Epoch 504/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.2798e-05 - val_loss: 5.2520e-05\n",
      "Epoch 505/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2607e-05 - val_loss: 5.7088e-05\n",
      "Epoch 506/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.0344e-05 - val_loss: 5.9083e-05\n",
      "Epoch 507/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0989e-05 - val_loss: 5.3996e-05\n",
      "Epoch 508/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2466e-05 - val_loss: 5.1226e-05\n",
      "Epoch 509/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8933e-05 - val_loss: 5.0849e-05\n",
      "Epoch 510/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9804e-05 - val_loss: 5.7991e-05\n",
      "Epoch 511/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7530e-05 - val_loss: 5.6978e-05\n",
      "Epoch 512/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0522e-05 - val_loss: 5.5750e-05\n",
      "Epoch 513/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5216e-05 - val_loss: 5.6771e-05\n",
      "Epoch 514/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4280e-05 - val_loss: 5.1146e-05\n",
      "Epoch 515/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1547e-05 - val_loss: 5.3034e-05\n",
      "Epoch 516/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6251e-05 - val_loss: 5.7579e-05\n",
      "Epoch 517/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.4156e-05 - val_loss: 6.3969e-05\n",
      "Epoch 518/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2554e-05 - val_loss: 5.8766e-05\n",
      "Epoch 519/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.0550e-05 - val_loss: 5.3691e-05\n",
      "Epoch 520/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8805e-05 - val_loss: 5.2126e-05\n",
      "Epoch 521/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7629e-05 - val_loss: 5.3855e-05\n",
      "Epoch 522/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.8553e-05 - val_loss: 5.5544e-05\n",
      "Epoch 523/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.9838e-05 - val_loss: 5.4082e-05\n",
      "Epoch 524/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9290e-05 - val_loss: 5.2140e-05\n",
      "Epoch 525/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6757e-05 - val_loss: 5.2825e-05\n",
      "Epoch 526/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.3509e-05 - val_loss: 5.9515e-05\n",
      "Epoch 527/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.0882e-05 - val_loss: 5.5512e-05\n",
      "Epoch 528/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.8856e-05 - val_loss: 4.9164e-05\n",
      "Epoch 529/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7243e-05 - val_loss: 5.1156e-05\n",
      "Epoch 530/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0560e-05 - val_loss: 5.5456e-05\n",
      "Epoch 531/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8387e-05 - val_loss: 4.8965e-05\n",
      "Epoch 532/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0957e-05 - val_loss: 5.1889e-05\n",
      "Epoch 533/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3339e-05 - val_loss: 6.0872e-05\n",
      "Epoch 534/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4653e-05 - val_loss: 5.5991e-05\n",
      "Epoch 535/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0347e-05 - val_loss: 5.1777e-05\n",
      "Epoch 536/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0062e-05 - val_loss: 4.8363e-05\n",
      "Epoch 537/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6671e-05 - val_loss: 7.2755e-05\n",
      "Epoch 538/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.3099e-05 - val_loss: 5.8851e-05\n",
      "Epoch 539/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6824e-05 - val_loss: 5.2175e-05\n",
      "Epoch 540/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8363e-05 - val_loss: 5.3872e-05\n",
      "Epoch 541/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7899e-05 - val_loss: 5.3399e-05\n",
      "Epoch 542/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8527e-05 - val_loss: 5.1256e-05\n",
      "Epoch 543/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.7140e-05 - val_loss: 5.2678e-05\n",
      "Epoch 544/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7311e-05 - val_loss: 5.0696e-05\n",
      "Epoch 545/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0483e-05 - val_loss: 4.7080e-05\n",
      "Epoch 546/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8355e-05 - val_loss: 7.0047e-05\n",
      "Epoch 547/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1522e-05 - val_loss: 6.3584e-05\n",
      "Epoch 548/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.1102e-05 - val_loss: 9.6535e-05\n",
      "Epoch 549/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.4529e-05 - val_loss: 1.2027e-04\n",
      "Epoch 550/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.0128e-05 - val_loss: 5.1913e-05\n",
      "Epoch 551/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.6367e-05 - val_loss: 5.3365e-05\n",
      "Epoch 552/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7455e-05 - val_loss: 5.1660e-05\n",
      "Epoch 553/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.6970e-05 - val_loss: 4.9661e-05\n",
      "Epoch 554/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5542e-05 - val_loss: 4.8079e-05\n",
      "Epoch 555/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5685e-05 - val_loss: 4.8008e-05\n",
      "Epoch 556/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5750e-05 - val_loss: 4.7199e-05\n",
      "Epoch 557/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5094e-05 - val_loss: 5.4629e-05\n",
      "Epoch 558/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0178e-05 - val_loss: 4.9509e-05\n",
      "Epoch 559/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6569e-05 - val_loss: 4.6630e-05\n",
      "Epoch 560/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4601e-05 - val_loss: 4.8602e-05\n",
      "Epoch 561/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5585e-05 - val_loss: 6.1692e-05\n",
      "Epoch 562/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9133e-05 - val_loss: 6.0703e-05\n",
      "Epoch 563/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8818e-05 - val_loss: 5.0815e-05\n",
      "Epoch 564/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2804e-05 - val_loss: 4.9680e-05\n",
      "Epoch 565/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.5050e-05 - val_loss: 6.2778e-05\n",
      "Epoch 566/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.5451e-05 - val_loss: 4.6626e-05\n",
      "Epoch 567/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3510e-05 - val_loss: 4.9108e-05\n",
      "Epoch 568/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0276e-05 - val_loss: 5.0509e-05\n",
      "Epoch 569/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4293e-05 - val_loss: 5.0032e-05\n",
      "Epoch 570/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6224e-05 - val_loss: 6.8184e-05\n",
      "Epoch 571/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4312e-05 - val_loss: 4.9274e-05\n",
      "Epoch 572/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3691e-05 - val_loss: 4.7293e-05\n",
      "Epoch 573/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4558e-05 - val_loss: 5.2126e-05\n",
      "Epoch 574/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7221e-05 - val_loss: 8.8886e-05\n",
      "Epoch 575/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2204e-05 - val_loss: 5.6635e-05\n",
      "Epoch 576/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.7137e-05 - val_loss: 7.0536e-05\n",
      "Epoch 577/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5290e-05 - val_loss: 4.6297e-05\n",
      "Epoch 578/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3301e-05 - val_loss: 4.8110e-05\n",
      "Epoch 579/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2187e-05 - val_loss: 4.5961e-05\n",
      "Epoch 580/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3735e-05 - val_loss: 5.3873e-05\n",
      "Epoch 581/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5950e-05 - val_loss: 7.5286e-05\n",
      "Epoch 582/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0585e-05 - val_loss: 4.7362e-05\n",
      "Epoch 583/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.2970e-05 - val_loss: 4.5737e-05\n",
      "Epoch 584/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2722e-05 - val_loss: 4.5944e-05\n",
      "Epoch 585/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.2519e-05 - val_loss: 4.4834e-05\n",
      "Epoch 586/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4716e-05 - val_loss: 4.6330e-05\n",
      "Epoch 587/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.2704e-05 - val_loss: 4.9548e-05\n",
      "Epoch 588/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8270e-05 - val_loss: 4.5415e-05\n",
      "Epoch 589/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2278e-05 - val_loss: 4.7592e-05\n",
      "Epoch 590/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2593e-05 - val_loss: 4.5771e-05\n",
      "Epoch 591/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.2807e-04 - val_loss: 1.6455e-04\n",
      "Epoch 592/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6996e-05 - val_loss: 5.0396e-05\n",
      "Epoch 593/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6341e-05 - val_loss: 4.7665e-05\n",
      "Epoch 594/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.4124e-05 - val_loss: 4.6246e-05\n",
      "Epoch 595/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.3603e-05 - val_loss: 4.4521e-05\n",
      "Epoch 596/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2238e-05 - val_loss: 5.5779e-05\n",
      "Epoch 597/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2855e-05 - val_loss: 4.5537e-05\n",
      "Epoch 598/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3605e-05 - val_loss: 5.0099e-05\n",
      "Epoch 599/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4949e-05 - val_loss: 4.3191e-05\n",
      "Epoch 600/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2027e-05 - val_loss: 4.4384e-05\n",
      "Epoch 601/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2988e-05 - val_loss: 4.6399e-05\n",
      "Epoch 602/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2104e-05 - val_loss: 4.7276e-05\n",
      "Epoch 603/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.4325e-05 - val_loss: 5.1461e-05\n",
      "Epoch 604/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3521e-05 - val_loss: 4.4461e-05\n",
      "Epoch 605/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.4353e-05 - val_loss: 7.4346e-05\n",
      "Epoch 606/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2713e-05 - val_loss: 4.3764e-05\n",
      "Epoch 607/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4753e-05 - val_loss: 5.2874e-05\n",
      "Epoch 608/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.6730e-05 - val_loss: 4.8633e-05\n",
      "Epoch 609/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.8244e-05 - val_loss: 6.1223e-05\n",
      "Epoch 610/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.7585e-05 - val_loss: 4.4714e-05\n",
      "Epoch 611/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2678e-05 - val_loss: 4.5682e-05\n",
      "Epoch 612/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2558e-05 - val_loss: 4.3072e-05\n",
      "Epoch 613/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0618e-05 - val_loss: 4.3228e-05\n",
      "Epoch 614/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1910e-05 - val_loss: 4.7161e-05\n",
      "Epoch 615/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2210e-05 - val_loss: 5.0244e-05\n",
      "Epoch 616/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.3390e-05 - val_loss: 4.2974e-05\n",
      "Epoch 617/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4565e-05 - val_loss: 4.4825e-05\n",
      "Epoch 618/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9880e-05 - val_loss: 4.2267e-05\n",
      "Epoch 619/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3124e-05 - val_loss: 5.3735e-05\n",
      "Epoch 620/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2210e-05 - val_loss: 4.6234e-05\n",
      "Epoch 621/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9537e-05 - val_loss: 4.6757e-05\n",
      "Epoch 622/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1613e-05 - val_loss: 4.2526e-05\n",
      "Epoch 623/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8642e-05 - val_loss: 5.4846e-05\n",
      "Epoch 624/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5837e-05 - val_loss: 4.2227e-05\n",
      "Epoch 625/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9174e-05 - val_loss: 4.3888e-05\n",
      "Epoch 626/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.1150e-05 - val_loss: 4.3971e-05\n",
      "Epoch 627/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0922e-05 - val_loss: 4.3170e-05\n",
      "Epoch 628/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1009e-05 - val_loss: 4.6572e-05\n",
      "Epoch 629/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8942e-05 - val_loss: 4.2256e-05\n",
      "Epoch 630/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0990e-05 - val_loss: 4.4044e-05\n",
      "Epoch 631/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5508e-05 - val_loss: 4.6585e-05\n",
      "Epoch 632/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9356e-05 - val_loss: 4.1205e-05\n",
      "Epoch 633/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8574e-05 - val_loss: 4.6443e-05\n",
      "Epoch 634/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2385e-05 - val_loss: 5.7195e-05\n",
      "Epoch 635/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1.0904e-04 - val_loss: 9.4139e-05\n",
      "Epoch 636/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6415e-05 - val_loss: 5.9155e-05\n",
      "Epoch 637/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6598e-05 - val_loss: 4.5721e-05\n",
      "Epoch 638/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.0758e-05 - val_loss: 4.1800e-05\n",
      "Epoch 639/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0566e-05 - val_loss: 4.9292e-05\n",
      "Epoch 640/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.2758e-05 - val_loss: 4.0845e-05\n",
      "Epoch 641/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9909e-05 - val_loss: 4.5462e-05\n",
      "Epoch 642/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8295e-05 - val_loss: 4.1698e-05\n",
      "Epoch 643/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9927e-05 - val_loss: 4.5683e-05\n",
      "Epoch 644/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.6461e-05 - val_loss: 6.5455e-05\n",
      "Epoch 645/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3996e-05 - val_loss: 4.2566e-05\n",
      "Epoch 646/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9316e-05 - val_loss: 4.1003e-05\n",
      "Epoch 647/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8047e-05 - val_loss: 4.0898e-05\n",
      "Epoch 648/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.7833e-05 - val_loss: 3.9940e-05\n",
      "Epoch 649/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7959e-05 - val_loss: 4.0691e-05\n",
      "Epoch 650/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8657e-05 - val_loss: 4.9992e-05\n",
      "Epoch 651/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8582e-05 - val_loss: 4.0061e-05\n",
      "Epoch 652/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0143e-05 - val_loss: 4.4272e-05\n",
      "Epoch 653/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2631e-05 - val_loss: 4.0953e-05\n",
      "Epoch 654/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8349e-05 - val_loss: 3.9789e-05\n",
      "Epoch 655/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.8960e-05 - val_loss: 4.2758e-05\n",
      "Epoch 656/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8654e-05 - val_loss: 3.9582e-05\n",
      "Epoch 657/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8921e-05 - val_loss: 4.2387e-05\n",
      "Epoch 658/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9000e-05 - val_loss: 4.1244e-05\n",
      "Epoch 659/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9536e-05 - val_loss: 3.9489e-05\n",
      "Epoch 660/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7908e-05 - val_loss: 4.0700e-05\n",
      "Epoch 661/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7294e-05 - val_loss: 3.9384e-05\n",
      "Epoch 662/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8155e-05 - val_loss: 3.9321e-05\n",
      "Epoch 663/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.9244e-05 - val_loss: 3.9208e-05\n",
      "Epoch 664/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.7407e-05 - val_loss: 4.9550e-05\n",
      "Epoch 665/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6053e-05 - val_loss: 5.2831e-05\n",
      "Epoch 666/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.2288e-05 - val_loss: 4.2700e-05\n",
      "Epoch 667/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2865e-05 - val_loss: 4.2453e-05\n",
      "Epoch 668/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6268e-05 - val_loss: 4.4255e-05\n",
      "Epoch 669/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3123e-05 - val_loss: 4.9501e-05\n",
      "Epoch 670/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2756e-05 - val_loss: 4.4249e-05\n",
      "Epoch 671/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9731e-05 - val_loss: 4.0309e-05\n",
      "Epoch 672/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 5.7277e-05 - val_loss: 4.1574e-05\n",
      "Epoch 673/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8659e-05 - val_loss: 4.2935e-05\n",
      "Epoch 674/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9088e-05 - val_loss: 5.6075e-05\n",
      "Epoch 675/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4504e-05 - val_loss: 4.6211e-05\n",
      "Epoch 676/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.8337e-05 - val_loss: 3.9794e-05\n",
      "Epoch 677/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.8722e-05 - val_loss: 4.3836e-05\n",
      "Epoch 678/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6015e-05 - val_loss: 3.9277e-05\n",
      "Epoch 679/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.7096e-05 - val_loss: 3.8330e-05\n",
      "Epoch 680/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9106e-05 - val_loss: 3.9893e-05\n",
      "Epoch 681/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 7.7878e-05 - val_loss: 5.1304e-05\n",
      "Epoch 682/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.9094e-05 - val_loss: 4.0103e-05\n",
      "Epoch 683/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1699e-05 - val_loss: 6.8180e-05\n",
      "Epoch 684/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0384e-05 - val_loss: 3.8747e-05\n",
      "Epoch 685/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.5942e-05 - val_loss: 3.7597e-05\n",
      "Epoch 686/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6262e-05 - val_loss: 4.1717e-05\n",
      "Epoch 687/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7437e-05 - val_loss: 3.8557e-05\n",
      "Epoch 688/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.7643e-05 - val_loss: 4.0354e-05\n",
      "Epoch 689/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.6429e-05 - val_loss: 3.8965e-05\n",
      "Epoch 690/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.7373e-05 - val_loss: 3.8698e-05\n",
      "Epoch 691/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8584e-05 - val_loss: 4.0822e-05\n",
      "Epoch 692/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.6038e-05 - val_loss: 3.9489e-05\n",
      "Epoch 693/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.2022e-05 - val_loss: 8.1183e-05\n",
      "Epoch 694/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2415e-05 - val_loss: 3.9109e-05\n",
      "Epoch 695/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6601e-05 - val_loss: 3.7111e-05\n",
      "Epoch 696/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6324e-05 - val_loss: 3.7713e-05\n",
      "Epoch 697/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6511e-05 - val_loss: 3.8686e-05\n",
      "Epoch 698/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.6655e-05 - val_loss: 4.1648e-05\n",
      "Epoch 699/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0071e-05 - val_loss: 4.4106e-05\n",
      "Epoch 700/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6081e-05 - val_loss: 3.7500e-05\n",
      "Epoch 701/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6140e-05 - val_loss: 6.3667e-05\n",
      "Epoch 702/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8442e-05 - val_loss: 4.4055e-05\n",
      "Epoch 703/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5785e-05 - val_loss: 9.4280e-05\n",
      "Epoch 704/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6974e-05 - val_loss: 3.7991e-05\n",
      "Epoch 705/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6577e-05 - val_loss: 3.7727e-05\n",
      "Epoch 706/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7663e-05 - val_loss: 3.7915e-05\n",
      "Epoch 707/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7004e-05 - val_loss: 3.8094e-05\n",
      "Epoch 708/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4916e-05 - val_loss: 3.8249e-05\n",
      "Epoch 709/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8553e-05 - val_loss: 4.3965e-05\n",
      "Epoch 710/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2035e-05 - val_loss: 5.4374e-05\n",
      "Epoch 711/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 1.4960e-04 - val_loss: 1.8124e-04\n",
      "Epoch 712/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.4629e-05 - val_loss: 3.9878e-05\n",
      "Epoch 713/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6512e-05 - val_loss: 3.7608e-05\n",
      "Epoch 714/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.5704e-05 - val_loss: 3.8304e-05\n",
      "Epoch 715/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4497e-05 - val_loss: 3.6040e-05\n",
      "Epoch 716/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.5151e-05 - val_loss: 3.7055e-05\n",
      "Epoch 717/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4173e-05 - val_loss: 3.6308e-05\n",
      "Epoch 718/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7081e-05 - val_loss: 3.8581e-05\n",
      "Epoch 719/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5825e-05 - val_loss: 3.7340e-05\n",
      "Epoch 720/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5286e-05 - val_loss: 3.6614e-05\n",
      "Epoch 721/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4622e-05 - val_loss: 4.0889e-05\n",
      "Epoch 722/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4867e-05 - val_loss: 3.8684e-05\n",
      "Epoch 723/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5283e-05 - val_loss: 3.6182e-05\n",
      "Epoch 724/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8867e-05 - val_loss: 4.1025e-05\n",
      "Epoch 725/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5255e-05 - val_loss: 3.6086e-05\n",
      "Epoch 726/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3794e-05 - val_loss: 3.6550e-05\n",
      "Epoch 727/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.4846e-05 - val_loss: 3.6415e-05\n",
      "Epoch 728/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4615e-05 - val_loss: 3.8600e-05\n",
      "Epoch 729/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6348e-05 - val_loss: 3.7148e-05\n",
      "Epoch 730/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4866e-05 - val_loss: 3.8374e-05\n",
      "Epoch 731/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7345e-05 - val_loss: 3.8290e-05\n",
      "Epoch 732/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8250e-05 - val_loss: 6.5700e-05\n",
      "Epoch 733/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2212e-05 - val_loss: 4.7179e-05\n",
      "Epoch 734/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.6197e-05 - val_loss: 4.6141e-05\n",
      "Epoch 735/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5255e-05 - val_loss: 3.5318e-05\n",
      "Epoch 736/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4538e-05 - val_loss: 3.6110e-05\n",
      "Epoch 737/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3799e-05 - val_loss: 3.5079e-05\n",
      "Epoch 738/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6947e-05 - val_loss: 3.6840e-05\n",
      "Epoch 739/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6677e-05 - val_loss: 4.0313e-05\n",
      "Epoch 740/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.2864e-05 - val_loss: 7.2997e-05\n",
      "Epoch 741/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.2289e-05 - val_loss: 4.8536e-05\n",
      "Epoch 742/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9729e-05 - val_loss: 3.6653e-05\n",
      "Epoch 743/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6017e-05 - val_loss: 3.9815e-05\n",
      "Epoch 744/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3334e-05 - val_loss: 4.3499e-05\n",
      "Epoch 745/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5408e-05 - val_loss: 3.6328e-05\n",
      "Epoch 746/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6160e-05 - val_loss: 6.1426e-05\n",
      "Epoch 747/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.0350e-05 - val_loss: 6.2960e-05\n",
      "Epoch 748/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8818e-05 - val_loss: 3.8617e-05\n",
      "Epoch 749/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3477e-05 - val_loss: 3.5327e-05\n",
      "Epoch 750/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5867e-05 - val_loss: 6.2499e-05\n",
      "Epoch 751/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6397e-05 - val_loss: 5.5335e-05\n",
      "Epoch 752/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4219e-05 - val_loss: 3.5270e-05\n",
      "Epoch 753/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2719e-05 - val_loss: 3.5617e-05\n",
      "Epoch 754/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.1822e-05 - val_loss: 6.3973e-05\n",
      "Epoch 755/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.2095e-05 - val_loss: 3.8497e-05\n",
      "Epoch 756/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6731e-05 - val_loss: 3.5907e-05\n",
      "Epoch 757/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2642e-05 - val_loss: 3.3813e-05\n",
      "Epoch 758/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3215e-05 - val_loss: 3.7717e-05\n",
      "Epoch 759/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3227e-05 - val_loss: 4.7374e-05\n",
      "Epoch 760/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4207e-05 - val_loss: 3.6142e-05\n",
      "Epoch 761/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4002e-05 - val_loss: 3.4623e-05\n",
      "Epoch 762/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2600e-05 - val_loss: 3.4630e-05\n",
      "Epoch 763/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.3553e-05 - val_loss: 3.6021e-05\n",
      "Epoch 764/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4619e-05 - val_loss: 3.6492e-05\n",
      "Epoch 765/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5144e-05 - val_loss: 4.2362e-05\n",
      "Epoch 766/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9409e-05 - val_loss: 3.6125e-05\n",
      "Epoch 767/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7896e-05 - val_loss: 3.5022e-05\n",
      "Epoch 768/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3471e-05 - val_loss: 3.6353e-05\n",
      "Epoch 769/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3545e-05 - val_loss: 3.8910e-05\n",
      "Epoch 770/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3699e-05 - val_loss: 3.8430e-05\n",
      "Epoch 771/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.0801e-05 - val_loss: 3.7501e-05\n",
      "Epoch 772/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4783e-05 - val_loss: 3.6022e-05\n",
      "Epoch 773/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6509e-05 - val_loss: 3.5303e-05\n",
      "Epoch 774/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.4921e-05 - val_loss: 3.4844e-05\n",
      "Epoch 775/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2169e-05 - val_loss: 3.3891e-05\n",
      "Epoch 776/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9160e-05 - val_loss: 4.1378e-05\n",
      "Epoch 777/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.4482e-05 - val_loss: 3.4288e-05\n",
      "Epoch 778/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3436e-05 - val_loss: 3.8639e-05\n",
      "Epoch 779/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7503e-05 - val_loss: 4.0162e-05\n",
      "Epoch 780/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.5675e-05 - val_loss: 3.3724e-05\n",
      "Epoch 781/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7397e-05 - val_loss: 3.8218e-05\n",
      "Epoch 782/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2652e-05 - val_loss: 3.4093e-05\n",
      "Epoch 783/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2945e-05 - val_loss: 3.9549e-05\n",
      "Epoch 784/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.4584e-05 - val_loss: 3.5726e-05\n",
      "Epoch 785/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1886e-05 - val_loss: 3.4025e-05\n",
      "Epoch 786/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2887e-05 - val_loss: 5.1726e-05\n",
      "Epoch 787/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5388e-05 - val_loss: 3.6028e-05\n",
      "Epoch 788/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.5679e-05 - val_loss: 5.0319e-05\n",
      "Epoch 789/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3884e-05 - val_loss: 3.5069e-05\n",
      "Epoch 790/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6123e-05 - val_loss: 4.0722e-05\n",
      "Epoch 791/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6316e-05 - val_loss: 3.4419e-05\n",
      "Epoch 792/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.1574e-05 - val_loss: 3.2756e-05\n",
      "Epoch 793/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6515e-05 - val_loss: 4.0417e-05\n",
      "Epoch 794/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.8201e-05 - val_loss: 3.3002e-05\n",
      "Epoch 795/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.4458e-05 - val_loss: 1.3518e-04\n",
      "Epoch 796/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5217e-05 - val_loss: 3.8730e-05\n",
      "Epoch 797/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.7550e-05 - val_loss: 3.4762e-05\n",
      "Epoch 798/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2918e-05 - val_loss: 3.5924e-05\n",
      "Epoch 799/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2771e-05 - val_loss: 3.4786e-05\n",
      "Epoch 800/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.8951e-05 - val_loss: 5.6068e-05\n",
      "Epoch 801/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.9309e-05 - val_loss: 3.4648e-05\n",
      "Epoch 802/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.2663e-05 - val_loss: 3.6483e-05\n",
      "Epoch 803/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3906e-05 - val_loss: 3.6607e-05\n",
      "Epoch 804/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4209e-05 - val_loss: 3.8259e-05\n",
      "Epoch 805/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3553e-05 - val_loss: 3.2497e-05\n",
      "Epoch 806/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.0388e-05 - val_loss: 5.0141e-05\n",
      "Epoch 807/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3539e-05 - val_loss: 3.6218e-05\n",
      "Epoch 808/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1803e-05 - val_loss: 3.5429e-05\n",
      "Epoch 809/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7038e-05 - val_loss: 5.4014e-05\n",
      "Epoch 810/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9.9090e-05 - val_loss: 4.3795e-05\n",
      "Epoch 811/1000\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 4.9642e-05 - val_loss: 3.3291e-05\n",
      "Epoch 812/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0484e-05 - val_loss: 3.4426e-05\n",
      "Epoch 813/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0482e-05 - val_loss: 3.3628e-05\n",
      "Epoch 814/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0780e-05 - val_loss: 3.2485e-05\n",
      "Epoch 815/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1918e-05 - val_loss: 3.2550e-05\n",
      "Epoch 816/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6323e-05 - val_loss: 3.2307e-05\n",
      "Epoch 817/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0293e-05 - val_loss: 3.1968e-05\n",
      "Epoch 818/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0841e-05 - val_loss: 3.3041e-05\n",
      "Epoch 819/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0153e-05 - val_loss: 3.2114e-05\n",
      "Epoch 820/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0015e-05 - val_loss: 3.2629e-05\n",
      "Epoch 821/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0491e-05 - val_loss: 3.1998e-05\n",
      "Epoch 822/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0864e-05 - val_loss: 3.3669e-05\n",
      "Epoch 823/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2735e-05 - val_loss: 3.2608e-05\n",
      "Epoch 824/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2615e-05 - val_loss: 3.4287e-05\n",
      "Epoch 825/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1033e-05 - val_loss: 3.3291e-05\n",
      "Epoch 826/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1084e-05 - val_loss: 3.2329e-05\n",
      "Epoch 827/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5118e-05 - val_loss: 3.2426e-05\n",
      "Epoch 828/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0896e-05 - val_loss: 3.2853e-05\n",
      "Epoch 829/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7449e-05 - val_loss: 5.3070e-05\n",
      "Epoch 830/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3748e-05 - val_loss: 3.8224e-05\n",
      "Epoch 831/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4636e-05 - val_loss: 3.5941e-05\n",
      "Epoch 832/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2386e-05 - val_loss: 3.6067e-05\n",
      "Epoch 833/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.2034e-05 - val_loss: 3.3258e-05\n",
      "Epoch 834/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1082e-05 - val_loss: 3.7163e-05\n",
      "Epoch 835/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.2231e-05 - val_loss: 3.4712e-05\n",
      "Epoch 836/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6842e-05 - val_loss: 3.6103e-05\n",
      "Epoch 837/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3250e-05 - val_loss: 3.8573e-05\n",
      "Epoch 838/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0013e-05 - val_loss: 3.2044e-05\n",
      "Epoch 839/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0353e-05 - val_loss: 3.1837e-05\n",
      "Epoch 840/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1631e-05 - val_loss: 3.7367e-05\n",
      "Epoch 841/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1262e-05 - val_loss: 3.1975e-05\n",
      "Epoch 842/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4297e-05 - val_loss: 1.0895e-04\n",
      "Epoch 843/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.5050e-05 - val_loss: 6.0154e-05\n",
      "Epoch 844/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 8.3689e-05 - val_loss: 4.0553e-05\n",
      "Epoch 845/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4750e-05 - val_loss: 3.3702e-05\n",
      "Epoch 846/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7347e-05 - val_loss: 4.2152e-05\n",
      "Epoch 847/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1052e-05 - val_loss: 3.1615e-05\n",
      "Epoch 848/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0872e-05 - val_loss: 3.2834e-05\n",
      "Epoch 849/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2510e-05 - val_loss: 3.1701e-05\n",
      "Epoch 850/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.2262e-05 - val_loss: 3.2478e-05\n",
      "Epoch 851/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9880e-05 - val_loss: 3.2363e-05\n",
      "Epoch 852/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1145e-05 - val_loss: 3.6929e-05\n",
      "Epoch 853/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2719e-05 - val_loss: 3.1284e-05\n",
      "Epoch 854/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1665e-05 - val_loss: 3.0777e-05\n",
      "Epoch 855/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8812e-05 - val_loss: 3.2589e-05\n",
      "Epoch 856/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0119e-05 - val_loss: 3.1285e-05\n",
      "Epoch 857/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9297e-05 - val_loss: 3.1964e-05\n",
      "Epoch 858/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9450e-05 - val_loss: 3.2353e-05\n",
      "Epoch 859/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0774e-05 - val_loss: 3.1282e-05\n",
      "Epoch 860/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1000e-05 - val_loss: 3.0262e-05\n",
      "Epoch 861/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9302e-05 - val_loss: 3.1301e-05\n",
      "Epoch 862/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7229e-05 - val_loss: 7.0363e-05\n",
      "Epoch 863/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.0818e-05 - val_loss: 3.2510e-05\n",
      "Epoch 864/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.4318e-05 - val_loss: 3.1996e-05\n",
      "Epoch 865/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0123e-05 - val_loss: 3.4575e-05\n",
      "Epoch 866/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0201e-05 - val_loss: 3.0500e-05\n",
      "Epoch 867/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.7792e-05 - val_loss: 1.5204e-04\n",
      "Epoch 868/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.1123e-05 - val_loss: 1.0360e-04\n",
      "Epoch 869/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 6.6072e-05 - val_loss: 3.6695e-05\n",
      "Epoch 870/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2051e-05 - val_loss: 3.2564e-05\n",
      "Epoch 871/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0225e-05 - val_loss: 3.1724e-05\n",
      "Epoch 872/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9651e-05 - val_loss: 3.2183e-05\n",
      "Epoch 873/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9080e-05 - val_loss: 3.0721e-05\n",
      "Epoch 874/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9186e-05 - val_loss: 3.1465e-05\n",
      "Epoch 875/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8896e-05 - val_loss: 3.3342e-05\n",
      "Epoch 876/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8649e-05 - val_loss: 3.0978e-05\n",
      "Epoch 877/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8770e-05 - val_loss: 3.0250e-05\n",
      "Epoch 878/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9360e-05 - val_loss: 3.2777e-05\n",
      "Epoch 879/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8942e-05 - val_loss: 3.1971e-05\n",
      "Epoch 880/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8855e-05 - val_loss: 2.9948e-05\n",
      "Epoch 881/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8993e-05 - val_loss: 3.0745e-05\n",
      "Epoch 882/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8853e-05 - val_loss: 3.0208e-05\n",
      "Epoch 883/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9982e-05 - val_loss: 3.0101e-05\n",
      "Epoch 884/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7565e-05 - val_loss: 3.0803e-05\n",
      "Epoch 885/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9779e-05 - val_loss: 3.0416e-05\n",
      "Epoch 886/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9223e-05 - val_loss: 3.0292e-05\n",
      "Epoch 887/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8851e-05 - val_loss: 3.2107e-05\n",
      "Epoch 888/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6034e-05 - val_loss: 1.7769e-04\n",
      "Epoch 889/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.3960e-05 - val_loss: 3.6308e-05\n",
      "Epoch 890/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9783e-05 - val_loss: 3.0954e-05\n",
      "Epoch 891/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9374e-05 - val_loss: 3.2789e-05\n",
      "Epoch 892/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9257e-05 - val_loss: 3.0270e-05\n",
      "Epoch 893/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9569e-05 - val_loss: 3.7471e-05\n",
      "Epoch 894/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0239e-05 - val_loss: 3.0095e-05\n",
      "Epoch 895/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9262e-05 - val_loss: 2.9807e-05\n",
      "Epoch 896/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0070e-05 - val_loss: 3.0041e-05\n",
      "Epoch 897/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8642e-05 - val_loss: 2.9494e-05\n",
      "Epoch 898/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8389e-05 - val_loss: 3.0128e-05\n",
      "Epoch 899/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8795e-05 - val_loss: 3.1008e-05\n",
      "Epoch 900/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8639e-05 - val_loss: 3.1758e-05\n",
      "Epoch 901/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9906e-05 - val_loss: 3.0535e-05\n",
      "Epoch 902/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5201e-05 - val_loss: 3.0480e-05\n",
      "Epoch 903/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.0601e-05 - val_loss: 3.8365e-05\n",
      "Epoch 904/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3292e-05 - val_loss: 4.4586e-05\n",
      "Epoch 905/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1161e-05 - val_loss: 3.1231e-05\n",
      "Epoch 906/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0459e-05 - val_loss: 3.0844e-05\n",
      "Epoch 907/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9628e-05 - val_loss: 3.2482e-05\n",
      "Epoch 908/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8400e-05 - val_loss: 3.0893e-05\n",
      "Epoch 909/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1369e-05 - val_loss: 4.6144e-05\n",
      "Epoch 910/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.2680e-05 - val_loss: 3.2440e-05\n",
      "Epoch 911/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1185e-05 - val_loss: 7.7411e-05\n",
      "Epoch 912/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3366e-05 - val_loss: 3.1319e-05\n",
      "Epoch 913/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2549e-05 - val_loss: 3.7369e-05\n",
      "Epoch 914/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8.9200e-05 - val_loss: 4.6119e-05\n",
      "Epoch 915/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.4381e-05 - val_loss: 3.2998e-05\n",
      "Epoch 916/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9717e-05 - val_loss: 3.0555e-05\n",
      "Epoch 917/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0802e-05 - val_loss: 3.2998e-05\n",
      "Epoch 918/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9172e-05 - val_loss: 3.1566e-05\n",
      "Epoch 919/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.7528e-05 - val_loss: 3.2442e-05\n",
      "Epoch 920/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8220e-05 - val_loss: 3.1419e-05\n",
      "Epoch 921/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8850e-05 - val_loss: 3.0840e-05\n",
      "Epoch 922/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0018e-05 - val_loss: 3.0429e-05\n",
      "Epoch 923/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9211e-05 - val_loss: 3.9443e-05\n",
      "Epoch 924/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0554e-05 - val_loss: 3.2696e-05\n",
      "Epoch 925/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.1115e-05 - val_loss: 3.1929e-05\n",
      "Epoch 926/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0026e-05 - val_loss: 2.9511e-05\n",
      "Epoch 927/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7463e-05 - val_loss: 2.9491e-05\n",
      "Epoch 928/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6786e-05 - val_loss: 2.9596e-05\n",
      "Epoch 929/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9154e-05 - val_loss: 2.9410e-05\n",
      "Epoch 930/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.7694e-05 - val_loss: 2.8880e-05\n",
      "Epoch 931/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.7352e-05 - val_loss: 2.9083e-05\n",
      "Epoch 932/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9666e-05 - val_loss: 3.0157e-05\n",
      "Epoch 933/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9258e-05 - val_loss: 2.9506e-05\n",
      "Epoch 934/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8136e-05 - val_loss: 2.9453e-05\n",
      "Epoch 935/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.5101e-05 - val_loss: 3.2481e-05\n",
      "Epoch 936/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1352e-05 - val_loss: 2.8904e-05\n",
      "Epoch 937/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.9302e-05 - val_loss: 4.1914e-05\n",
      "Epoch 938/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8997e-05 - val_loss: 2.8400e-05\n",
      "Epoch 939/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9841e-05 - val_loss: 3.7683e-05\n",
      "Epoch 940/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5.5904e-05 - val_loss: 2.1372e-04\n",
      "Epoch 941/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.1122e-05 - val_loss: 3.7833e-05\n",
      "Epoch 942/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3937e-05 - val_loss: 3.2381e-05\n",
      "Epoch 943/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8083e-05 - val_loss: 3.0812e-05\n",
      "Epoch 944/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8899e-05 - val_loss: 2.9543e-05\n",
      "Epoch 945/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6972e-05 - val_loss: 2.9474e-05\n",
      "Epoch 946/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6445e-05 - val_loss: 2.8354e-05\n",
      "Epoch 947/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2451e-05 - val_loss: 3.0642e-05\n",
      "Epoch 948/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.6457e-05 - val_loss: 4.2104e-05\n",
      "Epoch 949/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1775e-05 - val_loss: 3.6324e-05\n",
      "Epoch 950/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.0086e-05 - val_loss: 3.1747e-05\n",
      "Epoch 951/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8978e-05 - val_loss: 4.4080e-05\n",
      "Epoch 952/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7455e-05 - val_loss: 3.2767e-05\n",
      "Epoch 953/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1773e-05 - val_loss: 2.8512e-05\n",
      "Epoch 954/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7603e-05 - val_loss: 3.1420e-05\n",
      "Epoch 955/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0606e-05 - val_loss: 2.8392e-05\n",
      "Epoch 956/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8185e-05 - val_loss: 3.5538e-05\n",
      "Epoch 957/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.7205e-05 - val_loss: 3.0801e-05\n",
      "Epoch 958/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.6910e-05 - val_loss: 2.8720e-05\n",
      "Epoch 959/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7020e-05 - val_loss: 2.8611e-05\n",
      "Epoch 960/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6925e-05 - val_loss: 2.8356e-05\n",
      "Epoch 961/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3797e-05 - val_loss: 3.5729e-05\n",
      "Epoch 962/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0056e-05 - val_loss: 2.9728e-05\n",
      "Epoch 963/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 3.3482e-05 - val_loss: 3.2878e-05\n",
      "Epoch 964/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.2819e-05 - val_loss: 3.3618e-05\n",
      "Epoch 965/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.8147e-05 - val_loss: 2.7816e-05\n",
      "Epoch 966/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.7685e-05 - val_loss: 5.1043e-05\n",
      "Epoch 967/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7.7514e-05 - val_loss: 6.5699e-05\n",
      "Epoch 968/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 4.0681e-05 - val_loss: 3.2939e-05\n",
      "Epoch 969/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8715e-05 - val_loss: 2.8290e-05\n",
      "Epoch 970/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6992e-05 - val_loss: 2.8437e-05\n",
      "Epoch 971/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.6343e-05 - val_loss: 2.9676e-05\n",
      "Epoch 972/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6301e-05 - val_loss: 2.9110e-05\n",
      "Epoch 973/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6389e-05 - val_loss: 2.8085e-05\n",
      "Epoch 974/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.7517e-05 - val_loss: 2.8382e-05\n",
      "Epoch 975/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6294e-05 - val_loss: 2.9268e-05\n",
      "Epoch 976/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7400e-05 - val_loss: 2.7323e-05\n",
      "Epoch 977/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6004e-05 - val_loss: 2.7908e-05\n",
      "Epoch 978/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7828e-05 - val_loss: 2.8261e-05\n",
      "Epoch 979/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7884e-05 - val_loss: 3.0402e-05\n",
      "Epoch 980/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0118e-05 - val_loss: 2.7701e-05\n",
      "Epoch 981/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1100e-05 - val_loss: 2.7691e-05\n",
      "Epoch 982/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.7146e-05 - val_loss: 2.7677e-05\n",
      "Epoch 983/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.7813e-05 - val_loss: 3.4226e-05\n",
      "Epoch 984/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6440e-05 - val_loss: 3.0644e-05\n",
      "Epoch 985/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7437e-05 - val_loss: 2.9509e-05\n",
      "Epoch 986/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5753e-05 - val_loss: 2.9419e-05\n",
      "Epoch 987/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.6063e-05 - val_loss: 3.8966e-05\n",
      "Epoch 988/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4.9578e-05 - val_loss: 3.1507e-05\n",
      "Epoch 989/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6251e-05 - val_loss: 2.7292e-05\n",
      "Epoch 990/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5971e-05 - val_loss: 2.8416e-05\n",
      "Epoch 991/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.6139e-05 - val_loss: 2.8668e-05\n",
      "Epoch 992/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.5834e-05 - val_loss: 2.7666e-05\n",
      "Epoch 993/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.8287e-05 - val_loss: 3.7462e-05\n",
      "Epoch 994/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.0247e-05 - val_loss: 3.2274e-05\n",
      "Epoch 995/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.3767e-05 - val_loss: 3.2070e-05\n",
      "Epoch 996/1000\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 2.7166e-05 - val_loss: 2.8594e-05\n",
      "Epoch 997/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3.1804e-05 - val_loss: 4.3116e-05\n",
      "Epoch 998/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6.0761e-05 - val_loss: 3.4024e-05\n",
      "Epoch 999/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.9741e-05 - val_loss: 2.9126e-05\n",
      "Epoch 1000/1000\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2.7559e-05 - val_loss: 2.7942e-05\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "Accuracy: 99.96%\n",
      "54.43395031634155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shine\\AppData\\Local\\Temp\\ipykernel_16652\\2631512346.py:58: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  extreme_rows =df[df['rainfall_class'] == \"Extreme\"][df['actual_rainfall_class'] == \"Extreme\"]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Load the data\n",
    "\n",
    "# Split the data into normal and extreme rainfall\n",
    "extreme_data = df[df[\"rainfall\"] > 50]\n",
    "normal_data = df[df[\"rainfall\"] <= 50]\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "def create_model(reg_strength):\n",
    "    input_layer = Input(shape=(3,))\n",
    "    encoded = Dense(16, activation='relu', kernel_regularizer='l2', bias_regularizer='l2', activity_regularizer='l2', kernel_initializer='he_uniform')(input_layer)\n",
    "    decoded = Dense(3, activation=None, kernel_initializer='he_uniform')(encoded)\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='logcosh', metrics=['mse'])\n",
    "    return autoencoder\n",
    "\n",
    "# Create the KerasRegressor\n",
    "keras_reg = KerasRegressor(build_fn=create_model)\n",
    "\n",
    "# Define the grid search parameters\n",
    "reg_strengths = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = dict(reg_strength=reg_strengths)\n",
    "\n",
    "# Perform the grid search\n",
    "grid = GridSearchCV(estimator=keras_reg, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_result = grid.fit(normal_data[[\"windspeed_tree\",\"tpw_tree\",\"rainfall_tree\"]].values, normal_data[[\"windspeed_tree\",\"tpw_tree\",\"rainfall_tree\"]].values)\n",
    "\n",
    "# Print the best regularization strength found\n",
    "print(\"Best reg_strength: {:.3f} using {}\".format(grid_result.best_params_['reg_strength'], grid_result.best_params_))\n",
    "\n",
    "# Define the autoencoder architecture with the best regularization strength found by grid search\n",
    "input_layer = Input(shape=(3,))\n",
    "encoded = Dense(16, activation='relu', kernel_regularizer='l2', bias_regularizer='l2', activity_regularizer='l2', kernel_initializer='he_uniform')(input_layer)\n",
    "decoded = Dense(3, activation=None, kernel_initializer='he_uniform')(encoded)\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='logcosh')\n",
    "\n",
    "# Train the model\n",
    "train_data = normal_data[[\"windspeed_tree\",\"tpw_tree\",\"rainfall_tree\"]].values\n",
    "history = autoencoder.fit(train_data, train_data, epochs=1000, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder to predict the rainfall values for all data points\n",
    "test_data = df[[\"windspeed_tree\",\"tpw_tree\",\"rainfall_tree\"]].values\n",
    "predicted_data = autoencoder.predict(test_data)\n",
    "df['predicted_rainfall'] = predicted_data[:, 2]\n",
    "\n",
    "# Calculate the error between the predicted and actual rainfall values\n",
    "df['error'] = np.abs(df['predicted_rainfall'] - df['rainfall'])\n",
    "\n",
    "\n",
    "extreme_rows =df[df['rainfall_class'] == \"Extreme\"][df['actual_rainfall_class'] == \"Extreme\"]\n",
    "min_error = extreme_rows['error'].min()\n",
    "# Set the threshold as the median of the error values\n",
    "threshold = min_error\n",
    "\n",
    "# Classify the rainfall data into normal and extreme based on the error threshold\n",
    "df['rainfall_class'] = np.where(df['error'] > threshold, 'Extreme', 'Normal')\n",
    "\n",
    "# Define the actual rainfall class based on the threshold of 11\n",
    "df['actual_rainfall_class'] = np.where(df['rainfall'] > 50, 'Extreme', 'Normal')\n",
    "\n",
    "# Calculate the accuracy\n",
    "num_correct = len(df[df['rainfall_class'] == df['actual_rainfall_class']])\n",
    "num_total = len(df)\n",
    "accuracy = num_correct / num_total * 100\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "454fd8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  windspeed       tpw   rainfall  rainfall_tree  \\\n",
      "2388  8/10/2019    20.2768  0.043114  58.431061              4   \n",
      "\n",
      "      windspeed_tree  tpw_tree  predicted_rainfall     error rainfall_class  \\\n",
      "2388               0         0            3.997111  54.43395         Normal   \n",
      "\n",
      "     actual_rainfall_class  \n",
      "2388               Extreme  \n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = df[df['rainfall_class'] != df['actual_rainfall_class']]\n",
    "\n",
    "# Print the wrong predictions\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "525a159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shine\\AppData\\Local\\Temp\\ipykernel_16652\\3391782895.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  extreme_rows =df[df['rainfall_class'] == \"Extreme\"][df['actual_rainfall_class'] == \"Extreme\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>rainfall_tree</th>\n",
       "      <th>windspeed_tree</th>\n",
       "      <th>tpw_tree</th>\n",
       "      <th>predicted_rainfall</th>\n",
       "      <th>error</th>\n",
       "      <th>rainfall_class</th>\n",
       "      <th>actual_rainfall_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>21.575483</td>\n",
       "      <td>0.042829</td>\n",
       "      <td>80.636074</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.999425</td>\n",
       "      <td>76.636649</td>\n",
       "      <td>Extreme</td>\n",
       "      <td>Extreme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>8/10/2019</td>\n",
       "      <td>20.276800</td>\n",
       "      <td>0.043114</td>\n",
       "      <td>58.431061</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.999425</td>\n",
       "      <td>54.431636</td>\n",
       "      <td>Extreme</td>\n",
       "      <td>Extreme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  windspeed       tpw   rainfall  rainfall_tree  \\\n",
       "2387   8/9/2019  21.575483  0.042829  80.636074              4   \n",
       "2388  8/10/2019  20.276800  0.043114  58.431061              4   \n",
       "\n",
       "      windspeed_tree  tpw_tree  predicted_rainfall      error rainfall_class  \\\n",
       "2387               0         0            3.999425  76.636649        Extreme   \n",
       "2388               0         0            3.999425  54.431636        Extreme   \n",
       "\n",
       "     actual_rainfall_class  \n",
       "2387               Extreme  \n",
       "2388               Extreme  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the rows where actual_rainfall_class is Extreme\n",
    "extreme_rows =df[df['rainfall_class'] == \"Extreme\"][df['actual_rainfall_class'] == \"Extreme\"]\n",
    "\n",
    "# Set the error value to the minimum error in those rows\n",
    "min_error = extreme_rows['error'].min()\n",
    "#extreme_rows.loc[:, 'error'] == min_error\n",
    "extreme_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8efba9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.43163598719421"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "823602fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shine\\AppData\\Local\\Temp\\ipykernel_16652\\4277381040.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  extreme_rows =df[df['rainfall_class'] == \"Extreme\"][df['actual_rainfall_class'] == \"Extreme\"]\n",
      "C:\\Users\\shine\\AppData\\Local\\Temp\\ipykernel_16652\\4277381040.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  non=df[df['rainfall_class'] != \"Normal\"][df['actual_rainfall_class'] == \"Normal\"][df['rainfall_class'] != \"Extreme\"][df['actual_rainfall_class'] == \"Extreme\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>rainfall_tree</th>\n",
       "      <th>windspeed_tree</th>\n",
       "      <th>tpw_tree</th>\n",
       "      <th>predicted_rainfall</th>\n",
       "      <th>error</th>\n",
       "      <th>rainfall_class</th>\n",
       "      <th>actual_rainfall_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>21.575483</td>\n",
       "      <td>0.042829</td>\n",
       "      <td>80.636074</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.997111</td>\n",
       "      <td>76.638964</td>\n",
       "      <td>Extreme</td>\n",
       "      <td>Extreme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  windspeed       tpw   rainfall  rainfall_tree  windspeed_tree  \\\n",
       "2387  8/9/2019  21.575483  0.042829  80.636074              4               0   \n",
       "\n",
       "      tpw_tree  predicted_rainfall      error rainfall_class  \\\n",
       "2387         0            3.997111  76.638964        Extreme   \n",
       "\n",
       "     actual_rainfall_class  \n",
       "2387               Extreme  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_rows =df[df['rainfall_class'] == \"Extreme\"][df['actual_rainfall_class'] == \"Extreme\"]\n",
    "non=df[df['rainfall_class'] != \"Normal\"][df['actual_rainfall_class'] == \"Normal\"][df['rainfall_class'] != \"Extreme\"][df['actual_rainfall_class'] == \"Extreme\"]\n",
    "# Set the error value to the minimum error in those rows\n",
    "#if extreme_rows\n",
    "min_error = extreme_rows['error'].min()\n",
    "#extreme_rows.loc[:, 'error'] == min_error\n",
    "extreme_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "17366cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>tpw</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>rainfall_tree</th>\n",
       "      <th>windspeed_tree</th>\n",
       "      <th>tpw_tree</th>\n",
       "      <th>predicted_rainfall</th>\n",
       "      <th>error</th>\n",
       "      <th>rainfall_class</th>\n",
       "      <th>actual_rainfall_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, windspeed, tpw, rainfall, rainfall_tree, windspeed_tree, tpw_tree, predicted_rainfall, error, rainfall_class, actual_rainfall_class]\n",
       "Index: []"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7a6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a66f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
